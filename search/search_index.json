{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Repositories Code Samples Miscellaneous code samples. Details are available in the repository wiki and the technical docs section. Go... INFOSEC Computer and network security resources. Go... Python A collection of Python projects. Go... Systems Integration Code for various Web Service and Web API templates. Go... PHAROS This is primarily a repository for template code and development guides. Go...","title":"Home"},{"location":"index.html#repositories","text":"Code Samples Miscellaneous code samples. Details are available in the repository wiki and the technical docs section. Go... INFOSEC Computer and network security resources. Go... Python A collection of Python projects. Go... Systems Integration Code for various Web Service and Web API templates. Go... PHAROS This is primarily a repository for template code and development guides. Go...","title":"Repositories"},{"location":"projects.html","text":"Projects Sample Project Sample projects here. Go...","title":"Projects"},{"location":"projects.html#projects","text":"Sample Project Sample projects here. Go...","title":"Projects"},{"location":"MyDocs/index.html","text":"Index Page Suggested Toolset Development Standards Software Architecture Document GitHub Docker SOLID Development JSON Web Tokens Linux Kernel Modules OpenCart Development Notes DotNet AJAX View Injection Web Services Web APIs Setting Up a Data Source Using Code First Entity Framework Extending ASP.NET IdentityUsers Database Basic SQL Scripts SQLite3 Python Jupyter Notebooks Authentication with Python PyCrypto AES Smart Card Programming","title":"Index Page"},{"location":"MyDocs/index.html#index-page","text":"Suggested Toolset Development Standards Software Architecture Document GitHub Docker SOLID Development JSON Web Tokens Linux Kernel Modules OpenCart Development Notes","title":"Index Page"},{"location":"MyDocs/index.html#dotnet","text":"AJAX View Injection Web Services Web APIs Setting Up a Data Source Using Code First Entity Framework Extending ASP.NET IdentityUsers","title":"DotNet"},{"location":"MyDocs/index.html#database","text":"Basic SQL Scripts SQLite3","title":"Database"},{"location":"MyDocs/index.html#python","text":"Jupyter Notebooks Authentication with Python PyCrypto AES Smart Card Programming","title":"Python"},{"location":"MyDocs/ajax-view-injection.html","text":"AJAX View Injection Code for bypassing default MVC controller actions. This is useful if we only want to update specific elements in the application's UI, or to execute a controller action without reloading a view. There are three elements: a) AJAX handler, b) Controller action, and c) AJAX response formatter. AJAX Handler $(document).ready(function () { $.ajax({ type: \"GET\", cache: false, url: '/Home/MyView', dataType: \"JSON\", success: function (data) { $(\"#myviewcard\").html(data.view); } }); $.ajax({ type: \"GET\", cache: false, url: '/Home/AboutPage', dataType: \"JSON\", success: function (data) { $(\"#aboutpagecard\").html(data.view); } }); }); Controller [HttpGet] [CheckAjaxRequest] public ActionResult MyView() { ... Controller code here ... return Json(new { success = true, view = RenderPartialViewToString(\"_MyPartialView\", model) }, JsonRequestBehavior.AllowGet); } AJAX Response Formatter public class CheckAjaxRequestAttribute : ActionFilterAttribute { private const string AJAX_HEADER = \"X-Requested-With\"; public override void OnActionExecuting(ActionExecutingContext filterContext) { bool isAjaxRequest = filterContext.HttpContext.Request.Headers[AJAX_HEADER] != null; if (!isAjaxRequest) { filterContext.Result = new ViewResult { ViewName = \"Unauthorized\" }; } } } protected string RenderPartialViewToString(string viewName, object model) { if (string.IsNullOrEmpty(viewName)) viewName = ControllerContext.RouteData.GetRequiredString(\"action\"); ViewData.Model = model; using (var sw = new StringWriter()) { var viewResult = ViewEngines.Engines.FindPartialView(ControllerContext, viewName); var viewContext = new ViewContext(ControllerContext, viewResult.View, ViewData, TempData, sw); viewResult.View.Render(viewContext, sw); return sw.GetStringBuilder().ToString(); } }","title":"AJAX View Injection"},{"location":"MyDocs/ajax-view-injection.html#ajax-view-injection","text":"Code for bypassing default MVC controller actions. This is useful if we only want to update specific elements in the application's UI, or to execute a controller action without reloading a view. There are three elements: a) AJAX handler, b) Controller action, and c) AJAX response formatter.","title":"AJAX View Injection"},{"location":"MyDocs/ajax-view-injection.html#ajax-handler","text":"$(document).ready(function () { $.ajax({ type: \"GET\", cache: false, url: '/Home/MyView', dataType: \"JSON\", success: function (data) { $(\"#myviewcard\").html(data.view); } }); $.ajax({ type: \"GET\", cache: false, url: '/Home/AboutPage', dataType: \"JSON\", success: function (data) { $(\"#aboutpagecard\").html(data.view); } }); });","title":"AJAX Handler"},{"location":"MyDocs/ajax-view-injection.html#controller","text":"[HttpGet] [CheckAjaxRequest] public ActionResult MyView() { ... Controller code here ... return Json(new { success = true, view = RenderPartialViewToString(\"_MyPartialView\", model) }, JsonRequestBehavior.AllowGet); }","title":"Controller"},{"location":"MyDocs/ajax-view-injection.html#ajax-response-formatter","text":"public class CheckAjaxRequestAttribute : ActionFilterAttribute { private const string AJAX_HEADER = \"X-Requested-With\"; public override void OnActionExecuting(ActionExecutingContext filterContext) { bool isAjaxRequest = filterContext.HttpContext.Request.Headers[AJAX_HEADER] != null; if (!isAjaxRequest) { filterContext.Result = new ViewResult { ViewName = \"Unauthorized\" }; } } } protected string RenderPartialViewToString(string viewName, object model) { if (string.IsNullOrEmpty(viewName)) viewName = ControllerContext.RouteData.GetRequiredString(\"action\"); ViewData.Model = model; using (var sw = new StringWriter()) { var viewResult = ViewEngines.Engines.FindPartialView(ControllerContext, viewName); var viewContext = new ViewContext(ControllerContext, viewResult.View, ViewData, TempData, sw); viewResult.View.Render(viewContext, sw); return sw.GetStringBuilder().ToString(); } }","title":"AJAX Response Formatter"},{"location":"MyDocs/basic-sql.html","text":"Basic SQL Scripts These scripts are for MS-SQL. Create a new table BEGIN TRANSACTION SET QUOTED_IDENTIFIER ON SET ARITHABORT ON SET NUMERIC_ROUNDABORT OFF SET CONCAT_NULL_YIELDS_NULL ON SET ANSI_NULLS ON SET ANSI_PADDING ON SET ANSI_WARNINGS ON COMMIT BEGIN TRANSACTION GO CREATE TABLE dbo.Table_1 ( Id int NULL, English text NULL, German text NULL, Note text NULL, Category text NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE dbo.Table_1 SET (LOCK_ESCALATION = TABLE) GO COMMIT Select top x number of rows SELECT TOP (1000) [Id] ,[English] ,[German] ,[Note] ,[Category] FROM [GermanPhraseBook].[dbo].[phrases] Delete a row from the table. DELETE TOP (1) FROM phrases WHERE CONVERT(VARCHAR(MAX), [German]) = 'Phrase to be deleted'","title":"Basic SQL Scripts"},{"location":"MyDocs/basic-sql.html#basic-sql-scripts","text":"These scripts are for MS-SQL. Create a new table BEGIN TRANSACTION SET QUOTED_IDENTIFIER ON SET ARITHABORT ON SET NUMERIC_ROUNDABORT OFF SET CONCAT_NULL_YIELDS_NULL ON SET ANSI_NULLS ON SET ANSI_PADDING ON SET ANSI_WARNINGS ON COMMIT BEGIN TRANSACTION GO CREATE TABLE dbo.Table_1 ( Id int NULL, English text NULL, German text NULL, Note text NULL, Category text NULL ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO ALTER TABLE dbo.Table_1 SET (LOCK_ESCALATION = TABLE) GO COMMIT Select top x number of rows SELECT TOP (1000) [Id] ,[English] ,[German] ,[Note] ,[Category] FROM [GermanPhraseBook].[dbo].[phrases] Delete a row from the table. DELETE TOP (1) FROM phrases WHERE CONVERT(VARCHAR(MAX), [German]) = 'Phrase to be deleted'","title":"Basic SQL Scripts"},{"location":"MyDocs/code-first-setup.html","text":"Setting Up a Data Source Using the Code-First Entity Framework Model Since most my .NET projects for the past several years have been for clinical systems, I\u2019ve been working from Database-First Entity Framework models. The software needed to be designed around whatever database schemas and stored procedures already existed, and data models needed to be generated from them. So, it\u2019s only quite recently that I\u2019ve looked at Code-First models, which seem more appropriate to situations in which we\u2019d want a database schema to evolve as an application is being developed. With the Code-First method, the data model is defined in the application\u2019s code, and we sync the changes to the database. To try the following example, you\u2019ll need to create a new ASP.NET MVC project in Visual Studio, and ensure it\u2019s created with the \u2018 Individual User Accounts option (click the \u2018 Change Authentication \u2018 button when choosing the template), as I\u2019m using classes within a file called \u2018 IdentityModels.cs \u2018 to set up the initial model and schema. When the project is loaded, right-click on the project and select \u2018 Manage NuGet Packages\u2026 \u2018. If Entity Framework isn\u2019t already installed, install it. We should be set up for the first step. It might be worth hanging onto the project if you\u2019re following this example, because a future post will be going into developing the controllers to read and write data using this. Generating the First Migration Script and Database From IdentityModel First let\u2019s take a look at the IdentityModels.cs file, as the two classes in here are important for understanding the Code-First method. The first class is ApplicationUser . I\u2019m still not entirely sure how it works, as the implementations are hidden, but it uses IdentityUser as its base class, and I want to later extend an instance of this with a property that defines the application user name. public class ApplicationUser : IdentityUser { public async Task<ClaimsIdentity> GenerateUserIdentityAsync(UserManager<ApplicationUser> manager) { var UserIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); return userIdentity; } } The second is the DbContext . Currently it references a connection string in Web.config named \u2018 DefaultConnection . public class ApplicationDbContext : IdentityDbContext<ApplicationUser> { public ApplicationDbContext() :base(\"DefaultConnection\", throwIfV1Schema:false) { } public static ApplicationDbContext Create() { return new ApplicationDbContext(); } } In my project, these classes are within the namespace \u2018 CodeFirstExample.Models \u2018. Now it\u2019s possible to generate a database table and its schema from this by using the Migrations tool, which is run in the Package Manager Console \u2013 find this in \u2018 Tools \u2018 \u2013 \u2018 NuGet Package Manager \u2018 \u2013 \u2018 Package Manager Console \u2018. Basically this is a PowerShell command line interface, and we can do more than simply fetch and remove packages. PM> enable-migrations PM> add-migration InitialMigration The console should contain: PM> enable-migrations Checking if the context targets an existing database... Code First Migrations enabled for project CodeFirstExample. PM> add-migration InitialMigration Scaffolding migration 'InitialMigration'. The Designer Code for this migration file includes a snapshot of your current Code First model. This snapshot is used to calculate the changes to your model when you scaffold the next migration. If you make additional changes to your model that you want to include in this migration, then you can re-scaffold it by running 'Add-Migration InitialMigration' again. PM> We should also see a Migrations folder appear in Solution Explorer, containing a [migration name].cs file. This will have C# methods containing code that looks very much like SQL \u2013 these should be easily translatable to actual SQL scripts. To execute the migrations script, run the following command: PM> update-database Now refresh the SQL Server Object Explorer window. If there is no configuration string included in Web.config or the connection string name isn\u2019t specified in the DbContext, Entity Framework will create a database locally. Since I\u2019ve already got SQL Server Express installed, the new schema appears under that connection name. The new database will be called something like \u2018 aspnet-[project name]-timestamp \u2019, and it\u2019ll have tables for the ApplicationUser identity. The table it uses for local accounts is dbo.AspNetUsers . If the application was run, the login and register features should be functioning, and the [Authorize] attribute can be set on any controller action. A user could register and modify an account, and the login details will appear in the database table. Creating Our Own Model Next I\u2019ve defined my own model for a calendar application, by adding two classes: MyCalendar and EventType . In the Models folder, I\u2019ve created another class file called \u2018 MyCalendarModel.cs \u2018, and added the following code into it: namespace CodeFirstExample.Models { public class MyCalendar { public int Id { get; set; } [Required] public ApplicationUser CalendarUser { get; set; } public DateTime DateTime { get; set; } public string Location { get; set; } public EventType EventType { get; set; } } public class EventType { public byte Id { get; set; } [Required] [StringLength(160)] public string Name { get; set; } } } I want to also add the following import statements, in addition to the defaults: using System.Data.Entity; using System.ComponentModel.DataAnnotations; Also, what I typically do with the model is add validation attributes. I think it\u2019s safer and better to sort out the validation here rather than implement it solely in the view layer. I did also assume that these attributes would also give us more precise control of the data types in the database tables, but that doesn\u2019t seem the case. Adding the Model to DbContext Now we have two extra models that wouldn\u2019t do anything unless they\u2019re added to the DbContext class, so I\u2019ve added them here. public class ApplicationDbContext : IdentityDbContext<ApplicationUser> { public DbSet<MyCalendar> MyCalendars { get; set; } public DbSet<EventType> EventTypes { get; set; } public ApplicationDbContext() :base(\"DefaultConnection\", throwIfV1Schema: false) {} public static ApplicationDbContext Create() { return new ApplicationDbContext(); } } I then cleaned and rebuilt the project, just to make sure there were no obvious errors before executing another migration. Generating the Migration Script and Applying the Changes Similar to what I did previously, but the migration is given a different name so I\u2019d know later which script does what, and I\u2019ll need to force the update-database command to overwrite the existing schema: PM> add-migration AppendMyModels PM> update-database -Force In Server Explorer there\u2019ll be two additional tables, EventTypes and MyCalendars , that were created by running the above on the migration script. The only thing I needed to do after that is populate the EventTypes table, as it\u2019s there for populating a drop-down list. Also, if we look in the table definition, we can see the data types have been loosely applied from the attributes in the model class, and primary and foreign keys were set. Final Thing: Populating a Database Table for an MVC Drop-Down Menu We could populate the EventTypes table by entering the values into it using Server Explorer , but doing it from a migration script enables us to replicate the changes with other databases much faster. For this, I\u2019d need a black migration script template, which can be generated by running add migration PopulateEventTypes . I\u2019ve populated the Up() method with a series of SQL INSERT statements. I\u2019m assuming the other method, Down() , fetches or pulls things from the database. namespace CodeFirstExample.Migrations { using System; using System.Data.Entity.Migrations; public partial class PopulateEventTypesTable : DbMigration { public override void Up() { Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(1, 'Meeting')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(2, 'Travel')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(3, 'Family')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(4, 'Social')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(5, 'Holiday')\"); } public override void Down() { } } } Now run the migration script again, using \u2018 update-database \u2018 to populate the database table. Viewing the EventTypes table in the SQL Server Object Explorer, we should see it populated.","title":"Code First Entity Framework"},{"location":"MyDocs/code-first-setup.html#setting-up-a-data-source-using-the-code-first-entity-framework-model","text":"Since most my .NET projects for the past several years have been for clinical systems, I\u2019ve been working from Database-First Entity Framework models. The software needed to be designed around whatever database schemas and stored procedures already existed, and data models needed to be generated from them. So, it\u2019s only quite recently that I\u2019ve looked at Code-First models, which seem more appropriate to situations in which we\u2019d want a database schema to evolve as an application is being developed. With the Code-First method, the data model is defined in the application\u2019s code, and we sync the changes to the database. To try the following example, you\u2019ll need to create a new ASP.NET MVC project in Visual Studio, and ensure it\u2019s created with the \u2018 Individual User Accounts option (click the \u2018 Change Authentication \u2018 button when choosing the template), as I\u2019m using classes within a file called \u2018 IdentityModels.cs \u2018 to set up the initial model and schema. When the project is loaded, right-click on the project and select \u2018 Manage NuGet Packages\u2026 \u2018. If Entity Framework isn\u2019t already installed, install it. We should be set up for the first step. It might be worth hanging onto the project if you\u2019re following this example, because a future post will be going into developing the controllers to read and write data using this.","title":"Setting Up a Data Source Using the Code-First Entity Framework\u00a0Model"},{"location":"MyDocs/code-first-setup.html#generating-the-first-migration-script-and-database-from-identitymodel","text":"First let\u2019s take a look at the IdentityModels.cs file, as the two classes in here are important for understanding the Code-First method. The first class is ApplicationUser . I\u2019m still not entirely sure how it works, as the implementations are hidden, but it uses IdentityUser as its base class, and I want to later extend an instance of this with a property that defines the application user name. public class ApplicationUser : IdentityUser { public async Task<ClaimsIdentity> GenerateUserIdentityAsync(UserManager<ApplicationUser> manager) { var UserIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); return userIdentity; } } The second is the DbContext . Currently it references a connection string in Web.config named \u2018 DefaultConnection . public class ApplicationDbContext : IdentityDbContext<ApplicationUser> { public ApplicationDbContext() :base(\"DefaultConnection\", throwIfV1Schema:false) { } public static ApplicationDbContext Create() { return new ApplicationDbContext(); } } In my project, these classes are within the namespace \u2018 CodeFirstExample.Models \u2018. Now it\u2019s possible to generate a database table and its schema from this by using the Migrations tool, which is run in the Package Manager Console \u2013 find this in \u2018 Tools \u2018 \u2013 \u2018 NuGet Package Manager \u2018 \u2013 \u2018 Package Manager Console \u2018. Basically this is a PowerShell command line interface, and we can do more than simply fetch and remove packages. PM> enable-migrations PM> add-migration InitialMigration The console should contain: PM> enable-migrations Checking if the context targets an existing database... Code First Migrations enabled for project CodeFirstExample. PM> add-migration InitialMigration Scaffolding migration 'InitialMigration'. The Designer Code for this migration file includes a snapshot of your current Code First model. This snapshot is used to calculate the changes to your model when you scaffold the next migration. If you make additional changes to your model that you want to include in this migration, then you can re-scaffold it by running 'Add-Migration InitialMigration' again. PM> We should also see a Migrations folder appear in Solution Explorer, containing a [migration name].cs file. This will have C# methods containing code that looks very much like SQL \u2013 these should be easily translatable to actual SQL scripts. To execute the migrations script, run the following command: PM> update-database Now refresh the SQL Server Object Explorer window. If there is no configuration string included in Web.config or the connection string name isn\u2019t specified in the DbContext, Entity Framework will create a database locally. Since I\u2019ve already got SQL Server Express installed, the new schema appears under that connection name. The new database will be called something like \u2018 aspnet-[project name]-timestamp \u2019, and it\u2019ll have tables for the ApplicationUser identity. The table it uses for local accounts is dbo.AspNetUsers . If the application was run, the login and register features should be functioning, and the [Authorize] attribute can be set on any controller action. A user could register and modify an account, and the login details will appear in the database table.","title":"Generating the First Migration Script and Database From IdentityModel"},{"location":"MyDocs/code-first-setup.html#creating-our-own-model","text":"Next I\u2019ve defined my own model for a calendar application, by adding two classes: MyCalendar and EventType . In the Models folder, I\u2019ve created another class file called \u2018 MyCalendarModel.cs \u2018, and added the following code into it: namespace CodeFirstExample.Models { public class MyCalendar { public int Id { get; set; } [Required] public ApplicationUser CalendarUser { get; set; } public DateTime DateTime { get; set; } public string Location { get; set; } public EventType EventType { get; set; } } public class EventType { public byte Id { get; set; } [Required] [StringLength(160)] public string Name { get; set; } } } I want to also add the following import statements, in addition to the defaults: using System.Data.Entity; using System.ComponentModel.DataAnnotations; Also, what I typically do with the model is add validation attributes. I think it\u2019s safer and better to sort out the validation here rather than implement it solely in the view layer. I did also assume that these attributes would also give us more precise control of the data types in the database tables, but that doesn\u2019t seem the case.","title":"Creating Our Own Model"},{"location":"MyDocs/code-first-setup.html#adding-the-model-to-dbcontext","text":"Now we have two extra models that wouldn\u2019t do anything unless they\u2019re added to the DbContext class, so I\u2019ve added them here. public class ApplicationDbContext : IdentityDbContext<ApplicationUser> { public DbSet<MyCalendar> MyCalendars { get; set; } public DbSet<EventType> EventTypes { get; set; } public ApplicationDbContext() :base(\"DefaultConnection\", throwIfV1Schema: false) {} public static ApplicationDbContext Create() { return new ApplicationDbContext(); } } I then cleaned and rebuilt the project, just to make sure there were no obvious errors before executing another migration.","title":"Adding the Model to DbContext"},{"location":"MyDocs/code-first-setup.html#generating-the-migration-script-and-applying-the-changes","text":"Similar to what I did previously, but the migration is given a different name so I\u2019d know later which script does what, and I\u2019ll need to force the update-database command to overwrite the existing schema: PM> add-migration AppendMyModels PM> update-database -Force In Server Explorer there\u2019ll be two additional tables, EventTypes and MyCalendars , that were created by running the above on the migration script. The only thing I needed to do after that is populate the EventTypes table, as it\u2019s there for populating a drop-down list. Also, if we look in the table definition, we can see the data types have been loosely applied from the attributes in the model class, and primary and foreign keys were set.","title":"Generating the Migration Script and Applying the Changes"},{"location":"MyDocs/code-first-setup.html#final-thing-populating-a-database-table-for-an-mvc-drop-down-menu","text":"We could populate the EventTypes table by entering the values into it using Server Explorer , but doing it from a migration script enables us to replicate the changes with other databases much faster. For this, I\u2019d need a black migration script template, which can be generated by running add migration PopulateEventTypes . I\u2019ve populated the Up() method with a series of SQL INSERT statements. I\u2019m assuming the other method, Down() , fetches or pulls things from the database. namespace CodeFirstExample.Migrations { using System; using System.Data.Entity.Migrations; public partial class PopulateEventTypesTable : DbMigration { public override void Up() { Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(1, 'Meeting')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(2, 'Travel')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(3, 'Family')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(4, 'Social')\"); Sql(\"INSERT INTO EventTypes(Id, Name) VALUES(5, 'Holiday')\"); } public override void Down() { } } } Now run the migration script again, using \u2018 update-database \u2018 to populate the database table. Viewing the EventTypes table in the SQL Server Object Explorer, we should see it populated.","title":"Final Thing: Populating a Database Table for an MVC Drop-Down Menu"},{"location":"MyDocs/development-standards.html","text":"Software Development Best Practices The goal here is to produce software that not only works, but is easy as possible to maintain, support and extend. In order to achieve this, it is essential to adopt a common methods of software development across the team, in a way that enables member of a development team to move between projects with minimal effort required to familiarise themselves with them. The content in this page relates to the following principles: - Standardised - Highly integrated - Consistent - Enabling the business Software Design Multi-Factor Authentication (MFA) should be supported by applications and services. Accessibility should be considered, especially for UI development. Support shouldn't require the intervention of the software developer, so supportability should be part of the service's design. Alerts, logging and actions should be defined. Logging Services should be developed to provide adequate information to support staff, to assist them with the resolution of any problems that might arise. Log entries should indicate which actions were performed and which user performed them. This should be an identifier attributable to individual users. Log entry categories could include: INFORMATION: Event relevant to the successful execution of an operation involving data, driver or service. WARNING: Event that might indicate a potential problem with the service or the environment it's deployed on. ERROR: An event that indicates a loss of data or impaired functionality. SUCCESS AUDIT: Auditable event, especially security-related. For example, the user is successfully authenticated. FAILURE AUDIT: Auditable event, especially security-related. For example, an authentication attempt was unsuccessful. Builds Production software and services should be deployed on environments with a specific pre-determined configuration. The build and deployment process must be repeatable, with minimal complexity. Security It is a bad idea to include usernames, passwords and other authentication values in code. ASP.NET applications can use Web.config fields. Other options might include Azure KeyVault and Hashicorp. Developers should familiarise themselves with the OWASP Top 10. Applications should not access a database directly, for example through SQL commands in the server-side code. Coding Practices Code should be clear, well-written and self-explanatory. Comments should be added where appropriate for any code that might be ambiguous. Function names should be descriptive, and using upper CamelCase. e.g. MyFirstFunction() MySecondFunction() Variable names should be descriptive, and start with lower camelCase. e.g. myFirstVariable mySecondVariable Code should be self-documenting and readable. After all, programming languages are supposed to be human-readable abstractions of software. Appropriate commenting. If there's a section of code that's harder to understand or hides the implementation, it's worth adding a comment about its purpose. Especially with interfaces and implementations. Use an ORM, such as Entity Framework, instead of SQL commands. An application should never run SQL commands on a database server. Functions and methods should be single-purpose, and do one thing only (Single Responsibility Principle). Sometimes Unit of Work Repositories are recommended. Code patterns should be consistent across the application. Try to follow the existing code patterns and structures as much as possible. Try to code generic operations as base classes that could be extended. This helps to minimise code duplication. Implement validation at the data model where possible, if a model exists. Further Information Konstantin Taranov: C# Coding Standards and Naming Conventions","title":"Development Standards"},{"location":"MyDocs/development-standards.html#software-development-best-practices","text":"The goal here is to produce software that not only works, but is easy as possible to maintain, support and extend. In order to achieve this, it is essential to adopt a common methods of software development across the team, in a way that enables member of a development team to move between projects with minimal effort required to familiarise themselves with them. The content in this page relates to the following principles: - Standardised - Highly integrated - Consistent - Enabling the business","title":"Software Development Best Practices"},{"location":"MyDocs/development-standards.html#software-design","text":"Multi-Factor Authentication (MFA) should be supported by applications and services. Accessibility should be considered, especially for UI development. Support shouldn't require the intervention of the software developer, so supportability should be part of the service's design. Alerts, logging and actions should be defined.","title":"Software Design"},{"location":"MyDocs/development-standards.html#logging","text":"Services should be developed to provide adequate information to support staff, to assist them with the resolution of any problems that might arise. Log entries should indicate which actions were performed and which user performed them. This should be an identifier attributable to individual users. Log entry categories could include: INFORMATION: Event relevant to the successful execution of an operation involving data, driver or service. WARNING: Event that might indicate a potential problem with the service or the environment it's deployed on. ERROR: An event that indicates a loss of data or impaired functionality. SUCCESS AUDIT: Auditable event, especially security-related. For example, the user is successfully authenticated. FAILURE AUDIT: Auditable event, especially security-related. For example, an authentication attempt was unsuccessful.","title":"Logging"},{"location":"MyDocs/development-standards.html#builds","text":"Production software and services should be deployed on environments with a specific pre-determined configuration. The build and deployment process must be repeatable, with minimal complexity.","title":"Builds"},{"location":"MyDocs/development-standards.html#security","text":"It is a bad idea to include usernames, passwords and other authentication values in code. ASP.NET applications can use Web.config fields. Other options might include Azure KeyVault and Hashicorp. Developers should familiarise themselves with the OWASP Top 10. Applications should not access a database directly, for example through SQL commands in the server-side code.","title":"Security"},{"location":"MyDocs/development-standards.html#coding-practices","text":"Code should be clear, well-written and self-explanatory. Comments should be added where appropriate for any code that might be ambiguous. Function names should be descriptive, and using upper CamelCase. e.g. MyFirstFunction() MySecondFunction() Variable names should be descriptive, and start with lower camelCase. e.g. myFirstVariable mySecondVariable Code should be self-documenting and readable. After all, programming languages are supposed to be human-readable abstractions of software. Appropriate commenting. If there's a section of code that's harder to understand or hides the implementation, it's worth adding a comment about its purpose. Especially with interfaces and implementations. Use an ORM, such as Entity Framework, instead of SQL commands. An application should never run SQL commands on a database server. Functions and methods should be single-purpose, and do one thing only (Single Responsibility Principle). Sometimes Unit of Work Repositories are recommended. Code patterns should be consistent across the application. Try to follow the existing code patterns and structures as much as possible. Try to code generic operations as base classes that could be extended. This helps to minimise code duplication. Implement validation at the data model where possible, if a model exists.","title":"Coding Practices"},{"location":"MyDocs/development-standards.html#further-information","text":"Konstantin Taranov: C# Coding Standards and Naming Conventions","title":"Further Information"},{"location":"MyDocs/docker.html","text":"Docker A Docker container is a portable image that can run as a sandboxed process on a local system or a server. Containers can be managed through the Docker Dashboard application. As such, a Docker container will require its own configurations and script that enable it to run as intended. Using Docker Essentially it's a process of creating whatever project or application, as would be done normally. To make this into a Docker image, a 'Dockerfile' is added to the project. It might look something like: # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Then build the above project as a Docker container with the following command in the containing directory: $docker build -t my-project . The container should be ready to run with the following command: $docker run -dp 3000:3000 my-project The container should appear in the Docker Dashboard, where it could be managed there also. https://www.docker.com/ Docker","title":"Docker"},{"location":"MyDocs/docker.html#docker","text":"A Docker container is a portable image that can run as a sandboxed process on a local system or a server. Containers can be managed through the Docker Dashboard application. As such, a Docker container will require its own configurations and script that enable it to run as intended.","title":"Docker"},{"location":"MyDocs/docker.html#using-docker","text":"Essentially it's a process of creating whatever project or application, as would be done normally. To make this into a Docker image, a 'Dockerfile' is added to the project. It might look something like: # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Then build the above project as a Docker container with the following command in the containing directory: $docker build -t my-project . The container should be ready to run with the following command: $docker run -dp 3000:3000 my-project The container should appear in the Docker Dashboard, where it could be managed there also. https://www.docker.com/","title":"Using Docker"},{"location":"MyDocs/docker.html#docker_1","text":"","title":"Docker"},{"location":"MyDocs/github.html","text":"GitHub Each repository should have: - Name in lower-case - Description - README file - A .gitignore file Branches MAIN Release Development (e.g. mj-new-feature) Useful Commands $push dev to git $pull request into release $merge changes into release $git status $git checkout -b 'mj-new-feature' $git branch $git add . $git commit -a $git commit -m \" \" Create a new branch: $git checkout -b 'mj-new' Remove a branch: $git stash $git push :mj-new $git branch -D mj-new $git status $git checkout GitHub Wiki Home Accessibility Deploying Development Maintainance Mode Support System Information System Overview Testing","title":"GitHub"},{"location":"MyDocs/github.html#github","text":"Each repository should have: - Name in lower-case - Description - README file - A .gitignore file","title":"GitHub"},{"location":"MyDocs/github.html#branches","text":"MAIN Release Development (e.g. mj-new-feature)","title":"Branches"},{"location":"MyDocs/github.html#useful-commands","text":"$push dev to git $pull request into release $merge changes into release $git status $git checkout -b 'mj-new-feature' $git branch $git add . $git commit -a $git commit -m \" \" Create a new branch: $git checkout -b 'mj-new' Remove a branch: $git stash $git push :mj-new $git branch -D mj-new $git status $git checkout","title":"Useful Commands"},{"location":"MyDocs/github.html#github-wiki","text":"Home Accessibility Deploying Development Maintainance Mode Support System Information System Overview Testing","title":"GitHub Wiki"},{"location":"MyDocs/identity-users-extension.html","text":"Extending ASP.NET IdentityUsers I\u2019ve used the default ASP.NET IdentityModels to initialise a Code-First schema on a local database server. Anyone running the application after that could register an email address and password combination, login using that and manage his/her account. What if we wanted to extend this to include a user name, or some other field? Well, first the extra field, in this case \u2018Name\u2019, must be added to the ApplicationUser class in IdentityModels.cs : public class ApplicationUser:IdentityUser { [StringLength(160)] public string Name { get; set; } public async Task<ClaimsIdentity> GenerateUserIdentityAsync(UserManager)<ApplicationUser>manager) { var userIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); return userIdentity; } } To this property I\u2019ve added the \u2018 StringLength() \u2018 attribute, since I want to limit the user name to 160 characters \u2013 this limit is applied to the database column when the migration is run. If you want to list users in the application view, the names can be referenced using something like \u2018 @item.CalendarUser.Name \u2018. Users will need to add their (user)names on the Register page, in addition to their email addresses and passwords. To enable this, we can add the \u2018 Name \u2018 property to AccountViewModel.cs , under the RegisterViewModel class: public class RegisterViewModel { [Required] [StringLength(160)] [Display(Name=\"Name\"] public string Name { get; set; } ... } Modifying the code in /Views/Account/Register.cshtml is a simple matter of copying and pasting one of the form elements and renaming it. The Register action in AccountController.cs also needs to be modified, so the Register page will post back the user\u2019s registered name: public async Task<ActionResult> Register(RegisterViewModel model) { if (ModelState.IsValid) { var user = new ApplicationUser { UserName = model.Email, Email = model.Email, Name = model.Name } var result = await UserManager.CreateAsync(user, model.Password); if (result.Succeeded) { await SignInManager.SignInAsync(user, isPersistent:false, rememberBrowser.false); return RedirectToAction(\"Index\", \"Home\"); } AddErrors(result); } } All I\u2019ve really done to the default method is add \u2018 Name = model.Name \u2018 to the instance of ApplicationUser . In the Package Manager Console, add this change as a migration script, and update the database. You might want to populate the \u2018Name\u2019 field in the table in the Server Explorer manually. DotNet #Identity #Security #Authentication","title":"Extending ASP.NET IdentityUsers"},{"location":"MyDocs/identity-users-extension.html#extending-aspnet-identityusers","text":"I\u2019ve used the default ASP.NET IdentityModels to initialise a Code-First schema on a local database server. Anyone running the application after that could register an email address and password combination, login using that and manage his/her account. What if we wanted to extend this to include a user name, or some other field? Well, first the extra field, in this case \u2018Name\u2019, must be added to the ApplicationUser class in IdentityModels.cs : public class ApplicationUser:IdentityUser { [StringLength(160)] public string Name { get; set; } public async Task<ClaimsIdentity> GenerateUserIdentityAsync(UserManager)<ApplicationUser>manager) { var userIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); return userIdentity; } } To this property I\u2019ve added the \u2018 StringLength() \u2018 attribute, since I want to limit the user name to 160 characters \u2013 this limit is applied to the database column when the migration is run. If you want to list users in the application view, the names can be referenced using something like \u2018 @item.CalendarUser.Name \u2018. Users will need to add their (user)names on the Register page, in addition to their email addresses and passwords. To enable this, we can add the \u2018 Name \u2018 property to AccountViewModel.cs , under the RegisterViewModel class: public class RegisterViewModel { [Required] [StringLength(160)] [Display(Name=\"Name\"] public string Name { get; set; } ... } Modifying the code in /Views/Account/Register.cshtml is a simple matter of copying and pasting one of the form elements and renaming it. The Register action in AccountController.cs also needs to be modified, so the Register page will post back the user\u2019s registered name: public async Task<ActionResult> Register(RegisterViewModel model) { if (ModelState.IsValid) { var user = new ApplicationUser { UserName = model.Email, Email = model.Email, Name = model.Name } var result = await UserManager.CreateAsync(user, model.Password); if (result.Succeeded) { await SignInManager.SignInAsync(user, isPersistent:false, rememberBrowser.false); return RedirectToAction(\"Index\", \"Home\"); } AddErrors(result); } } All I\u2019ve really done to the default method is add \u2018 Name = model.Name \u2018 to the instance of ApplicationUser . In the Package Manager Console, add this change as a migration script, and update the database. You might want to populate the \u2018Name\u2019 field in the table in the Server Explorer manually.","title":"Extending ASP.NET IdentityUsers"},{"location":"MyDocs/identity-users-extension.html#dotnet-identity-security-authentication","text":"","title":"DotNet #Identity #Security #Authentication"},{"location":"MyDocs/json-web-tokens.html","text":"JSON Web Tokens An authentication server provides a client with a JSON Web Token , and the authentication server will also handle requests from application servers to verify tokens supplied by clients. Algorithm Generally, the token generation code follows this process: Initialise the token header as a set of JSON objects. Initialise the token payload as a set of JSON objects. Base64-encode the header and payload independently. This gives us the unsigned token. Using the HMAC SHA256 algorithm, and a secret key value, generate the signature for the unsigned payload. A token, therefore, will have three segments: a) header, b) payload, and c) signature. The first two are Base64-encoded JSON objects, and the third is the encoded signature of the first two segments. This can be seen in action when generating a token with the debugger at jwt.io: https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5 cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gR G9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36P Ok6yJV_adQssw5c When decoded, the token would look something like this: { \"alg\": \"HS256\", \"typ\": \"JWT\" } { \"sub\": \"Web Application\", \"name\": \"Michael\", \"iat\": 45435334 } { SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c } .NET Implementation When working with .NET, one could use System.IdentityModel.Tokens.Jwt , but it can also be done using Newtonsoft.Json and System.Security.Cryptography . I defined the header properties in a TokenHeader class: public class TokenHeader { public string alg { get; set; } public string typ { get; set; } } Instantiating the TokenHeader class in the main method, we can declare static variables for the objects: TokenHeader tokenHeader = new TokenHeader(); tokenHeader.alg = \"HS256\"; tokenHeader.typ = \"JWT\"; The variables are formatted as JSON objects, then Base64-encoded: string headerString = JsonConvert.SerializeObject(tokenHeader); string encodedHeaderString = ConvertHeaderToJson(headerString); private static string ConvertHeaderToJson(string headerString) { byte[] headerStringAsBytes = Encoding.ASCII.GetBytes(headerString); string encodedHeaderString = Convert.ToBase64String(headerStringAsBytes); return encodedHeaderString; } For generating the third segment of the token \u2013 the signature \u2013 the process is different. In this case, we need to generate a hash digest of the header and payload, along with a 256-bit secret value, then encode that digest. The algorithm for this is given as: HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), your-256-bit-secret ) This algorithm is implemented by the following code: string message = encodedHeaderString + \".\" + encodedPayloadString; System.Text.ASCIIEncoding encoding = new System.Text.ASCIIEncoding(); byte[] keyByte = encoding.GetBytes(myKey); HMACSHA256 hmacsha256 = new HMACSHA256(keyByte); byte[] messageBytes = encoding.GetBytes(message); byte[] hashmessage = hmacsha256.ComputeHash(messageBytes); string hashAsString = BitConverter.ToString(hashmessage); byte[] signatureStringAsBytes = Encoding.ASCII.GetBytes(hashAsString); string encodedSignature = Convert.ToBase64String(signatureStringAsBytes); return encodedSignature; JavaScript Implementation Several JavaScript libraries are needed for the implementation here. The crypto ones are referenced on Joe Kampschmidt\u2019s blog : jquery-3.3.1.min.js jquery.base64.min.js crypto-js.min.js hmac-sha256.min.js enc-base64.min.js To initialise the header and payload as JSON objects, JSON.parse() is used. var headerjson = '{ \"alg\": \"HS256\", \"typ\": \"JWT\" }', headerobj = JSON.parse(headerjson); var payloadjson = '{ \"sub\": \"x555\", \"name\": \"Michael\", \"iat\": 445644543534, \"issued\": ' + Math.floor(Date.now() / 1000) + ' }', payloadobj = JSON.parse(payloadjson); Next, independently encode the header and the payload: headerb64 = btoa(unescape(encodeURIComponent(headerjson))); headerb64str = decodeURIComponent(escape(window.atob(headerb64))); payloadb64 = btoa(unescape(encodeURIComponent(payloadjson))); payloadb64str = decodeURIComponent(escape(window.atob(payloadb64))); The unsigned token will consist of two segments, one for the encoded header and the other the encoded payload: var unsignedToken = headerb64 + \".\" + payloadb64; Finally, the crypto libraries provide the HMAC SHA256 algorithm used here for signing the token: var secretKey = \"Password1\"; var tokenSignature = CryptoJS.HmacSHA256(unsignedToken, secretKey); var encodedTokenSignature = CryptoJS.enc.Base64.stringify(tokenSignature); gentoken.value = encodedTokenSignature; And finally, I\u2019ve put all three segments together to form the signed token: allsegments.value = headerb64 + \".\" + payloadb64 + \".\" + encodedTokenSignature;","title":"JSON Web Tokens"},{"location":"MyDocs/json-web-tokens.html#json-web-tokens","text":"An authentication server provides a client with a JSON Web Token , and the authentication server will also handle requests from application servers to verify tokens supplied by clients.","title":"JSON Web Tokens"},{"location":"MyDocs/json-web-tokens.html#algorithm","text":"Generally, the token generation code follows this process: Initialise the token header as a set of JSON objects. Initialise the token payload as a set of JSON objects. Base64-encode the header and payload independently. This gives us the unsigned token. Using the HMAC SHA256 algorithm, and a secret key value, generate the signature for the unsigned payload. A token, therefore, will have three segments: a) header, b) payload, and c) signature. The first two are Base64-encoded JSON objects, and the third is the encoded signature of the first two segments. This can be seen in action when generating a token with the debugger at jwt.io: https://jwt.io/#debugger-io?token=eyJhbGciOiJIUzI1NiIsInR5 cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gR G9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36P Ok6yJV_adQssw5c When decoded, the token would look something like this: { \"alg\": \"HS256\", \"typ\": \"JWT\" } { \"sub\": \"Web Application\", \"name\": \"Michael\", \"iat\": 45435334 } { SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c } .NET Implementation When working with .NET, one could use System.IdentityModel.Tokens.Jwt , but it can also be done using Newtonsoft.Json and System.Security.Cryptography . I defined the header properties in a TokenHeader class: public class TokenHeader { public string alg { get; set; } public string typ { get; set; } } Instantiating the TokenHeader class in the main method, we can declare static variables for the objects: TokenHeader tokenHeader = new TokenHeader(); tokenHeader.alg = \"HS256\"; tokenHeader.typ = \"JWT\"; The variables are formatted as JSON objects, then Base64-encoded: string headerString = JsonConvert.SerializeObject(tokenHeader); string encodedHeaderString = ConvertHeaderToJson(headerString); private static string ConvertHeaderToJson(string headerString) { byte[] headerStringAsBytes = Encoding.ASCII.GetBytes(headerString); string encodedHeaderString = Convert.ToBase64String(headerStringAsBytes); return encodedHeaderString; } For generating the third segment of the token \u2013 the signature \u2013 the process is different. In this case, we need to generate a hash digest of the header and payload, along with a 256-bit secret value, then encode that digest. The algorithm for this is given as: HMACSHA256( base64UrlEncode(header) + \".\" + base64UrlEncode(payload), your-256-bit-secret ) This algorithm is implemented by the following code: string message = encodedHeaderString + \".\" + encodedPayloadString; System.Text.ASCIIEncoding encoding = new System.Text.ASCIIEncoding(); byte[] keyByte = encoding.GetBytes(myKey); HMACSHA256 hmacsha256 = new HMACSHA256(keyByte); byte[] messageBytes = encoding.GetBytes(message); byte[] hashmessage = hmacsha256.ComputeHash(messageBytes); string hashAsString = BitConverter.ToString(hashmessage); byte[] signatureStringAsBytes = Encoding.ASCII.GetBytes(hashAsString); string encodedSignature = Convert.ToBase64String(signatureStringAsBytes); return encodedSignature;","title":"Algorithm"},{"location":"MyDocs/json-web-tokens.html#javascript-implementation","text":"Several JavaScript libraries are needed for the implementation here. The crypto ones are referenced on Joe Kampschmidt\u2019s blog : jquery-3.3.1.min.js jquery.base64.min.js crypto-js.min.js hmac-sha256.min.js enc-base64.min.js To initialise the header and payload as JSON objects, JSON.parse() is used. var headerjson = '{ \"alg\": \"HS256\", \"typ\": \"JWT\" }', headerobj = JSON.parse(headerjson); var payloadjson = '{ \"sub\": \"x555\", \"name\": \"Michael\", \"iat\": 445644543534, \"issued\": ' + Math.floor(Date.now() / 1000) + ' }', payloadobj = JSON.parse(payloadjson); Next, independently encode the header and the payload: headerb64 = btoa(unescape(encodeURIComponent(headerjson))); headerb64str = decodeURIComponent(escape(window.atob(headerb64))); payloadb64 = btoa(unescape(encodeURIComponent(payloadjson))); payloadb64str = decodeURIComponent(escape(window.atob(payloadb64))); The unsigned token will consist of two segments, one for the encoded header and the other the encoded payload: var unsignedToken = headerb64 + \".\" + payloadb64; Finally, the crypto libraries provide the HMAC SHA256 algorithm used here for signing the token: var secretKey = \"Password1\"; var tokenSignature = CryptoJS.HmacSHA256(unsignedToken, secretKey); var encodedTokenSignature = CryptoJS.enc.Base64.stringify(tokenSignature); gentoken.value = encodedTokenSignature; And finally, I\u2019ve put all three segments together to form the signed token: allsegments.value = headerb64 + \".\" + payloadb64 + \".\" + encodedTokenSignature;","title":"JavaScript Implementation"},{"location":"MyDocs/jupyter.html","text":"Jupyter Notebooks Although Jupyter Notebook is designed primarily for data analysts, I think there are other use cases in which it can be an effective method of distributing Python code and associated documentation in a single .ipynb file. There are other editors we can use to handle .ipynb files: JupyterLab, Deepnote and Visual Studio Code (with the right extensions) are just a few I've tried. Jupyter Notebook is essentially a markdown editor with an embedded Python interpreter that executes whatever code is included in a document. All the code within a document can be run as a single Python script, but it's possible to run the contents of specific code blocks. I got started by installing the Anaconda Navigator . Anaconda appears to be a Web server that hosts a range of data analysis applications, including Jupyter Notebook and JupyterLab, a Python interpreter and a large collection of modules that a data analyst might use. It can also launch the Spyder IDE and PyQT Console. To get started, run the Anaconda Navigator, and in the options select ' Jupyter Notebook '. The application's interface is accessible in a Web browser at localhost:8888. All the mathematical and arithmetic things are done the same way as with any Python-capable IDE, because that's essentially what's happening here. I haven't tried using specialist Python modules (e.g. networking, cryptography, etc.) with Jupyter yet, though. There are functions that are particularly relevant for working with data sets. To find the max and min values in a set of variables: min(myFirstVariable, mySecondVariable, myThirdVariable) max(myFirstVariable, mySecondVariable, myThirdVariable) And use this to find the range of the set: largest = max(myFirstVariable, mySecondVariable, myThirdVariable) smallest = min(myFirstVariable, mySecondVariable, myThirdVariablel) mySetRange = largest - smallest mySetRange However, a more efficient way of dealing with relatively small data sets might be to use arrays: dataSet = [2312, 1339, 9878, 4521] average = sum(dataSet)/len(dataSet) average for i in dataSet: print(i) If we wanted to find the median of the set, we might be better off using the median() function from the statistics module: import statistics as stats stats.median(dataSet) Loading JSON Arrays What about if we really wanted to work with data sets with real-world data, and use Jupyter/Python to query data sources with numerous records? One method is to read it from a JSON array. jsonData = { 'FirstVariable' : {'name': 'First Variable', 'value' : 498 }, 'SecondVariable' : {'name': 'Second Variable', 'value' : 677 }, 'ThirdVariable' : {'name': 'Third Variable', 'value' : 121 }} jsonData To retrieve a specific named element: jsonData['SecondVariable'] We can load JSON from an external file: import json jsonFile = open(\"sample.json\",) data = json.load(jsonFile) data Importing Tables Of course, Jupyter Notebook wouldn't be of much use if it could work only with a limited amount of data. Fortunately there are ways of importing and working with much larger data sets, using the pandas module. To read a spreadsheet file and display its contents in Jupyter: from pandas import * data = read_excel('MySpreadsheet.xls') data The table that appears is called a 'dataframe'. You'll probably want to work on data in a given column. The following code declares tbColumn as an array of values in the spreadsheet's Value column. tbColumn = data['Value'] tbColumn The output will be something like: 0 2343 1 23653 2 5312 3 9884 Name: Value, dtype: int64 So, tbColumn is an array of int64 values. We can, of course, retrieve whatever values we want from this, in the usual way. e.g. dbColumn[3] There are a few handy things we can do with an array in Python: tbColumn.sum() tbColumn.min() tbColumn.max() tbColumn.mean() tbColumn.median() We also have a table sorting function, so we can sort rows by the values in a given column: data.sort_values('Value') CSV Data Sources and Matplotlib - A Real-World Example I've used using pandas to read and query real-world data provided by Our World in Data as .csv files - the ones used here were downloaded on 26th November. The first data set is ' UK: Daily new confirmed COVID-19 cases per 100,000 '. from pandas import * csvdata = read_csv('covid-cases.csv') As expected, there was a large number of records, and they were for each region in Britain. I wanted just the stats for Wales in 2021. It was at this point that I discovered there's a pandas.query() function that enables us to use SQL-like syntax for this. csvdata = read_csv('covid-cases.csv') filteredData = csvdata.query(\"(Entity=='Wales') and (Day > '2021-01-01')\") filteredData For some very basic data visualisation, I used Matplotlib , setting the two columns in my filtered data as the x and y axes. import matplotlib.pyplot as plt x = filteredData.Day y = filteredData.daily_cases_rate_rolling_average The same could be done with the second data set, which is for ' UK: Number of COVID-19 patients in hospital '. It's very important to be very careful when naming the variables here, to avoid accidentally reading, querying or rendering from the previous data set (Note: I didn't filter for 2021 data in this second example). import matplotlib.pyplot as plt2 csvpatients = read_csv('covid-hospital.csv') filteredHospitalData = csvpatients.query(\"(Entity=='Wales') and (Day > '2020-01-01')\") xh = filteredHospitalData.Day yh = filteredHospitalData.people_in_hospital plt2.plot(xh, yh) plt2.show() The .ipynb and .csv files used here can be downloaded from my GitHub repo .","title":"Jupyter Notebooks"},{"location":"MyDocs/jupyter.html#jupyter-notebooks","text":"Although Jupyter Notebook is designed primarily for data analysts, I think there are other use cases in which it can be an effective method of distributing Python code and associated documentation in a single .ipynb file. There are other editors we can use to handle .ipynb files: JupyterLab, Deepnote and Visual Studio Code (with the right extensions) are just a few I've tried. Jupyter Notebook is essentially a markdown editor with an embedded Python interpreter that executes whatever code is included in a document. All the code within a document can be run as a single Python script, but it's possible to run the contents of specific code blocks. I got started by installing the Anaconda Navigator . Anaconda appears to be a Web server that hosts a range of data analysis applications, including Jupyter Notebook and JupyterLab, a Python interpreter and a large collection of modules that a data analyst might use. It can also launch the Spyder IDE and PyQT Console. To get started, run the Anaconda Navigator, and in the options select ' Jupyter Notebook '. The application's interface is accessible in a Web browser at localhost:8888. All the mathematical and arithmetic things are done the same way as with any Python-capable IDE, because that's essentially what's happening here. I haven't tried using specialist Python modules (e.g. networking, cryptography, etc.) with Jupyter yet, though. There are functions that are particularly relevant for working with data sets. To find the max and min values in a set of variables: min(myFirstVariable, mySecondVariable, myThirdVariable) max(myFirstVariable, mySecondVariable, myThirdVariable) And use this to find the range of the set: largest = max(myFirstVariable, mySecondVariable, myThirdVariable) smallest = min(myFirstVariable, mySecondVariable, myThirdVariablel) mySetRange = largest - smallest mySetRange However, a more efficient way of dealing with relatively small data sets might be to use arrays: dataSet = [2312, 1339, 9878, 4521] average = sum(dataSet)/len(dataSet) average for i in dataSet: print(i) If we wanted to find the median of the set, we might be better off using the median() function from the statistics module: import statistics as stats stats.median(dataSet)","title":"Jupyter Notebooks"},{"location":"MyDocs/jupyter.html#loading-json-arrays","text":"What about if we really wanted to work with data sets with real-world data, and use Jupyter/Python to query data sources with numerous records? One method is to read it from a JSON array. jsonData = { 'FirstVariable' : {'name': 'First Variable', 'value' : 498 }, 'SecondVariable' : {'name': 'Second Variable', 'value' : 677 }, 'ThirdVariable' : {'name': 'Third Variable', 'value' : 121 }} jsonData To retrieve a specific named element: jsonData['SecondVariable'] We can load JSON from an external file: import json jsonFile = open(\"sample.json\",) data = json.load(jsonFile) data","title":"Loading JSON Arrays"},{"location":"MyDocs/jupyter.html#importing-tables","text":"Of course, Jupyter Notebook wouldn't be of much use if it could work only with a limited amount of data. Fortunately there are ways of importing and working with much larger data sets, using the pandas module. To read a spreadsheet file and display its contents in Jupyter: from pandas import * data = read_excel('MySpreadsheet.xls') data The table that appears is called a 'dataframe'. You'll probably want to work on data in a given column. The following code declares tbColumn as an array of values in the spreadsheet's Value column. tbColumn = data['Value'] tbColumn The output will be something like: 0 2343 1 23653 2 5312 3 9884 Name: Value, dtype: int64 So, tbColumn is an array of int64 values. We can, of course, retrieve whatever values we want from this, in the usual way. e.g. dbColumn[3] There are a few handy things we can do with an array in Python: tbColumn.sum() tbColumn.min() tbColumn.max() tbColumn.mean() tbColumn.median() We also have a table sorting function, so we can sort rows by the values in a given column: data.sort_values('Value')","title":"Importing Tables"},{"location":"MyDocs/jupyter.html#csv-data-sources-and-matplotlib-a-real-world-example","text":"I've used using pandas to read and query real-world data provided by Our World in Data as .csv files - the ones used here were downloaded on 26th November. The first data set is ' UK: Daily new confirmed COVID-19 cases per 100,000 '. from pandas import * csvdata = read_csv('covid-cases.csv') As expected, there was a large number of records, and they were for each region in Britain. I wanted just the stats for Wales in 2021. It was at this point that I discovered there's a pandas.query() function that enables us to use SQL-like syntax for this. csvdata = read_csv('covid-cases.csv') filteredData = csvdata.query(\"(Entity=='Wales') and (Day > '2021-01-01')\") filteredData For some very basic data visualisation, I used Matplotlib , setting the two columns in my filtered data as the x and y axes. import matplotlib.pyplot as plt x = filteredData.Day y = filteredData.daily_cases_rate_rolling_average The same could be done with the second data set, which is for ' UK: Number of COVID-19 patients in hospital '. It's very important to be very careful when naming the variables here, to avoid accidentally reading, querying or rendering from the previous data set (Note: I didn't filter for 2021 data in this second example). import matplotlib.pyplot as plt2 csvpatients = read_csv('covid-hospital.csv') filteredHospitalData = csvpatients.query(\"(Entity=='Wales') and (Day > '2020-01-01')\") xh = filteredHospitalData.Day yh = filteredHospitalData.people_in_hospital plt2.plot(xh, yh) plt2.show() The .ipynb and .csv files used here can be downloaded from my GitHub repo .","title":"CSV Data Sources and Matplotlib - A Real-World Example"},{"location":"MyDocs/linux-kernel-module.html","text":"Making a Linux Kernel Module I set myself the task of adding code to the Linux Kernel by writing a loadable kernel module , since I\u2019m trying to get back into C programming. Recap There are two main types of kernel module: static and loadable . The static modules are added into the kernel as its compiled. Loadable modules, on the other hand, are dynamic, and they\u2019re loaded/unloaded during runtime. When loaded, they pretty much become part of the kernel. There are two locations in the file system to note: /lib/modules is where the kernel modules are stored. There are subdirectories for the different types. /etc/modules lists the modules to load during system boot. A standard Linux installation also ships with something called modutils , that is a group of programs for manipulating loadable kernel modules (LKMs): - insmod: Load module - rmmod: Unload module - depmod: Find dependencies - kerneld: Kernel daemon - ksyms: Show exported symbols - lsmod: List loaded modules - modinfo: Show module information - modprobe: Load module and resolve dependencies But these can only be used with whatever\u2019s already in the file system. Structure of a Kernel Module A module is created a little differently from the typical C program. Whereas the typical program has a main() function, these have entry and exit functions, module_init() and module_exit() , as they must receive and hand over kernel execution. These correspond to the loading and unloading from kernel\u2019s memory space, and they probably invoke whatever system calls for sorting out memory allocation. Writing the Module The concept is simple: we merely write a C program, compile it, then use the insmod command. A simple \u2018Hello World\u2019 program is all we need to get things going, as the learning curve here is in getting our own module successfully loaded. First we might need to install kernel headers, or kernel-dev , from the distribution\u2019s repository. Other tutorials recommend compiling and running from the most recent kernel. The following is an edited \u2018Hello World\u2019 example from The Linux Documentation Project : #include <linux/module.h> /* Needed by all modules */ #include <linux/kernel.h> /* Needed for KERN_INFO */ int init_module(void) { printk(KERN_INFO \"Hello world 1.\\n\"); */ Non-0 return if init_module failed;*/ return 0; } void cleanup_module(void) { printk(KERN_INFO \"Goodbye world 1.\\n\"); } We might think the next step is a matter of putting the code in an IDE, clicking compile/build, then loading the resulting object file. Unfortunately it\u2019s not that simple. If we try, the compiler will throw up an error because it can\u2019t find the headers to include. Also, the compiler links code to whatever objects are available in the standard library, and the objects for this module aren\u2019t available until it\u2019s loaded into kernel space. Instead, we use a \u2018makefile\u2019. Create a new directory (here I\u2019ve called it \u2018 MyModule \u2018). This will contain mymodule.c and the makefile. Next, open a new file in the IDE or text editor, then add the following: obj-m += mymodule.o all: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean Save this as \u2018 Makefile \u2018, in the directory that was just created. Now that directory will contain mymodule.c and Makefile . The next step is to build this using \u2018 make \u2018. The output should be something along the lines of: michae;@scarbrough ~/MyModule $make make -C /lib/modules/3.2.0-generic/build M=/home/michael/MyModule modules make[1]: Entering directory '/usr/src/linux-headers-3.2.0-generic CC [M] /home/michael/MyModule/mymodule.o Building modules, stage 2. MODPOST 1 modules CC /home/michael/MyModule/mymodule.mod.o LD [M] make[1]: Leaving directory '/usr/src/linux-headers-3.2.0-generic The directory should now be populated by the newly-created module and several other files. The module here is mymodule.ko . The next step is to load mymodule.ko into the kernel\u2019s memory space using insmod() as root. $su root #insmod mymodule.ko Since I used \u2018 printk \u2018 and not \u2018 printf \u2018, nothing was printed to the screen. How can we determine whether it was successfully loaded? Use lsmod , of course: And by checking syslog , we should find that the module has indeed printed a kernel message: #cd /var/log #cat syslog The entry looking something like: Sep 29 23:12:05 BEATRIX kernel: [19161.607371] Hello world 1. The module doesn\u2019t do much, but the important thing is we managed to create a module and load it. Practical Uses and Security Implications By writing our own kernel modules, we can extend the functionality of the kernel, add our own drivers, interfaces and features, and as we\u2019ve seen, a kernel module can be any C program created to perform any conceivable function. On a standard Linux installation there could be hundreds of them, even if only a fraction are ever used. A malicious program, for example a kernel-mode rootkit, could easily be hidden anywhere here. Next we could look in /etc/modules to see which of them load when the system boots. This seems the perfect way to hide a rootkit, short of compiling it into the kernel itself.","title":"Linux Kernel Modules"},{"location":"MyDocs/linux-kernel-module.html#making-a-linux-kernel-module","text":"I set myself the task of adding code to the Linux Kernel by writing a loadable kernel module , since I\u2019m trying to get back into C programming.","title":"Making a Linux Kernel Module"},{"location":"MyDocs/linux-kernel-module.html#recap","text":"There are two main types of kernel module: static and loadable . The static modules are added into the kernel as its compiled. Loadable modules, on the other hand, are dynamic, and they\u2019re loaded/unloaded during runtime. When loaded, they pretty much become part of the kernel. There are two locations in the file system to note: /lib/modules is where the kernel modules are stored. There are subdirectories for the different types. /etc/modules lists the modules to load during system boot. A standard Linux installation also ships with something called modutils , that is a group of programs for manipulating loadable kernel modules (LKMs): - insmod: Load module - rmmod: Unload module - depmod: Find dependencies - kerneld: Kernel daemon - ksyms: Show exported symbols - lsmod: List loaded modules - modinfo: Show module information - modprobe: Load module and resolve dependencies But these can only be used with whatever\u2019s already in the file system.","title":"Recap"},{"location":"MyDocs/linux-kernel-module.html#structure-of-a-kernel-module","text":"A module is created a little differently from the typical C program. Whereas the typical program has a main() function, these have entry and exit functions, module_init() and module_exit() , as they must receive and hand over kernel execution. These correspond to the loading and unloading from kernel\u2019s memory space, and they probably invoke whatever system calls for sorting out memory allocation.","title":"Structure of a Kernel Module"},{"location":"MyDocs/linux-kernel-module.html#writing-the-module","text":"The concept is simple: we merely write a C program, compile it, then use the insmod command. A simple \u2018Hello World\u2019 program is all we need to get things going, as the learning curve here is in getting our own module successfully loaded. First we might need to install kernel headers, or kernel-dev , from the distribution\u2019s repository. Other tutorials recommend compiling and running from the most recent kernel. The following is an edited \u2018Hello World\u2019 example from The Linux Documentation Project : #include <linux/module.h> /* Needed by all modules */ #include <linux/kernel.h> /* Needed for KERN_INFO */ int init_module(void) { printk(KERN_INFO \"Hello world 1.\\n\"); */ Non-0 return if init_module failed;*/ return 0; } void cleanup_module(void) { printk(KERN_INFO \"Goodbye world 1.\\n\"); } We might think the next step is a matter of putting the code in an IDE, clicking compile/build, then loading the resulting object file. Unfortunately it\u2019s not that simple. If we try, the compiler will throw up an error because it can\u2019t find the headers to include. Also, the compiler links code to whatever objects are available in the standard library, and the objects for this module aren\u2019t available until it\u2019s loaded into kernel space. Instead, we use a \u2018makefile\u2019. Create a new directory (here I\u2019ve called it \u2018 MyModule \u2018). This will contain mymodule.c and the makefile. Next, open a new file in the IDE or text editor, then add the following: obj-m += mymodule.o all: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean Save this as \u2018 Makefile \u2018, in the directory that was just created. Now that directory will contain mymodule.c and Makefile . The next step is to build this using \u2018 make \u2018. The output should be something along the lines of: michae;@scarbrough ~/MyModule $make make -C /lib/modules/3.2.0-generic/build M=/home/michael/MyModule modules make[1]: Entering directory '/usr/src/linux-headers-3.2.0-generic CC [M] /home/michael/MyModule/mymodule.o Building modules, stage 2. MODPOST 1 modules CC /home/michael/MyModule/mymodule.mod.o LD [M] make[1]: Leaving directory '/usr/src/linux-headers-3.2.0-generic The directory should now be populated by the newly-created module and several other files. The module here is mymodule.ko . The next step is to load mymodule.ko into the kernel\u2019s memory space using insmod() as root. $su root #insmod mymodule.ko Since I used \u2018 printk \u2018 and not \u2018 printf \u2018, nothing was printed to the screen. How can we determine whether it was successfully loaded? Use lsmod , of course: And by checking syslog , we should find that the module has indeed printed a kernel message: #cd /var/log #cat syslog The entry looking something like: Sep 29 23:12:05 BEATRIX kernel: [19161.607371] Hello world 1. The module doesn\u2019t do much, but the important thing is we managed to create a module and load it.","title":"Writing the Module"},{"location":"MyDocs/linux-kernel-module.html#practical-uses-and-security-implications","text":"By writing our own kernel modules, we can extend the functionality of the kernel, add our own drivers, interfaces and features, and as we\u2019ve seen, a kernel module can be any C program created to perform any conceivable function. On a standard Linux installation there could be hundreds of them, even if only a fraction are ever used. A malicious program, for example a kernel-mode rootkit, could easily be hidden anywhere here. Next we could look in /etc/modules to see which of them load when the system boots. This seems the perfect way to hide a rootkit, short of compiling it into the kernel itself.","title":"Practical Uses and Security Implications"},{"location":"MyDocs/opencart.html","text":"OpenCart Development Notes Essentially OpenCart is a PHP-based platform that typically runs on a LAMP stack. I have, however, been managing it through cPanel on a conventional hosting plan. Application changes will entail the installation, and perhaps the creation or modification of modules. Operating an OpenCart portal is therefore best done with the support of PHP developers who understand something of the MVC pattern. The platform must be installed on a Web server, which means backing up the application state and data, contingency planning and security are the responsibility of the business. The dashboard is easy enough to navigate, but it's the files that make up the platform that we need to understand. The application follows an MVC structure, but there are two main sections: The /admin directory contains the source files for the administration dashboard, and the /catalogue directory has the source for what the site's vistors are presented with. Application Structure Files are organised into /admin and /catalog directories. /admin /controller /view /catalog /controller /view The scripts in /admin determine what site admins see in the dashboard. Scripts in the /catalog directory determine what others see when visiting the site. Defined by .twig Files: - The structure of the site's interface. By default, it defines a layout with a header, body, left column, right column and footer. Defined by modules: - The modules define what appears within each element (e.g. header or body) of the site's interface. Module Code Template class ControllerExtensionModule MyModule extends Controller { private $error = array(); public function index() {} public function validate() {} public function install() {} public function uninstall() {} } Five Layer Categories Module /admin - Enable drag-and drop or category customisation in the dashboard. - Also features to enable the management of categories. /catalogue - Show category groups and entries on the page. * CategoryManager.php Viewing Changes to the Front End The theme cache must be refreshed in order to view application changes. In the Dashboard tab, click the blue cog button at the top right. In the Developer Settings modal, clich the Refresh button for the theme. Any additional JavaScript funtions and AJAX calls to be added to common.js . Backing Up There are a couple of things that should be backed up. One of these is the application state - everything in the home directory that's accessed through cPanel or FTP. The OpenCart dashboard allows a backup to be made of the store's database. This can quickly be restored using the Import feature. System -> Maintainence -> Backup/Restore Case Study in What Could Go Wrong A dodgy module was installed through the module repository. After installation this screwed up OpenCart core files, causing main features of the application to fail. Resolvving this entailed repairing an incorrect reference to a system file in the vqmod cache code. The model also needed to be repaired. Had to copy a system file from a backup to the current installation. Troubleshooting See the error log in /home/user/logs . This will indicate the cause of an error. Sometimes the log entries indicate a PHP error. Fixing this should require a minor change to the code. Another useful source of diagnostic information is the Developer tool in the browser. This will reveal the server response codes to a request. - 200: Okay - 300: Usually a routing problem - 400: Missing file(s) - 500: Server configuration problem Settings DB Table setting_id store_id (Default value is '0') group key value serialized Look at the Data tab in Categories , under Catalogue . The Columns field might allow the number of subcategories to be set.","title":"OpenCart Development Notes"},{"location":"MyDocs/opencart.html#opencart-development-notes","text":"Essentially OpenCart is a PHP-based platform that typically runs on a LAMP stack. I have, however, been managing it through cPanel on a conventional hosting plan. Application changes will entail the installation, and perhaps the creation or modification of modules. Operating an OpenCart portal is therefore best done with the support of PHP developers who understand something of the MVC pattern. The platform must be installed on a Web server, which means backing up the application state and data, contingency planning and security are the responsibility of the business. The dashboard is easy enough to navigate, but it's the files that make up the platform that we need to understand. The application follows an MVC structure, but there are two main sections: The /admin directory contains the source files for the administration dashboard, and the /catalogue directory has the source for what the site's vistors are presented with.","title":"OpenCart Development Notes"},{"location":"MyDocs/opencart.html#application-structure","text":"Files are organised into /admin and /catalog directories. /admin /controller /view /catalog /controller /view The scripts in /admin determine what site admins see in the dashboard. Scripts in the /catalog directory determine what others see when visiting the site. Defined by .twig Files: - The structure of the site's interface. By default, it defines a layout with a header, body, left column, right column and footer. Defined by modules: - The modules define what appears within each element (e.g. header or body) of the site's interface.","title":"Application Structure"},{"location":"MyDocs/opencart.html#module-code-template","text":"class ControllerExtensionModule MyModule extends Controller { private $error = array(); public function index() {} public function validate() {} public function install() {} public function uninstall() {} }","title":"Module Code Template"},{"location":"MyDocs/opencart.html#five-layer-categories-module","text":"/admin - Enable drag-and drop or category customisation in the dashboard. - Also features to enable the management of categories. /catalogue - Show category groups and entries on the page. * CategoryManager.php","title":"Five Layer Categories Module"},{"location":"MyDocs/opencart.html#viewing-changes-to-the-front-end","text":"The theme cache must be refreshed in order to view application changes. In the Dashboard tab, click the blue cog button at the top right. In the Developer Settings modal, clich the Refresh button for the theme. Any additional JavaScript funtions and AJAX calls to be added to common.js .","title":"Viewing Changes to the Front End"},{"location":"MyDocs/opencart.html#backing-up","text":"There are a couple of things that should be backed up. One of these is the application state - everything in the home directory that's accessed through cPanel or FTP. The OpenCart dashboard allows a backup to be made of the store's database. This can quickly be restored using the Import feature. System -> Maintainence -> Backup/Restore","title":"Backing Up"},{"location":"MyDocs/opencart.html#case-study-in-what-could-go-wrong","text":"A dodgy module was installed through the module repository. After installation this screwed up OpenCart core files, causing main features of the application to fail. Resolvving this entailed repairing an incorrect reference to a system file in the vqmod cache code. The model also needed to be repaired. Had to copy a system file from a backup to the current installation.","title":"Case Study in What Could Go Wrong"},{"location":"MyDocs/opencart.html#troubleshooting","text":"See the error log in /home/user/logs . This will indicate the cause of an error. Sometimes the log entries indicate a PHP error. Fixing this should require a minor change to the code. Another useful source of diagnostic information is the Developer tool in the browser. This will reveal the server response codes to a request. - 200: Okay - 300: Usually a routing problem - 400: Missing file(s) - 500: Server configuration problem","title":"Troubleshooting"},{"location":"MyDocs/opencart.html#settings-db-table","text":"setting_id store_id (Default value is '0') group key value serialized Look at the Data tab in Categories , under Catalogue . The Columns field might allow the number of subcategories to be set.","title":"Settings DB Table"},{"location":"MyDocs/pycrypto-aes.html","text":"PyCrypto AES Text Encryption The first problem was that AES and other block ciphers only accept message and key sizes of a certain number or bytes, so both must be padded in some way. AES works on blocks of 128 bits (16 bytes = 128 bits). In Electronic Code Book (EBC) mode it works by encrypting each block against the key. In Cipher Block Chaining (CBC) mode AES will encrypt each block with a previous block, using the key for the first block(s). The latter should be the more secure against known plaintext attacks, plus EBC would potentially output the same bit pattern for each repeating set of characters in the plaintext \u2013 if that set of characters can be determined, so can the key and therefore the entire message could be decrypted by a third party. Producing a set key length isn\u2019t difficult, as we can simply take any password and always produce a SHA256 value. This is a known method of key hardening, but in this case I only used it for convenience. Pass = raw_input('KEY: ') encodedPass = hashlib.sha256(Pass).digest() Another method of padding must be used for the text message, as SHA256 is non-reversible. The AES function only allows plaintext lengths that are multiples of 16 bytes, but each character is a byte, and the string functions can be used for rounding a message up to the nearst 16 characters. message = raw_input('MESSAGE: ') length = 16 - (len(message) % 16) message += chr(length)*length In the following section I\u2019ve used ECB mode AES anyway. Because the algorithm shifts bits around, and not whole ASCII character bytes, the ciphertext is normally binary data that doesn\u2019t translate well to ASCII, so the output is Base64 encoded: EncryptData = AES.new(encodedPass, AES.MODE_ECB) ciphertext = EncryptData.encrypt(message) print base64.b64encode(ciphertext) The password used for encrypting the message will have the same SHA256 value as before, so therefore the key will be exactly the same for decryption. Pass = raw_input('KEY: ') encodedPass = hashlib.sha256(Pass).digest() message = raw_input('CIPHERTEXT: ') length = 16 - (len(message) % 16) message += chr(length)*length The padding sometimes causes the output to be appended with erroneous symbols, but the plaintext is still quite readable. If characters are missing, check that both encryption and decryption functions are using the same mode (CBC or EBC). DecryptData = AES.new(encodedPass, AES.MODE_EBC) plaintext = DecryptData.decrypt(base64.b64decode(message)) print plaintext","title":"PyCrypto AES"},{"location":"MyDocs/pycrypto-aes.html#pycrypto-aes-text-encryption","text":"The first problem was that AES and other block ciphers only accept message and key sizes of a certain number or bytes, so both must be padded in some way. AES works on blocks of 128 bits (16 bytes = 128 bits). In Electronic Code Book (EBC) mode it works by encrypting each block against the key. In Cipher Block Chaining (CBC) mode AES will encrypt each block with a previous block, using the key for the first block(s). The latter should be the more secure against known plaintext attacks, plus EBC would potentially output the same bit pattern for each repeating set of characters in the plaintext \u2013 if that set of characters can be determined, so can the key and therefore the entire message could be decrypted by a third party. Producing a set key length isn\u2019t difficult, as we can simply take any password and always produce a SHA256 value. This is a known method of key hardening, but in this case I only used it for convenience. Pass = raw_input('KEY: ') encodedPass = hashlib.sha256(Pass).digest() Another method of padding must be used for the text message, as SHA256 is non-reversible. The AES function only allows plaintext lengths that are multiples of 16 bytes, but each character is a byte, and the string functions can be used for rounding a message up to the nearst 16 characters. message = raw_input('MESSAGE: ') length = 16 - (len(message) % 16) message += chr(length)*length In the following section I\u2019ve used ECB mode AES anyway. Because the algorithm shifts bits around, and not whole ASCII character bytes, the ciphertext is normally binary data that doesn\u2019t translate well to ASCII, so the output is Base64 encoded: EncryptData = AES.new(encodedPass, AES.MODE_ECB) ciphertext = EncryptData.encrypt(message) print base64.b64encode(ciphertext) The password used for encrypting the message will have the same SHA256 value as before, so therefore the key will be exactly the same for decryption. Pass = raw_input('KEY: ') encodedPass = hashlib.sha256(Pass).digest() message = raw_input('CIPHERTEXT: ') length = 16 - (len(message) % 16) message += chr(length)*length The padding sometimes causes the output to be appended with erroneous symbols, but the plaintext is still quite readable. If characters are missing, check that both encryption and decryption functions are using the same mode (CBC or EBC). DecryptData = AES.new(encodedPass, AES.MODE_EBC) plaintext = DecryptData.decrypt(base64.b64decode(message)) print plaintext","title":"PyCrypto AES Text Encryption"},{"location":"MyDocs/python-authentication.html","text":"Login Screens, Hashes and Password Files with Python Probably everyone\u2019s seen a program that works something along the lines of: read UserInput if UserInput = 'password' print 'Login Success' else print 'Password Incorrect' The flaw with this method should be obvious: someone could either read the source code or reverse engineer the program to determine the password. To get around this, UNIX systems use another method that involves storing password hashes: when a user enters a password, it\u2019s hashed and stored in a file, or its hash is compared with the values already in that file. This method is intractable \u2013 that is, nobody should be able to determine the password from its digest, since MD5/SHA256 are one-way algorithms. Entering the hash value in the password screen also shouldn\u2019t work, as that value will also be hashed to generate an entirely different digest. Hashing in Python Let\u2019s start with the basics, with a simple Python script that generates a SHA256 hash from an input string, using the sha256() function from hashlib: import os import hashlib #Get txtPassword txtPassword = raw_input(\"Enter password: \") #Print the hashed password print hashlib.sha256(txtPassword).hexdigest() HashString Having written a program that hashes user text, the next stage is a simple matter of adding code that writes the output to a file: import os import hashlib #Load password file for appending f = open('Passwords.txt', 'a') #Get password txtPassword = raw_input(\"Enter password: \") #Hash newPassword and write to file newPassword = hashlib.sha256(txtPassword).hexdigest() f.write(newPassword) After running the program several times, there should now be a list of digests in Passwords.txt. Checking a Password What turned out to be exceedingly tricky was getting it to work in reverse, with the program again hashing an input string to find a matching value in Passwords.txt. Since I couldn\u2019t get this to work with a conditional branching statement, I resorted to printing \u2018Login Fail\u2019 by default and inserting exit() to terminate the program before that point if there\u2019s a match. Probably not a bad thing, security-wise. import os, sys import hashlib import string import re #Use hash value of user input as the search term txtPassword = raw_input(\"Enter password: \") searchKey = hashlib.sha256(txtPassword).hexdigest() #Open the password file lines = open(\"Passwords.txt\", \"r\" ).readlines() #Search the file for matching hash value for line in lines: if re.search(searchKey, line ): print \"Login Success\" exit() print \"Login Fail\" A PythonCard Login Screen Of course, this wasn\u2019t good enough for me. I wanted to apply this as a PythonCard GUI (which I\u2019ll probably cover in my next post). It works a little differently this time, as the program is event driven. Installing PythonCard gives us a couple of tools: resourceEditor GUI designer, and the codeEditor IDE. The resourceEditor is used for creating the interface itself stuff, while the handlers and back-end code are added to the Python script manually in codeEditor, replacing variables with component names. When sorting out the Python script (after creating the GUI), the first step is to import the Python modules used in the earlier programs: from PythonCard import dialog, model import os, sys import hashlib import string import re The handler for the \u2018Hash\u2019 button does exactly the same thing as the first script, but I/O is done the same way as with VB.NET: #The hash button def on_cmdCancel_mouseClick(self, event): self.components.txtHash.text = hashlib.sha256(self.components.txtPassword.text).hexdigest() Creating the handler for the checking (\u2018Login\u2019) button was much easier than I expected, and was a matter of renaming the variables: #The login button def on_cmdLogin_mouseClick(self, event): searchKey = hashlib.sha256(self.components.txtPassword.text).hexdigest() #Buffer the password file lines = open(\"Passwords.txt\", \"r\" ).readlines() #Search lines in password file for a match for line in lines: if re.search(searchKey, line ): dialog.alertDialog(self, 'Login Success') And as a finishing touch, another function to update the password file: #Password set button def on_cmdSetPassword_mouseClick(self, event): f = open('Passwords.txt', 'a') newPassword = hashlib.sha256(self.components.txtPassword.text).hexdigest() f.write(newPassword) dialog.alertDialog(self, 'Password Set')","title":"Authentication with Python"},{"location":"MyDocs/python-authentication.html#login-screens-hashes-and-password-files-with-python","text":"Probably everyone\u2019s seen a program that works something along the lines of: read UserInput if UserInput = 'password' print 'Login Success' else print 'Password Incorrect' The flaw with this method should be obvious: someone could either read the source code or reverse engineer the program to determine the password. To get around this, UNIX systems use another method that involves storing password hashes: when a user enters a password, it\u2019s hashed and stored in a file, or its hash is compared with the values already in that file. This method is intractable \u2013 that is, nobody should be able to determine the password from its digest, since MD5/SHA256 are one-way algorithms. Entering the hash value in the password screen also shouldn\u2019t work, as that value will also be hashed to generate an entirely different digest.","title":"Login Screens, Hashes and Password Files with Python"},{"location":"MyDocs/python-authentication.html#hashing-in-python","text":"Let\u2019s start with the basics, with a simple Python script that generates a SHA256 hash from an input string, using the sha256() function from hashlib: import os import hashlib #Get txtPassword txtPassword = raw_input(\"Enter password: \") #Print the hashed password print hashlib.sha256(txtPassword).hexdigest() HashString Having written a program that hashes user text, the next stage is a simple matter of adding code that writes the output to a file: import os import hashlib #Load password file for appending f = open('Passwords.txt', 'a') #Get password txtPassword = raw_input(\"Enter password: \") #Hash newPassword and write to file newPassword = hashlib.sha256(txtPassword).hexdigest() f.write(newPassword) After running the program several times, there should now be a list of digests in Passwords.txt.","title":"Hashing in Python"},{"location":"MyDocs/python-authentication.html#checking-a-password","text":"What turned out to be exceedingly tricky was getting it to work in reverse, with the program again hashing an input string to find a matching value in Passwords.txt. Since I couldn\u2019t get this to work with a conditional branching statement, I resorted to printing \u2018Login Fail\u2019 by default and inserting exit() to terminate the program before that point if there\u2019s a match. Probably not a bad thing, security-wise. import os, sys import hashlib import string import re #Use hash value of user input as the search term txtPassword = raw_input(\"Enter password: \") searchKey = hashlib.sha256(txtPassword).hexdigest() #Open the password file lines = open(\"Passwords.txt\", \"r\" ).readlines() #Search the file for matching hash value for line in lines: if re.search(searchKey, line ): print \"Login Success\" exit() print \"Login Fail\"","title":"Checking a Password"},{"location":"MyDocs/python-authentication.html#a-pythoncard-login-screen","text":"Of course, this wasn\u2019t good enough for me. I wanted to apply this as a PythonCard GUI (which I\u2019ll probably cover in my next post). It works a little differently this time, as the program is event driven. Installing PythonCard gives us a couple of tools: resourceEditor GUI designer, and the codeEditor IDE. The resourceEditor is used for creating the interface itself stuff, while the handlers and back-end code are added to the Python script manually in codeEditor, replacing variables with component names. When sorting out the Python script (after creating the GUI), the first step is to import the Python modules used in the earlier programs: from PythonCard import dialog, model import os, sys import hashlib import string import re The handler for the \u2018Hash\u2019 button does exactly the same thing as the first script, but I/O is done the same way as with VB.NET: #The hash button def on_cmdCancel_mouseClick(self, event): self.components.txtHash.text = hashlib.sha256(self.components.txtPassword.text).hexdigest() Creating the handler for the checking (\u2018Login\u2019) button was much easier than I expected, and was a matter of renaming the variables: #The login button def on_cmdLogin_mouseClick(self, event): searchKey = hashlib.sha256(self.components.txtPassword.text).hexdigest() #Buffer the password file lines = open(\"Passwords.txt\", \"r\" ).readlines() #Search lines in password file for a match for line in lines: if re.search(searchKey, line ): dialog.alertDialog(self, 'Login Success') And as a finishing touch, another function to update the password file: #Password set button def on_cmdSetPassword_mouseClick(self, event): f = open('Passwords.txt', 'a') newPassword = hashlib.sha256(self.components.txtPassword.text).hexdigest() f.write(newPassword) dialog.alertDialog(self, 'Password Set')","title":"A PythonCard Login Screen"},{"location":"MyDocs/smartcards.html","text":"Smart Card Programming Under the right conditions, Smart Cards are an almost perfect method of two-factor authentication \u2013 authenticating data is stored on the card encrypted, and the cryptographic key is derived from the PIN. In other words, the card and the owner\u2019s PIN is required to complete the authentication. But what\u2019s to prevent an attacker reading the data off the EEPROM and bruteforcing the encryption? Something else is needed to implement access control. In this case it\u2019s a microprocessor that mediates access to the EEPROM where the data is stored, along with some level of tamper-resistance to prevent that memory being accessed directly. Read/Write operations must therefore be performed through the microprocessor. After a little probing I learned that it\u2019s the microprocessor itself, and not an ATM machine, that manages PIN changes and bricks a bank card after three incorrect PINs are entered. The non-management aim of my project, which is slightly related to my work on the IPv6 darknet, is simple: To develop an application that uses Smart Cards as a method of storing cryptographic keys for secure Internet-based comms. Ideally the cards themselves would be endpoints in a widely deployed comms network. The first step is to create software that calls various functions to interact with a card\u2019s microprocessor. Currently I have installed: libpam-p11: PAM modules for PKCS#11 Smart Cards. libpcsclite: PCSC Lite library. OpenCT OpenSC: Libraries and utilities for interacting with Smart Cards OpenSC PKCS#11 APIs. pcsc_scan pcsc-tools pcscd: Middleware. pyscard: Smart Card module for Python . The Kit What I ordered from SmartCard Focus was pretty much the cheapest and most generic hardware I could find, so there\u2019s the greatest chance of getting Python modules, OpenSC, OpenPGP and maybe C++ libraries (as a last resort) to interact with the cards: - 3x ACOS3 ISO 7816 Smart Cards with 32KB EEPROM - 1x Gemalto IDBridge CT30 card reader - Drivers and utilities for Microsoft Windows SmartCard Focus also do cheap RF shielding products for protecting RFID cards. ACOS3 Microprocessor Card The cards themselves were designed on the ISO 7816 standard (which isn\u2019t what the banks primarily use), and memory access is mediated by the card\u2019s own microprocessor. Of course, this wasn\u2019t originally what I had in mind when starting the project, preferring to read/write data as encrypted blocks with the card acting simply as a storage medium for an encrypted block of data. The ACOS3 isn\u2019t suitable for high-security applications such as \u2018e-government\u2019 or national ID schemes, partly (perhaps mainly) because their microprocessors handle DES/Triple-DES only, so you\u2019re either getting 64-bit (which is actually 48-bit) or 192-bit encryption depending on the specific Triple-DES method. Interestingly, I\u2019ve learned, through a bit of field research, that some Chip-and-PIN payment systems still work with an ACOS3 card. The Card Reader According to Gemalto\u2019s product description, the IDBridge CT30 is a very basic card reader that works with any ISO 7816 Smart Card. In theory it acts simply as a data bus between the USB port and the card contacts, but in practice card readers have a controller chip that might need additional drivers. Communication and Test So far I\u2019ve done some initial testing of the hardware with ACOS3 and a range of other cards. The OpenCT and OpenSC tools detected the reader perfectly, but getting Linux-based stuff talking with the microprocessors is really a matter of testing different manufacturers and libraries, even though numerous technical docs say that certain types should work. The pcsc_scan tool reads the chip parameters and correctly identified the card manufacturer, so there\u2019s definitely some communication happening with the processor. I\u2019ve yet to properly try this with Python modules or C++ libraries (which I\u2019ve also installed). Application Data Protocol Units The functions are provided by an internal chip containing sets of transistors in different configurations, and a machine code instruction (e.g. 0111) is just a reference to physical pins feeding the chip. When voltage is applied to the right pins, data is passed from a tiny diode array (memory register A), through a specific transistor configuration and into another diode array (memory register B). In turn, the memory registers themselves cache whatever data was fetched from RAM. This is the long and short of what\u2019s happening at the hardware level, and ultimately what happens when people run any program/application on their computer. APDUs and Machine Code Communication with the microprocessor is done at a very low level, as byte strings called \u2018Application Protocol Data Units\u2019 (APDUs). The concept is vaguely similar to that of TCP packets, in that it involves a header and payload. The header is always 4 bytes in length (at least with the ACOS3), even when not all of them are used. The first byte is the machine code instruction itself, and because we\u2019re dealing with direct memory addressing we also specify the number of bytes to be processed when the instruction executes. While APDUs are pretty standard across most microprocessor-based cards, the instruction sets appear to be vendor-specfic. Knowing exactly how to put together an instruction demands a bit of research, being shit hot at hex-binary-ASCI-decimal conversion and the ability to keep track of memory addresses (or offsets) being used. Luckily SmartCard Focus had included enough documentation (especially the ACOS3 Smart Card Reference Manual) and an excellent range of diagnostic tools on the CD to make the task slightly easier. ACOS3 Programming Overview A typical instruction for the ACOS3\u2019s microprocessor has the initial instruction and data length bytes, followed by the payload, which I estimate to have a max size of around 250-253 bytes. [Instruction] [Data Length] [Data] [Data] ... [Data] To write data to the card\u2019s EEPROM, the command must also specify the address to write to and how many bytes should be written there. This is important because we\u2019re not dealing with files. Data is overwritten instead of deleted, and data blocks can only be addressed as offsets from the EEPROM\u2019s start location (whatever that might be). For the ACOS3 the write instruction is always 91h. As an example, if we wanted to write \u2018Hello World\u2019 to memory address 3Ah on the card\u2019s EEPROM, the pseudocode would be something like: [Instruction] [Data Length] [Memory Address] [Memory Address] [Data] [Data] WRITE: 11 BYTES TO 00 3Ah: HELLO WORLD Which translates into the following instruction: 91 00 3A 48 65 6C 6C 6F 20 57 6F 72 6C 64 It\u2019s a similar case with reading the data back off the card, as we must specify how many bytes to read from a given adress. Here the machine code instruction is 90h. If we wanted to read \u2018Hello World\u2019 from the address it was written to earlier, the pseudocode would be something like: [Instruction] [Data Length] [00] [Memory Address] [Length] READ: 11 BYTES FROM 00 3Ah: 00 Which gives us: 90 00 00 00 3A 0A There are a couple more things I tried out with the ACOS3 and some other cards. The crypto processor has a PRNG, and a 64-bit pseudo-random number (suffixed with a status code) is generated by sending instruction \u2019 80 84 00 00 08 \u2032. It probably uses this to generate session keys for transactions, which would mean the Triple-DES function is only using a 64-bit key. If the processor\u2019s PRNG is capable of generating a 128-bit value (which is a 16-bytes), we could get that by substituting the last byte with \u201910\u2019 (10h = 16). If the card\u2019s messed up, it could be reset to its factory state with \u2019 80 30 00 00 00 \u2032. The \u2019 80 24 00 00 08 5555 \u2032 command changes the PIN to \u20185555\u2019. What we should start to notice is a pattern in the instruction set, where \u201980h\u2019 refers to control operations, \u201990h\u2019 refers to read operations, and \u201991h\u2019 instructions write data. (g)scriptor Is there an IDE for writing machine code to a Smart Card? Well, there sort of is one, in the form of (g)scriptor . This tool can run a sequence of predefined bytestrings as a \u2018script\u2019, so a series of operations could be automated. It\u2019s important to confirm the reader is connected and the card\u2019s processor is functioning, by selecting \u2018 Reader \u2014 Status\u2026 \u2019. Now we can send bytes to the card and see what\u2019s returned. Here I think I\u2019ve established that \u2018 6E 00 \u2032 is an error message or no response, while \u2019 90 00 \u2019 is okay message. Python Unfortunately I haven\u2019t got very far writing decent Smart Card programs for Python, for the reasons above. At the moment I only have a script, using the PysCard module, that does generic card reader detection and ATR reading .","title":"Smart Card Programming"},{"location":"MyDocs/smartcards.html#smart-card-programming","text":"Under the right conditions, Smart Cards are an almost perfect method of two-factor authentication \u2013 authenticating data is stored on the card encrypted, and the cryptographic key is derived from the PIN. In other words, the card and the owner\u2019s PIN is required to complete the authentication. But what\u2019s to prevent an attacker reading the data off the EEPROM and bruteforcing the encryption? Something else is needed to implement access control. In this case it\u2019s a microprocessor that mediates access to the EEPROM where the data is stored, along with some level of tamper-resistance to prevent that memory being accessed directly. Read/Write operations must therefore be performed through the microprocessor. After a little probing I learned that it\u2019s the microprocessor itself, and not an ATM machine, that manages PIN changes and bricks a bank card after three incorrect PINs are entered. The non-management aim of my project, which is slightly related to my work on the IPv6 darknet, is simple: To develop an application that uses Smart Cards as a method of storing cryptographic keys for secure Internet-based comms. Ideally the cards themselves would be endpoints in a widely deployed comms network. The first step is to create software that calls various functions to interact with a card\u2019s microprocessor. Currently I have installed: libpam-p11: PAM modules for PKCS#11 Smart Cards. libpcsclite: PCSC Lite library. OpenCT OpenSC: Libraries and utilities for interacting with Smart Cards OpenSC PKCS#11 APIs. pcsc_scan pcsc-tools pcscd: Middleware. pyscard: Smart Card module for Python .","title":"Smart Card Programming"},{"location":"MyDocs/smartcards.html#the-kit","text":"What I ordered from SmartCard Focus was pretty much the cheapest and most generic hardware I could find, so there\u2019s the greatest chance of getting Python modules, OpenSC, OpenPGP and maybe C++ libraries (as a last resort) to interact with the cards: - 3x ACOS3 ISO 7816 Smart Cards with 32KB EEPROM - 1x Gemalto IDBridge CT30 card reader - Drivers and utilities for Microsoft Windows SmartCard Focus also do cheap RF shielding products for protecting RFID cards.","title":"The Kit"},{"location":"MyDocs/smartcards.html#acos3-microprocessor-card","text":"The cards themselves were designed on the ISO 7816 standard (which isn\u2019t what the banks primarily use), and memory access is mediated by the card\u2019s own microprocessor. Of course, this wasn\u2019t originally what I had in mind when starting the project, preferring to read/write data as encrypted blocks with the card acting simply as a storage medium for an encrypted block of data. The ACOS3 isn\u2019t suitable for high-security applications such as \u2018e-government\u2019 or national ID schemes, partly (perhaps mainly) because their microprocessors handle DES/Triple-DES only, so you\u2019re either getting 64-bit (which is actually 48-bit) or 192-bit encryption depending on the specific Triple-DES method. Interestingly, I\u2019ve learned, through a bit of field research, that some Chip-and-PIN payment systems still work with an ACOS3 card.","title":"ACOS3 Microprocessor Card"},{"location":"MyDocs/smartcards.html#the-card-reader","text":"According to Gemalto\u2019s product description, the IDBridge CT30 is a very basic card reader that works with any ISO 7816 Smart Card. In theory it acts simply as a data bus between the USB port and the card contacts, but in practice card readers have a controller chip that might need additional drivers.","title":"The Card Reader"},{"location":"MyDocs/smartcards.html#communication-and-test","text":"So far I\u2019ve done some initial testing of the hardware with ACOS3 and a range of other cards. The OpenCT and OpenSC tools detected the reader perfectly, but getting Linux-based stuff talking with the microprocessors is really a matter of testing different manufacturers and libraries, even though numerous technical docs say that certain types should work. The pcsc_scan tool reads the chip parameters and correctly identified the card manufacturer, so there\u2019s definitely some communication happening with the processor. I\u2019ve yet to properly try this with Python modules or C++ libraries (which I\u2019ve also installed).","title":"Communication and Test"},{"location":"MyDocs/smartcards.html#application-data-protocol-units","text":"The functions are provided by an internal chip containing sets of transistors in different configurations, and a machine code instruction (e.g. 0111) is just a reference to physical pins feeding the chip. When voltage is applied to the right pins, data is passed from a tiny diode array (memory register A), through a specific transistor configuration and into another diode array (memory register B). In turn, the memory registers themselves cache whatever data was fetched from RAM. This is the long and short of what\u2019s happening at the hardware level, and ultimately what happens when people run any program/application on their computer.","title":"Application Data Protocol Units"},{"location":"MyDocs/smartcards.html#apdus-and-machine-code","text":"Communication with the microprocessor is done at a very low level, as byte strings called \u2018Application Protocol Data Units\u2019 (APDUs). The concept is vaguely similar to that of TCP packets, in that it involves a header and payload. The header is always 4 bytes in length (at least with the ACOS3), even when not all of them are used. The first byte is the machine code instruction itself, and because we\u2019re dealing with direct memory addressing we also specify the number of bytes to be processed when the instruction executes. While APDUs are pretty standard across most microprocessor-based cards, the instruction sets appear to be vendor-specfic. Knowing exactly how to put together an instruction demands a bit of research, being shit hot at hex-binary-ASCI-decimal conversion and the ability to keep track of memory addresses (or offsets) being used. Luckily SmartCard Focus had included enough documentation (especially the ACOS3 Smart Card Reference Manual) and an excellent range of diagnostic tools on the CD to make the task slightly easier.","title":"APDUs and Machine Code"},{"location":"MyDocs/smartcards.html#acos3-programming-overview","text":"A typical instruction for the ACOS3\u2019s microprocessor has the initial instruction and data length bytes, followed by the payload, which I estimate to have a max size of around 250-253 bytes. [Instruction] [Data Length] [Data] [Data] ... [Data] To write data to the card\u2019s EEPROM, the command must also specify the address to write to and how many bytes should be written there. This is important because we\u2019re not dealing with files. Data is overwritten instead of deleted, and data blocks can only be addressed as offsets from the EEPROM\u2019s start location (whatever that might be). For the ACOS3 the write instruction is always 91h. As an example, if we wanted to write \u2018Hello World\u2019 to memory address 3Ah on the card\u2019s EEPROM, the pseudocode would be something like: [Instruction] [Data Length] [Memory Address] [Memory Address] [Data] [Data] WRITE: 11 BYTES TO 00 3Ah: HELLO WORLD Which translates into the following instruction: 91 00 3A 48 65 6C 6C 6F 20 57 6F 72 6C 64 It\u2019s a similar case with reading the data back off the card, as we must specify how many bytes to read from a given adress. Here the machine code instruction is 90h. If we wanted to read \u2018Hello World\u2019 from the address it was written to earlier, the pseudocode would be something like: [Instruction] [Data Length] [00] [Memory Address] [Length] READ: 11 BYTES FROM 00 3Ah: 00 Which gives us: 90 00 00 00 3A 0A There are a couple more things I tried out with the ACOS3 and some other cards. The crypto processor has a PRNG, and a 64-bit pseudo-random number (suffixed with a status code) is generated by sending instruction \u2019 80 84 00 00 08 \u2032. It probably uses this to generate session keys for transactions, which would mean the Triple-DES function is only using a 64-bit key. If the processor\u2019s PRNG is capable of generating a 128-bit value (which is a 16-bytes), we could get that by substituting the last byte with \u201910\u2019 (10h = 16). If the card\u2019s messed up, it could be reset to its factory state with \u2019 80 30 00 00 00 \u2032. The \u2019 80 24 00 00 08 5555 \u2032 command changes the PIN to \u20185555\u2019. What we should start to notice is a pattern in the instruction set, where \u201980h\u2019 refers to control operations, \u201990h\u2019 refers to read operations, and \u201991h\u2019 instructions write data.","title":"ACOS3 Programming Overview"},{"location":"MyDocs/smartcards.html#gscriptor","text":"Is there an IDE for writing machine code to a Smart Card? Well, there sort of is one, in the form of (g)scriptor . This tool can run a sequence of predefined bytestrings as a \u2018script\u2019, so a series of operations could be automated. It\u2019s important to confirm the reader is connected and the card\u2019s processor is functioning, by selecting \u2018 Reader \u2014 Status\u2026 \u2019. Now we can send bytes to the card and see what\u2019s returned. Here I think I\u2019ve established that \u2018 6E 00 \u2032 is an error message or no response, while \u2019 90 00 \u2019 is okay message.","title":"(g)scriptor"},{"location":"MyDocs/smartcards.html#python","text":"Unfortunately I haven\u2019t got very far writing decent Smart Card programs for Python, for the reasons above. At the moment I only have a script, using the PysCard module, that does generic card reader detection and ATR reading .","title":"Python"},{"location":"MyDocs/software-architecture-doc.html","text":"Software Architecture Document Introduction Purpose Scope Definitions, Acronyms and Abbreviations References Architecture Goals and Constraints Use Cases Logical View Packages and Subsystem Layers Processes Deployment Performance Quality","title":"Software Architecture Document"},{"location":"MyDocs/software-architecture-doc.html#software-architecture-document","text":"","title":"Software Architecture Document"},{"location":"MyDocs/software-architecture-doc.html#introduction","text":"Purpose Scope Definitions, Acronyms and Abbreviations References","title":"Introduction"},{"location":"MyDocs/software-architecture-doc.html#architecture","text":"Goals and Constraints","title":"Architecture"},{"location":"MyDocs/software-architecture-doc.html#use-cases","text":"","title":"Use Cases"},{"location":"MyDocs/software-architecture-doc.html#logical-view","text":"Packages and Subsystem Layers","title":"Logical View"},{"location":"MyDocs/software-architecture-doc.html#processes","text":"","title":"Processes"},{"location":"MyDocs/software-architecture-doc.html#deployment","text":"","title":"Deployment"},{"location":"MyDocs/software-architecture-doc.html#performance","text":"","title":"Performance"},{"location":"MyDocs/software-architecture-doc.html#quality","text":"","title":"Quality"},{"location":"MyDocs/solid.html","text":"SOLID Principles Class, Method, Variable and Property Naming As anyone who\u2019s tried to analyse the output of a decompiler would attest, the descriptive naming of objects within code makes a huge difference to its readability. This principle should be applied to classes, methods, variables and other objects, so the names are descriptive of their purpose and function. This will become important as we refactor our code. Minimal-Responsibility Methods This is conventionally referred to as the \u2018Single Responsibility Principle\u2019, and states that a method or function should do one thing only, and have a single responsibility. Structuring code this way should make testing each unit in isolation easier (when loose-coupling is applied), and it should be easier to extend/modify units without inadvertently affecting the general behaviour of the program. However, I\u2019m calling it the \u2018minimal-responsibility principle\u2019 here, as I find it\u2019s not always possible or productive to refactor methods/classes to multiple single-function units. In an existing project, I\u2019m looking at methods for distinct units of operation that could be extracted out. There is a \u2018 Quick Actions and Refactorings\u2026 \u2018 feature in Visual Studio that can do this for us. Open/Closed Principle \u2013 Maker of Things Visible and Invisible This SOLID principle states that a unit should be closed to modification, but open for extension. The idea behind this principle is that developers should extend existing units of code, instead of modifying them. A good reason for this is the assumption that a working iteration of a program is dependent on code that already exists, and therefore modifications come with a higher risk of introducing defects. Secondly, the practice of extending existing methods or classes helps us to avoid the duplication of code. What does this mean in practice, though? Any class that could be re-used should be instantiated as a base class for something that extends it. Visual Studio already does this by default with commonly-implemented features, such as MVC controllers. Let\u2019s look at a WebAPI controller that I\u2019ve refactored: public class GetLookupDataController : ApiController { LookupDataHelper GetLookupDataHelper = new LookupDataHelper(); public GetLookupDataResponse Get(string code, bool validItemsOnly) { var result = GetLookupDataHelper.GetLookupDataHelper(code, validItemsOnly); return result; } } As we can see, GetLookupDataController is a class that\u2019s derived from ApiController , and everything visible here is really an extension of that base class. Whenever we want to add a new WebAPI controller to a project, we declare the same ApiController as a base class instead of duplicating it under a different name. I can provide an even simpler illustration: In my project I have a data object called \u2018 Item \u2018, which has three properties: public class Item { public string Band { get; set; } public string Name { get; set; } public string Parent { get; set; } } What if I anticipated a feature request that involves a similar object with several more properties? In that case, I\u2019d rename the \u2018 Item \u2018 class as \u2018 BaseItem \u2018: public class BaseItem { public string Band { get; set; } public string Name { get; set; } public string Parent { get; set; } } I\u2019d add another class containing the additional properties that extend BaseItem : public class Item : BaseItem { public bool? DataNode { get; set; } public DateTime? ValidFrom { get; set; } public DateTime? ValidTo { get; set; } public int SortOrder { get; set; } } When executed, the software would construct a data structure containing all properties from the base and derived classes. Liskov Substitution Principle This principle essentially seems an extension of the previous one. A derived class should implement all the functionality provided by its base class, and without modifying whatever\u2019s being inherited. If the latter isn\u2019t the case, it\u2019s an indication that the base class violates the minimal responsibility principle. In other words, a program\u2019s behaviour should remain unchanged if the reference to a base class was replaced with a duplicate of its code. Interface Segregation A client shouldn\u2019t be dependent on things it doesn\u2019t use. This dependency could be inadvertently created if we\u2019re setting up an interface with multiple methods. Imagine an interface called \u2018 IFileOperations \u2018 that contains three methods: Read() , Write() and Save() , that respectively implements three operations, read file, write to file and save. Any client calling that interface would need to either use all three methods, even if only one is needed, or throw a \u2018not implemented\u2019 exception. One way to solve this would be to put each method within its own interface, IRead , IWrite and ISave . Or we could logically segregate the interfaces by mapping the names to the methods, e.g. class FileOperations : IRead, IWrite, ISave { // All three methods here } Dependency Injection Here I\u2019m using the constructor injection method of resolving the tight coupling between a Web API helper and a GetDbReader class. In this project, GetReader() is the method that executes SqlCommand() using parameters passed from the helper method. I started out with the following code in Helper.cs : public class LookupDataHelper : System.Web.Services.WebService { ... GetDbReader getDbReader = new GetDbReader(); ... public GetLookupDataResponse GetLookupDataHelper(string code, bool validItemsOnly) { ... SqlDataReader reader; getDbReader.GetReader(code, validItemsOnly, out SqlCommand, out reader); ... return result; } } As we can see, GetLookupDataHelper() is dependent on an instance of GetDbReader() , which contains a method that implements the database reader function. public class GetDbReader { public void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) { ... SqlCommand.CommandText = query; SqlCommand.Connection.Open(); reader = SqlCommand.ExecuteReader(); } } It is possible to use a form of dependency injection here, so the helper class and GetDbReader() aren\u2019t so tightly coupled. I added an interface called \u2018 IGetDbReader \u2018 and declared the GetDbReader class as a member of it. After the interface is created, the helper instantiates the interface: public interface IGetDbReader { void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) } public class GetDbReader : IGetDbReader { public void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) { ... SqlCommand.CommandText = query; SqlCommand.Connection.Open(); reader = SqlCommand.ExecuteReader(); } } Getting the helper method to use the interface was easy, a simple matter of changing a line so that the interface was instantiated as \u2018 getDbReader \u2018. public class LookupDataHelper : System.Web.Services.WebService { ... IGetDbReader getDbReader = new GetDbReader(); ... public GetLookupDataResponse GetLookupDataHelper(string code, bool validItemsOnly) { SqlDataReader reader; getDbReader.GetReader(code, validItemsOnly, out SqlCommand, out reader); ... return result; } }","title":"SOLID Development"},{"location":"MyDocs/solid.html#solid-principles","text":"","title":"SOLID Principles"},{"location":"MyDocs/solid.html#class-method-variable-and-property-naming","text":"As anyone who\u2019s tried to analyse the output of a decompiler would attest, the descriptive naming of objects within code makes a huge difference to its readability. This principle should be applied to classes, methods, variables and other objects, so the names are descriptive of their purpose and function. This will become important as we refactor our code.","title":"Class, Method, Variable and Property Naming"},{"location":"MyDocs/solid.html#minimal-responsibility-methods","text":"This is conventionally referred to as the \u2018Single Responsibility Principle\u2019, and states that a method or function should do one thing only, and have a single responsibility. Structuring code this way should make testing each unit in isolation easier (when loose-coupling is applied), and it should be easier to extend/modify units without inadvertently affecting the general behaviour of the program. However, I\u2019m calling it the \u2018minimal-responsibility principle\u2019 here, as I find it\u2019s not always possible or productive to refactor methods/classes to multiple single-function units. In an existing project, I\u2019m looking at methods for distinct units of operation that could be extracted out. There is a \u2018 Quick Actions and Refactorings\u2026 \u2018 feature in Visual Studio that can do this for us.","title":"Minimal-Responsibility Methods"},{"location":"MyDocs/solid.html#openclosed-principle-maker-of-things-visible-and-invisible","text":"This SOLID principle states that a unit should be closed to modification, but open for extension. The idea behind this principle is that developers should extend existing units of code, instead of modifying them. A good reason for this is the assumption that a working iteration of a program is dependent on code that already exists, and therefore modifications come with a higher risk of introducing defects. Secondly, the practice of extending existing methods or classes helps us to avoid the duplication of code. What does this mean in practice, though? Any class that could be re-used should be instantiated as a base class for something that extends it. Visual Studio already does this by default with commonly-implemented features, such as MVC controllers. Let\u2019s look at a WebAPI controller that I\u2019ve refactored: public class GetLookupDataController : ApiController { LookupDataHelper GetLookupDataHelper = new LookupDataHelper(); public GetLookupDataResponse Get(string code, bool validItemsOnly) { var result = GetLookupDataHelper.GetLookupDataHelper(code, validItemsOnly); return result; } } As we can see, GetLookupDataController is a class that\u2019s derived from ApiController , and everything visible here is really an extension of that base class. Whenever we want to add a new WebAPI controller to a project, we declare the same ApiController as a base class instead of duplicating it under a different name. I can provide an even simpler illustration: In my project I have a data object called \u2018 Item \u2018, which has three properties: public class Item { public string Band { get; set; } public string Name { get; set; } public string Parent { get; set; } } What if I anticipated a feature request that involves a similar object with several more properties? In that case, I\u2019d rename the \u2018 Item \u2018 class as \u2018 BaseItem \u2018: public class BaseItem { public string Band { get; set; } public string Name { get; set; } public string Parent { get; set; } } I\u2019d add another class containing the additional properties that extend BaseItem : public class Item : BaseItem { public bool? DataNode { get; set; } public DateTime? ValidFrom { get; set; } public DateTime? ValidTo { get; set; } public int SortOrder { get; set; } } When executed, the software would construct a data structure containing all properties from the base and derived classes.","title":"Open/Closed Principle \u2013 Maker of Things Visible and Invisible"},{"location":"MyDocs/solid.html#liskov-substitution-principle","text":"This principle essentially seems an extension of the previous one. A derived class should implement all the functionality provided by its base class, and without modifying whatever\u2019s being inherited. If the latter isn\u2019t the case, it\u2019s an indication that the base class violates the minimal responsibility principle. In other words, a program\u2019s behaviour should remain unchanged if the reference to a base class was replaced with a duplicate of its code.","title":"Liskov Substitution Principle"},{"location":"MyDocs/solid.html#interface-segregation","text":"A client shouldn\u2019t be dependent on things it doesn\u2019t use. This dependency could be inadvertently created if we\u2019re setting up an interface with multiple methods. Imagine an interface called \u2018 IFileOperations \u2018 that contains three methods: Read() , Write() and Save() , that respectively implements three operations, read file, write to file and save. Any client calling that interface would need to either use all three methods, even if only one is needed, or throw a \u2018not implemented\u2019 exception. One way to solve this would be to put each method within its own interface, IRead , IWrite and ISave . Or we could logically segregate the interfaces by mapping the names to the methods, e.g. class FileOperations : IRead, IWrite, ISave { // All three methods here }","title":"Interface Segregation"},{"location":"MyDocs/solid.html#dependency-injection","text":"Here I\u2019m using the constructor injection method of resolving the tight coupling between a Web API helper and a GetDbReader class. In this project, GetReader() is the method that executes SqlCommand() using parameters passed from the helper method. I started out with the following code in Helper.cs : public class LookupDataHelper : System.Web.Services.WebService { ... GetDbReader getDbReader = new GetDbReader(); ... public GetLookupDataResponse GetLookupDataHelper(string code, bool validItemsOnly) { ... SqlDataReader reader; getDbReader.GetReader(code, validItemsOnly, out SqlCommand, out reader); ... return result; } } As we can see, GetLookupDataHelper() is dependent on an instance of GetDbReader() , which contains a method that implements the database reader function. public class GetDbReader { public void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) { ... SqlCommand.CommandText = query; SqlCommand.Connection.Open(); reader = SqlCommand.ExecuteReader(); } } It is possible to use a form of dependency injection here, so the helper class and GetDbReader() aren\u2019t so tightly coupled. I added an interface called \u2018 IGetDbReader \u2018 and declared the GetDbReader class as a member of it. After the interface is created, the helper instantiates the interface: public interface IGetDbReader { void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) } public class GetDbReader : IGetDbReader { public void GetReader(string code, bool validItemsOnly, out SqlCommand SqlCommand, out SqlDataReader reader) { ... SqlCommand.CommandText = query; SqlCommand.Connection.Open(); reader = SqlCommand.ExecuteReader(); } } Getting the helper method to use the interface was easy, a simple matter of changing a line so that the interface was instantiated as \u2018 getDbReader \u2018. public class LookupDataHelper : System.Web.Services.WebService { ... IGetDbReader getDbReader = new GetDbReader(); ... public GetLookupDataResponse GetLookupDataHelper(string code, bool validItemsOnly) { SqlDataReader reader; getDbReader.GetReader(code, validItemsOnly, out SqlCommand, out reader); ... return result; } }","title":"Dependency Injection"},{"location":"MyDocs/sqlite.html","text":"SQLite3 Create a table CREATE TABLE Introduction (Id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, English TEXT, German TEXT, Note TEXT, Category TEXT); Insert a record into a table INSERT INTO Introduction VALUES (0001, 'Test', 'Test', '', 'Introduction'); Select all records from a table SELECT * FROM Introduction Display table schema .schema Introduction Update an entry UPDATE Introduction SET English = 'How are you?' WHERE Id = 2;","title":"SQLite3"},{"location":"MyDocs/sqlite.html#sqlite3","text":"Create a table CREATE TABLE Introduction (Id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, English TEXT, German TEXT, Note TEXT, Category TEXT); Insert a record into a table INSERT INTO Introduction VALUES (0001, 'Test', 'Test', '', 'Introduction'); Select all records from a table SELECT * FROM Introduction Display table schema .schema Introduction Update an entry UPDATE Introduction SET English = 'How are you?' WHERE Id = 2;","title":"SQLite3"},{"location":"MyDocs/suggested-toolset.html","text":"Software Toolset Atom Editor Not the most efficient editor, but renders Markup nicely. Alternatives to consider include Visual Studio Code and Notepad++. https://atom.io/ Azure Storage Explorer Manage your cloud storage on Azure. Upload, download, and manage Azure Storage blobs, files, queues, and tables, as well as Azure Data Lake Storage entities and Azure managed disks. Configure storage permissions and access controls, tiers, and rules. https://azure.microsoft.com/en-us/features/storage-explorer/ Chrome The standard browser for our development, testing and debugging. www.google.com/intl/en_uk/chrome/ Docker www.docker.com/ GitHub Desktop For those who aren't too familiar with the command line. GitHub Desktop | Simple collaboration from your desktop. https://desktop.github.com/ Microsoft SQL Server Management Studio https://docs.microsoft.com/en-us/sql/ssms MySQL Workbench https://www.mysql.com/products/workbench/ Notepad++ A lightweight code editor with comprehensive features and syntax highlighting for a large range of languages. https://notepad-plus-plus.org pgAdmin pgAdmin is the most popular and feature rich Open Source administration and development platform for PostgreSQL, the most advanced Open Source database in the world. https://www.pgadmin.org Postman Send requests, inspect responses, and easily debug REST APIs. www.postman.com Visual Studio Standard developer environment. https://visualstudio.microsoft.com/ Visual Studio Code Windows Terminal https://github.com/microsoft/terminal/","title":"Suggested Toolset"},{"location":"MyDocs/suggested-toolset.html#software-toolset","text":"","title":"Software Toolset"},{"location":"MyDocs/suggested-toolset.html#atom-editor","text":"Not the most efficient editor, but renders Markup nicely. Alternatives to consider include Visual Studio Code and Notepad++. https://atom.io/","title":"Atom Editor"},{"location":"MyDocs/suggested-toolset.html#azure-storage-explorer","text":"Manage your cloud storage on Azure. Upload, download, and manage Azure Storage blobs, files, queues, and tables, as well as Azure Data Lake Storage entities and Azure managed disks. Configure storage permissions and access controls, tiers, and rules. https://azure.microsoft.com/en-us/features/storage-explorer/","title":"Azure Storage Explorer"},{"location":"MyDocs/suggested-toolset.html#chrome","text":"The standard browser for our development, testing and debugging. www.google.com/intl/en_uk/chrome/","title":"Chrome"},{"location":"MyDocs/suggested-toolset.html#docker","text":"www.docker.com/","title":"Docker"},{"location":"MyDocs/suggested-toolset.html#github-desktop","text":"For those who aren't too familiar with the command line. GitHub Desktop | Simple collaboration from your desktop. https://desktop.github.com/","title":"GitHub Desktop"},{"location":"MyDocs/suggested-toolset.html#microsoft-sql-server-management-studio","text":"https://docs.microsoft.com/en-us/sql/ssms","title":"Microsoft SQL Server Management Studio"},{"location":"MyDocs/suggested-toolset.html#mysql-workbench","text":"https://www.mysql.com/products/workbench/","title":"MySQL Workbench"},{"location":"MyDocs/suggested-toolset.html#notepad","text":"A lightweight code editor with comprehensive features and syntax highlighting for a large range of languages. https://notepad-plus-plus.org","title":"Notepad++"},{"location":"MyDocs/suggested-toolset.html#pgadmin","text":"pgAdmin is the most popular and feature rich Open Source administration and development platform for PostgreSQL, the most advanced Open Source database in the world. https://www.pgadmin.org","title":"pgAdmin"},{"location":"MyDocs/suggested-toolset.html#postman","text":"Send requests, inspect responses, and easily debug REST APIs. www.postman.com","title":"Postman"},{"location":"MyDocs/suggested-toolset.html#visual-studio","text":"Standard developer environment. https://visualstudio.microsoft.com/","title":"Visual Studio"},{"location":"MyDocs/suggested-toolset.html#visual-studio-code","text":"","title":"Visual Studio Code"},{"location":"MyDocs/suggested-toolset.html#windows-terminal","text":"https://github.com/microsoft/terminal/","title":"Windows Terminal"},{"location":"MyDocs/web-apis.html","text":"Web APIs Based on REST. Like Web Services, the APIs provide an entry point to services. Being a RESTful service, an API call is made through a URI that looks like the following: http://localhost:63457/api/records This would, depending on the security in place, return all records in XML format. A specific entry can be returned by sending a URI that specifies a record identifier. e.g. http://localhost:63457/api/records/26","title":"Web APIs"},{"location":"MyDocs/web-apis.html#web-apis","text":"Based on REST. Like Web Services, the APIs provide an entry point to services. Being a RESTful service, an API call is made through a URI that looks like the following: http://localhost:63457/api/records This would, depending on the security in place, return all records in XML format. A specific entry can be returned by sending a URI that specifies a record identifier. e.g. http://localhost:63457/api/records/26","title":"Web APIs"},{"location":"MyDocs/web-services.html","text":"Web Services ASMX C# File: [WebMethod(MessageName = \"GetDepartmentReferenceData\")] public DepartmentReferenceDataResponse GetDepartmentReferenceData() { LogStart(); try { var response = myWebServiceHelper.GetReferenceDataResponse(); LogSuccess(String.Format(\"{0} items returned\", response.Body.Items.Length)); return response; } catch (Exception ex) { LogFailure(ex); throw; } } [WebMethod(MessageName = \"GetAuditDataViewer\")] public GetAuditDataResponse GetAuditData(string SpecimenID, string Department, int Days, DateTime StartDate, DateTime EndDate) { LogStart(); try { var response = WrrsHelper.GetAuditResponse(SpecimenID, Department, Days, StartDate, EndDate); LogSuccess(String.Format(\"{0} items returned\", response.Body.Items.Length)); return response; } catch (Exception ex) { LogFailure(ex); throw; } } Helper method: internal static GetAuditDataResponse GetAuditResponse(string SpecimenID, string Department) { var commandAudit = new SqlCommand { CommandText = \"prGetAuditData\", CommandType = CommandType.StoredProcedure, }; var paramListAudit = new List<SqlParameter> { new SqlParameter { ParameterName = \"@SpecimenID\", Direction = ParameterDirection.Input, SqlDbType = SqlDbType.VarChar, Value = SpecimenID }, new SqlParameter { ParameterName = \"@Department\", Direction = ParameterDirection.Input, SqlDbType = SqlDbType.VarChar, Value = Department } }; commandAudit.Parameters.AddRange(paramListAudit.ToArray()); var dsAudit = Global.DbConnectionInfoManager.ExecuteCommand(commandAudit); return new GetAuditDataResponse(new GetAuditDataResponseBody(dsAudit.Tables[0].Select()), new Query()); }","title":"Web Services"},{"location":"MyDocs/web-services.html#web-services","text":"ASMX C# File: [WebMethod(MessageName = \"GetDepartmentReferenceData\")] public DepartmentReferenceDataResponse GetDepartmentReferenceData() { LogStart(); try { var response = myWebServiceHelper.GetReferenceDataResponse(); LogSuccess(String.Format(\"{0} items returned\", response.Body.Items.Length)); return response; } catch (Exception ex) { LogFailure(ex); throw; } } [WebMethod(MessageName = \"GetAuditDataViewer\")] public GetAuditDataResponse GetAuditData(string SpecimenID, string Department, int Days, DateTime StartDate, DateTime EndDate) { LogStart(); try { var response = WrrsHelper.GetAuditResponse(SpecimenID, Department, Days, StartDate, EndDate); LogSuccess(String.Format(\"{0} items returned\", response.Body.Items.Length)); return response; } catch (Exception ex) { LogFailure(ex); throw; } } Helper method: internal static GetAuditDataResponse GetAuditResponse(string SpecimenID, string Department) { var commandAudit = new SqlCommand { CommandText = \"prGetAuditData\", CommandType = CommandType.StoredProcedure, }; var paramListAudit = new List<SqlParameter> { new SqlParameter { ParameterName = \"@SpecimenID\", Direction = ParameterDirection.Input, SqlDbType = SqlDbType.VarChar, Value = SpecimenID }, new SqlParameter { ParameterName = \"@Department\", Direction = ParameterDirection.Input, SqlDbType = SqlDbType.VarChar, Value = Department } }; commandAudit.Parameters.AddRange(paramListAudit.ToArray()); var dsAudit = Global.DbConnectionInfoManager.ExecuteCommand(commandAudit); return new GetAuditDataResponse(new GetAuditDataResponseBody(dsAudit.Tables[0].Select()), new Query()); }","title":"Web Services"}]}