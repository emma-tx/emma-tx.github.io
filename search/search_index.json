{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Scarbrough @ GitHub This is where I publish software projects, useful code samples and documentation. Repositories Code Samples Various useful code samples. INFOSEC Code related to computer and network security. Python Python code samples. Systems Integration Systems integration code. PHAROS Mainly .NET Core project templates. Articles","title":"Home"},{"location":"#welcome-to-scarbrough-github","text":"This is where I publish software projects, useful code samples and documentation.","title":"Welcome to Scarbrough @ GitHub"},{"location":"#repositories","text":"Code Samples Various useful code samples. INFOSEC Code related to computer and network security. Python Python code samples. Systems Integration Systems integration code. PHAROS Mainly .NET Core project templates.","title":"Repositories"},{"location":"#articles","text":"","title":"Articles"},{"location":"about/","text":"About Me Most of my work is currently related to addressing legacy software, technical debt and documentation, essentially making existing systems more robust, secure and maintainable. Technologies .NET Core and Entity Framework Core Visual Studio Visual Studio Code SQL Server Microsoft Azure Docker GitHub","title":"About"},{"location":"about/#about-me","text":"Most of my work is currently related to addressing legacy software, technical debt and documentation, essentially making existing systems more robust, secure and maintainable.","title":"About Me"},{"location":"about/#technologies","text":".NET Core and Entity Framework Core Visual Studio Visual Studio Code SQL Server Microsoft Azure Docker GitHub","title":"Technologies"},{"location":"resources/","text":"Resources Codemanship's Blog GitHub Commands Cheat Sheet Hacker News OWASP Top 10 W3Schools","title":"Resources"},{"location":"resources/#resources","text":"Codemanship's Blog GitHub Commands Cheat Sheet Hacker News OWASP Top 10 W3Schools","title":"Resources"},{"location":"articles/docker-basics/","text":"Docker Basics A Docker container runs as a sandboxed process on a local system or a server, and consists of at least one image. An image is built from the source code of an application or service, and can be distributed, through a repository, to provide others the means of testing an existing software product with minimal setup. Containers and images can be managed through the Docker Dashboard application. As such, a Docker container will require its own configurations and script that enable it to run as intended. Though we are currently using the Docker Desktop application and Docker CLI, the repository of images is hosted by an Azure Container Registry. Requirements Docker Hub account Docker Desktop application Docker CLI Windows Terminal or PowerShell Azure CLI Access to the repositories Access to the Azure Container Registry Getting Started First, register an account with Docker Hub, at https://hub.docker.com/. After registering the account and doing whatever configuration is necessary, install the Docker Desktop client. The Docker CLI will also be required. Note: When registering a Docker Hub account, make a note of the username/ID, as the portal logins sometimes don't work with the registered email address. There is a 'Getting Started' container, which has an nginx server and a few Web pages, that can be fetched and run, to check everything is working. To see this demo in action, run the following command in PowerShell or Windows Terminal: docker run -d -p 80:80 docker/getting-started Navigate the browser to http://localhost to view the site. Creating an Image From an Existing Project To build an image from a software project this into a Docker image, a 'Dockerfile' is added to the project directory. It might look something like: # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Then build the above project as a Docker container with the following command in the containing directory: $docker build -t my-project . The container should be ready to run with the following command: $docker run -dp 3000:3000 my-project The container should appear in the Docker Dashboard, where it could be managed. Pushing Image to Azure Container Registry docker tag local-image:tagname new-repo:tagname docker push new-repo:tagname The following is an example of the commands I used to push an image to the Docker Hub: PS C:\\Users\\emma> docker login Authenticating with existing credentials... Login Succeeded PS C:\\Users\\emma> docker tag test-application:dev test-repository/test-application:dev PS C:\\Users\\emma> docker push test-repository/test-application:dev The push refers to repository [docker.io/test-repository/test-application] e7a596a1b658: Pushed d89241eb9d34: Pushed bb19eedc1dad: Pushed d1597429d57f: Pushed c891b6c5469a: Pushed ad6562704f37: Pushed dev: digest: [digest] size: 1578 Recommended Image Tags latest stage production Useful Commands $docker image ls | grep [application-name] $docker build -t [application-name]:latest $docker build -t [application-name]:test docker run -dp 3000:3000 [my-project] Volumes In Docker, volumes should be created if application data needs to be made persistent. A Docker volume is very much like a partition, with its own Linux file system. If a service or database image is to use a volume, it must be declared in the service section of docker-compose.yml and at the end of the file, e.g. test.db: image: mcr.microsoft.com/mssql/server environment: - SA_PASSWORD=TestDB123 - ACCEPT_EULA=Y ports: - \"1433:1433\" volumes: - test-datastore:/var/opt/mssql ... volumes: - test-datastore:/var/opt/mssql SQL Server Image In order to set up a database for the Docker application, it's necessary to have a base Microsoft SQL Server image. We can pull this from an external repository: docker pull mcr.microsoft.com/mssql/server:2019-latest The following command will initialise the database, configuring it to have the name 'testdb', run on port 1433, and with the default login password of 'TestDB123'. The database will use the test-datastore volume to make its data persistent: docker run -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=TestDB123\" -p 1433:1433 --name testdb -v test-datastore:/var/opt/mssql -d mcr.microsoft.com/mssql/server:2019-latest In order to get a .NET application working with a SQL Server running as a Docker image, it might be necessary to use 'host.docker.internal' in the connection string as the host name, e.g. \"ConnectionStrings\": { \"DefaultConnection\": \"Server=host.docker.internal;Initial Catalog=dockertest;Integrated Security=false;User ID=SA;Password=StandardPass56\" } If, for some reason, the project references an older database connection in Connected Services and Secrets.json , these must be removed, as the application will otherwise revert to that. As for the docker-compose.yml configuration, this seems to work: version: '3.4' services: dotnet-core-6-mvc: image: ${DOCKER_REGISTRY-}dotnetcore6mvc build: context: . dockerfile: Test-MVC/Dockerfile DockerSqlServerNew: image: mcr.microsoft.com/mssql/server environment: - SA_PASSWORD=StandardPass56 - ACCEPT_EULA=Y ports: - \"1443:1433\" After running ' docker-compose build ' and ' docker-compose up ', the Docker registry should list a container that includes the DB service with the application. Docker and Visual Studio The first step is to set up Docker support in the Visual Studio project: - Right-click on the project root, and select ' Add ' -> ' Docker Support... '. - Right-click on the project root again, and select ' Add ' -> ' Container Orchestrator Support... '. At this point, there should be a docker-compose section in the Visual Studio solution, with .dockerignore and docker-compose.yml files in the solution directory. There will also be a hidden override file there, plus a Dockerfile within the project. Combined, these should enable other developers to replicate the process of creating the images. The docker-compose.yml file should initially contain the following: version: '3.4' services: dotnet-core-6-mvc: image: ${DOCKER_REGISTRY-}dotnetcore6mvc build: context: . dockerfile: Test-MVC/Dockerfile Also, the Visual Studio debug options will be replaced with the configuration for Docker Compose . To run the project using IIS Express again, just click the drop-down with ' docker-compose ' and select the original project name. The .NET Core application will run as a Docker container, referenced in the Docker registry. Aside from not being able to connect to a local database server, it runs well enough. Docker Compose Files Each repository should include the following docker-compose files: - docker-compose.yml: This file is used for pulling all images from the Azure Container Registry and composing a Docker container with them. To be run within Windows Terminal or PowerShell. The image(s) for this project do not exist in the Azure Container Registry at the time of writing. - docker-compose-dependencies.yml: # This file is used for pulling images for any additional services required for debugging this project in the developer environment (e.g. Visual Studio or Visual Studio Code) - docker-compose-build.yml: This file is to be used when building a new Docker image from the source, with images for associated services being pulled from Azure Container Registry to ensure there are no breaking changes. Building this image and running it, alongside the others in a container, could be considered an integration test. Process for Running Services in a Container 1. Clone the GitHub repository for the service/application being tested or built. This repository should include all the required Docker configurations. 2. Autheticate the Windows Terminal or PowerShell session with Azure. Use the az acr login command. 3. Get the image name(s) from the Azure Container Registry az acr repository list --name [registry name] 4. Pull the required images from the Azure Container Registry docker pull [registry].azurecr.io/[image name] By default, the Azure client will pull the image tagged 'latest'. 5. Run the docker-compose script How docker-compose should be used will depend on the context in which the container is to be run. To simply compose the container from a number of images, without making changes, navigate to the project's directory and run the following command: $ docker-compose -f docker-compose.yml up To build a new image for the selected project, and run it in a container with other images: $ docker-compose -f docker-compose-build.yml And to build a container that enables debugging in an Integrated Developer Environment, with other images providing the dependencies: $ docker-compose -f docker-compose-dependencies.yml 5. Run the docker-compose script How docker-compose should be used will depend on the context in which the container is to be run. To simply compose the container from a number of images, without making changes, navigate to the project's directory and run the following command: $ docker-compose -f docker-compose.yml To build a new image for the selected project, and run it in a container with other images: $ docker-compose -f docker-compose-build.yml And to build a container that enables debugging in an Integrated Developer Environment, with other images providing the dependencies: $ docker-compose -f docker-compose-dependencies.yml Troubleshooting (Getting Started) Open PowerShell as admin, and use the following: & 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchDaemon In many cases the problem is caused by the Docker Engine failing to start. The indicators for this are the coloured bar at the lower left of the desktop GUI, and the set of ions to the right of the Windows task bar. Try enabling the Windows Subsystem for Linux (Turn Windows features on or off), and installing the latest WSL kernel update. Troubleshooting (Building Docker Image from Source) There were three problems encountered in getting one of the services to run as a Docker container. Missing SDK Error: The two most likely causes are that a) The SDK base image doesn't exist locally, and b) Docker is attempting to build the service without its dependencies, which are provided another project in the same solution directory. Check that docker-compose.yml is present in the solution's root directory, and the Dockerfile is copying the entire solution directory into /app and building the service and the referenced projects. See the wiki page for this error for further details. Other Information Overview of docker-compose CLI","title":"Docker Basics"},{"location":"articles/docker-basics/#docker-basics","text":"A Docker container runs as a sandboxed process on a local system or a server, and consists of at least one image. An image is built from the source code of an application or service, and can be distributed, through a repository, to provide others the means of testing an existing software product with minimal setup. Containers and images can be managed through the Docker Dashboard application. As such, a Docker container will require its own configurations and script that enable it to run as intended. Though we are currently using the Docker Desktop application and Docker CLI, the repository of images is hosted by an Azure Container Registry.","title":"Docker Basics"},{"location":"articles/docker-basics/#requirements","text":"Docker Hub account Docker Desktop application Docker CLI Windows Terminal or PowerShell Azure CLI Access to the repositories Access to the Azure Container Registry","title":"Requirements"},{"location":"articles/docker-basics/#getting-started","text":"First, register an account with Docker Hub, at https://hub.docker.com/. After registering the account and doing whatever configuration is necessary, install the Docker Desktop client. The Docker CLI will also be required. Note: When registering a Docker Hub account, make a note of the username/ID, as the portal logins sometimes don't work with the registered email address. There is a 'Getting Started' container, which has an nginx server and a few Web pages, that can be fetched and run, to check everything is working. To see this demo in action, run the following command in PowerShell or Windows Terminal: docker run -d -p 80:80 docker/getting-started Navigate the browser to http://localhost to view the site.","title":"Getting Started"},{"location":"articles/docker-basics/#creating-an-image-from-an-existing-project","text":"To build an image from a software project this into a Docker image, a 'Dockerfile' is added to the project directory. It might look something like: # syntax=docker/dockerfile:1 FROM node:12-alpine RUN apk add --no-cache python2 g++ make WORKDIR /app COPY . . RUN yarn install --production CMD [\"node\", \"src/index.js\"] EXPOSE 3000 Then build the above project as a Docker container with the following command in the containing directory: $docker build -t my-project . The container should be ready to run with the following command: $docker run -dp 3000:3000 my-project The container should appear in the Docker Dashboard, where it could be managed.","title":"Creating an Image From an Existing Project"},{"location":"articles/docker-basics/#pushing-image-to-azure-container-registry","text":"docker tag local-image:tagname new-repo:tagname docker push new-repo:tagname The following is an example of the commands I used to push an image to the Docker Hub: PS C:\\Users\\emma> docker login Authenticating with existing credentials... Login Succeeded PS C:\\Users\\emma> docker tag test-application:dev test-repository/test-application:dev PS C:\\Users\\emma> docker push test-repository/test-application:dev The push refers to repository [docker.io/test-repository/test-application] e7a596a1b658: Pushed d89241eb9d34: Pushed bb19eedc1dad: Pushed d1597429d57f: Pushed c891b6c5469a: Pushed ad6562704f37: Pushed dev: digest: [digest] size: 1578","title":"Pushing Image to Azure Container Registry"},{"location":"articles/docker-basics/#recommended-image-tags","text":"latest stage production","title":"Recommended Image Tags"},{"location":"articles/docker-basics/#useful-commands","text":"$docker image ls | grep [application-name] $docker build -t [application-name]:latest $docker build -t [application-name]:test docker run -dp 3000:3000 [my-project]","title":"Useful Commands"},{"location":"articles/docker-basics/#volumes","text":"In Docker, volumes should be created if application data needs to be made persistent. A Docker volume is very much like a partition, with its own Linux file system. If a service or database image is to use a volume, it must be declared in the service section of docker-compose.yml and at the end of the file, e.g. test.db: image: mcr.microsoft.com/mssql/server environment: - SA_PASSWORD=TestDB123 - ACCEPT_EULA=Y ports: - \"1433:1433\" volumes: - test-datastore:/var/opt/mssql ... volumes: - test-datastore:/var/opt/mssql","title":"Volumes"},{"location":"articles/docker-basics/#sql-server-image","text":"In order to set up a database for the Docker application, it's necessary to have a base Microsoft SQL Server image. We can pull this from an external repository: docker pull mcr.microsoft.com/mssql/server:2019-latest The following command will initialise the database, configuring it to have the name 'testdb', run on port 1433, and with the default login password of 'TestDB123'. The database will use the test-datastore volume to make its data persistent: docker run -e \"ACCEPT_EULA=Y\" -e \"SA_PASSWORD=TestDB123\" -p 1433:1433 --name testdb -v test-datastore:/var/opt/mssql -d mcr.microsoft.com/mssql/server:2019-latest In order to get a .NET application working with a SQL Server running as a Docker image, it might be necessary to use 'host.docker.internal' in the connection string as the host name, e.g. \"ConnectionStrings\": { \"DefaultConnection\": \"Server=host.docker.internal;Initial Catalog=dockertest;Integrated Security=false;User ID=SA;Password=StandardPass56\" } If, for some reason, the project references an older database connection in Connected Services and Secrets.json , these must be removed, as the application will otherwise revert to that. As for the docker-compose.yml configuration, this seems to work: version: '3.4' services: dotnet-core-6-mvc: image: ${DOCKER_REGISTRY-}dotnetcore6mvc build: context: . dockerfile: Test-MVC/Dockerfile DockerSqlServerNew: image: mcr.microsoft.com/mssql/server environment: - SA_PASSWORD=StandardPass56 - ACCEPT_EULA=Y ports: - \"1443:1433\" After running ' docker-compose build ' and ' docker-compose up ', the Docker registry should list a container that includes the DB service with the application.","title":"SQL Server Image"},{"location":"articles/docker-basics/#docker-and-visual-studio","text":"The first step is to set up Docker support in the Visual Studio project: - Right-click on the project root, and select ' Add ' -> ' Docker Support... '. - Right-click on the project root again, and select ' Add ' -> ' Container Orchestrator Support... '. At this point, there should be a docker-compose section in the Visual Studio solution, with .dockerignore and docker-compose.yml files in the solution directory. There will also be a hidden override file there, plus a Dockerfile within the project. Combined, these should enable other developers to replicate the process of creating the images. The docker-compose.yml file should initially contain the following: version: '3.4' services: dotnet-core-6-mvc: image: ${DOCKER_REGISTRY-}dotnetcore6mvc build: context: . dockerfile: Test-MVC/Dockerfile Also, the Visual Studio debug options will be replaced with the configuration for Docker Compose . To run the project using IIS Express again, just click the drop-down with ' docker-compose ' and select the original project name. The .NET Core application will run as a Docker container, referenced in the Docker registry. Aside from not being able to connect to a local database server, it runs well enough.","title":"Docker and Visual Studio"},{"location":"articles/docker-basics/#docker-compose-files","text":"Each repository should include the following docker-compose files: - docker-compose.yml: This file is used for pulling all images from the Azure Container Registry and composing a Docker container with them. To be run within Windows Terminal or PowerShell. The image(s) for this project do not exist in the Azure Container Registry at the time of writing. - docker-compose-dependencies.yml: # This file is used for pulling images for any additional services required for debugging this project in the developer environment (e.g. Visual Studio or Visual Studio Code) - docker-compose-build.yml: This file is to be used when building a new Docker image from the source, with images for associated services being pulled from Azure Container Registry to ensure there are no breaking changes. Building this image and running it, alongside the others in a container, could be considered an integration test.","title":"Docker Compose Files"},{"location":"articles/docker-basics/#process-for-running-services-in-a-container","text":"","title":"Process for Running Services in a Container"},{"location":"articles/docker-basics/#1-clone-the-github-repository-for-the-serviceapplication-being-tested-or-built","text":"This repository should include all the required Docker configurations.","title":"1. Clone the GitHub repository for the service/application being tested or built."},{"location":"articles/docker-basics/#2-autheticate-the-windows-terminal-or-powershell-session-with-azure","text":"Use the az acr login command.","title":"2. Autheticate the Windows Terminal or PowerShell session with Azure."},{"location":"articles/docker-basics/#3-get-the-image-names-from-the-azure-container-registry","text":"az acr repository list --name [registry name]","title":"3. Get the image name(s) from the Azure Container Registry"},{"location":"articles/docker-basics/#4-pull-the-required-images-from-the-azure-container-registry","text":"docker pull [registry].azurecr.io/[image name] By default, the Azure client will pull the image tagged 'latest'.","title":"4. Pull the required images from the Azure Container Registry"},{"location":"articles/docker-basics/#5-run-the-docker-compose-script","text":"How docker-compose should be used will depend on the context in which the container is to be run. To simply compose the container from a number of images, without making changes, navigate to the project's directory and run the following command: $ docker-compose -f docker-compose.yml up To build a new image for the selected project, and run it in a container with other images: $ docker-compose -f docker-compose-build.yml And to build a container that enables debugging in an Integrated Developer Environment, with other images providing the dependencies: $ docker-compose -f docker-compose-dependencies.yml","title":"5. Run the docker-compose script"},{"location":"articles/docker-basics/#5-run-the-docker-compose-script_1","text":"How docker-compose should be used will depend on the context in which the container is to be run. To simply compose the container from a number of images, without making changes, navigate to the project's directory and run the following command: $ docker-compose -f docker-compose.yml To build a new image for the selected project, and run it in a container with other images: $ docker-compose -f docker-compose-build.yml And to build a container that enables debugging in an Integrated Developer Environment, with other images providing the dependencies: $ docker-compose -f docker-compose-dependencies.yml","title":"5. Run the docker-compose script"},{"location":"articles/docker-basics/#troubleshooting-getting-started","text":"Open PowerShell as admin, and use the following: & 'C:\\Program Files\\Docker\\Docker\\DockerCli.exe' -SwitchDaemon In many cases the problem is caused by the Docker Engine failing to start. The indicators for this are the coloured bar at the lower left of the desktop GUI, and the set of ions to the right of the Windows task bar. Try enabling the Windows Subsystem for Linux (Turn Windows features on or off), and installing the latest WSL kernel update.","title":"Troubleshooting (Getting Started)"},{"location":"articles/docker-basics/#troubleshooting-building-docker-image-from-source","text":"There were three problems encountered in getting one of the services to run as a Docker container. Missing SDK Error: The two most likely causes are that a) The SDK base image doesn't exist locally, and b) Docker is attempting to build the service without its dependencies, which are provided another project in the same solution directory. Check that docker-compose.yml is present in the solution's root directory, and the Dockerfile is copying the entire solution directory into /app and building the service and the referenced projects. See the wiki page for this error for further details.","title":"Troubleshooting (Building Docker Image from Source)"},{"location":"articles/docker-basics/#other-information","text":"Overview of docker-compose CLI","title":"Other Information"},{"location":"articles/docker-missing-dotnet-sdk/","text":"Several potential fixes for the missing .NET SDK error (Docker and .NET Core) There have been many people asking about this on Stack Overflow, and none of the answers there brought me closer to resolving the problem, which, it later transpired, was caused by something really silly I'd overlooked. When running ' docker-compose up ' in the Docker CLI, for a .NET Core service, the output reports that it cannot find an installed SDK: It was not possible to find any installed .NET Core SDKs Did you mean to run .NET Core SDK commands? Install a .NET Core SDK from: https://aka.ms/dotnet-download The error message can be misleading, as the cause isn't necessarily a missing SDK. In my case, the cause was missing dependencies that were almost entirely unrelated to the SDK itself. I tried the following: Check the SDK image is installed The most obvious thing to check is that the SDK it's looking for actually exists on the local system. Using ' dotnet --info ', we can get a list of SDKs currently installed. Removing the Docker image for the relevant one and pulling it again from the official repository could be a solution if the image was corrupted somehow. docker pull mcr.microsoft.com/dotnet/sdk:6.0 Check references to the SDK in the Dockerfile Are the correct .NET base images and versions being referenced in the Dockerfile? Again, the ' dotnet --info ' command will provide the image names and version numbers, and they can be checked against the ones being referenced in the Dockerfile. Check entry points As I've said, the cause isn't always a missing SDK or an incorrect reference to it. Sometimes the same error message will appear if Docker can't find the entry point for the service it's trying to run - basically the EXE or DLL for the compiled application/service. This would be caused by an incorrect file path somewhere in the Dockerfile. To check for this, I opened the command line for the Docker image I was trying to run, and used the usual Linux commands to inspect its filesystem. Getting more details in Visual Studio's PowerShell Run ' docker build -t ' in the PowerShell window in Visual Studio. This might provide more specific exception messages pointing to the cause of the problem. In my case, it displayed the following somewhere in the output: warning MSB3245: Could not resolve this reference. Could not locate the assembly \"*System.Configuration*\". Check to make sure the assembly exists on disk. If this reference is required by your code, you may get compilation errors. [/src/Application.Web/Application.Web.csproj] Visual Studio solution with multiple projects In my case, I encountered the problem because another project in the Visual Studio solution ( Application.Common ) provided the dependencies for the one I was attempting to build ( Application.Web ). What I was doing wrong was trying to build and run Application.Web using a docker-compose.yml file within that project's directory, without referencing Application.Common or anything else in the solution directory. As a first step, I shifted docker-compose.yml and Dockerfile to the solution's root directory. Next, I added several lines to copy the entire solution to the virtual directory before building the projects/dependencies that were needed. e.g. [...] FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build WORKDIR /src COPY *.sln ./ COPY Application.Web/Application.Web.csproj Application.Web/ COPY . . RUN dotnet restore Application.Web/Application.Web.csproj WORKDIR /src/Application.Web [...] Then, using the following command, I was able to build the service (and its dependencies) and run its image in the Docker container: docker-compose up --build","title":"Missing .NET SDK Error (Docker)"},{"location":"articles/docker-missing-dotnet-sdk/#several-potential-fixes-for-the-missing-net-sdk-error-docker-and-net-core","text":"There have been many people asking about this on Stack Overflow, and none of the answers there brought me closer to resolving the problem, which, it later transpired, was caused by something really silly I'd overlooked. When running ' docker-compose up ' in the Docker CLI, for a .NET Core service, the output reports that it cannot find an installed SDK: It was not possible to find any installed .NET Core SDKs Did you mean to run .NET Core SDK commands? Install a .NET Core SDK from: https://aka.ms/dotnet-download The error message can be misleading, as the cause isn't necessarily a missing SDK. In my case, the cause was missing dependencies that were almost entirely unrelated to the SDK itself. I tried the following:","title":"Several potential fixes for the missing .NET SDK error (Docker and .NET Core)"},{"location":"articles/docker-missing-dotnet-sdk/#check-the-sdk-image-is-installed","text":"The most obvious thing to check is that the SDK it's looking for actually exists on the local system. Using ' dotnet --info ', we can get a list of SDKs currently installed. Removing the Docker image for the relevant one and pulling it again from the official repository could be a solution if the image was corrupted somehow. docker pull mcr.microsoft.com/dotnet/sdk:6.0","title":"Check the SDK image is installed"},{"location":"articles/docker-missing-dotnet-sdk/#check-references-to-the-sdk-in-the-dockerfile","text":"Are the correct .NET base images and versions being referenced in the Dockerfile? Again, the ' dotnet --info ' command will provide the image names and version numbers, and they can be checked against the ones being referenced in the Dockerfile.","title":"Check references to the SDK in the Dockerfile"},{"location":"articles/docker-missing-dotnet-sdk/#check-entry-points","text":"As I've said, the cause isn't always a missing SDK or an incorrect reference to it. Sometimes the same error message will appear if Docker can't find the entry point for the service it's trying to run - basically the EXE or DLL for the compiled application/service. This would be caused by an incorrect file path somewhere in the Dockerfile. To check for this, I opened the command line for the Docker image I was trying to run, and used the usual Linux commands to inspect its filesystem.","title":"Check entry points"},{"location":"articles/docker-missing-dotnet-sdk/#getting-more-details-in-visual-studios-powershell","text":"Run ' docker build -t ' in the PowerShell window in Visual Studio. This might provide more specific exception messages pointing to the cause of the problem. In my case, it displayed the following somewhere in the output: warning MSB3245: Could not resolve this reference. Could not locate the assembly \"*System.Configuration*\". Check to make sure the assembly exists on disk. If this reference is required by your code, you may get compilation errors. [/src/Application.Web/Application.Web.csproj]","title":"Getting more details in Visual Studio's PowerShell"},{"location":"articles/docker-missing-dotnet-sdk/#visual-studio-solution-with-multiple-projects","text":"In my case, I encountered the problem because another project in the Visual Studio solution ( Application.Common ) provided the dependencies for the one I was attempting to build ( Application.Web ). What I was doing wrong was trying to build and run Application.Web using a docker-compose.yml file within that project's directory, without referencing Application.Common or anything else in the solution directory. As a first step, I shifted docker-compose.yml and Dockerfile to the solution's root directory. Next, I added several lines to copy the entire solution to the virtual directory before building the projects/dependencies that were needed. e.g. [...] FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build WORKDIR /src COPY *.sln ./ COPY Application.Web/Application.Web.csproj Application.Web/ COPY . . RUN dotnet restore Application.Web/Application.Web.csproj WORKDIR /src/Application.Web [...] Then, using the following command, I was able to build the service (and its dependencies) and run its image in the Docker container: docker-compose up --build","title":"Visual Studio solution with multiple projects"},{"location":"articles/dotnet-core-6-ef-mvc/","text":"Thoughts on NET Core 6.0, Entity Framework and the New MVC Pattern The following mostly doesn't refer to problems native to the .NET Core 6 Framework itself, but rather some difficulties I encountered while getting an Entity Framework Database-First model working in the MVC project template generated by Visual Studio 2022. Some of the problems centred around: - Breaking changes and things that might be awkward for developers accustomed to working with legacy non-Core versions of .NET (espectially .NET MVC). - Not much documentation out there, relative to what's published for the older .NET MVC patterns. - Apparently not many developers having worked with it yet. - A heavy amount of dependency injection included in the later MVC code patterns. This occasionally made it considerably more difficult to trace errors, and would present a learning curve for less experienced developers. MVC Project Changes When the ASP.NET MVC project template is set up by Visual Studio, the code patterns and source file structure are going to be different to what was generated by previous versions. The main three differences are a) a heavy amount of dependency injection, b) the Web.config has been replaced by an appsettings.json file and Startup.cs has been replaced by Program.cs (the code pattern here is also different), and c) The method for setting up an Entity Framework model, DbContext and data source connection is different. Database-First Entity Framework Model The first problem is that Visual Studio won't scaffold the application to use Entity Framework or DbContext , and there isn't an Entity Framework model designer. A partial solution is to run the following in NuGet Package Manager Console: Install-Package Microsoft.EntityFrameworkCore.Tools And: Scaffold-DbContext \"Server=.\\SQLExpress;Database=emmatest;Trusted_Connection=True;\" Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models Click here for further information about this and related tools... I recommend this method, as it will save a considerable amount of time setting up the model, and it should be a more reliable way of applying the correct data types. In addition to the model, the script will also create a DbContext class. This will need to be modified, if not rewritten. One reason for this is the connection string must be moved into appsettings.json . The following was my rewrite of DbContext : using Microsoft.EntityFrameworkCore; using WebApplication8.Models; namespace WebApplication8.Data { public class ApplicationDbContext : DbContext { protected readonly IConfiguration Configuration; public ApplicationDbContext(DbContextOptions<DbContext> options) : base(options) { } public ApplicationDbContext(IConfiguration configuration) { Configuration = configuration; } protected override void OnConfiguring(DbContextOptionsBuilder options) { // connect to sql server with connection string from app settings options.UseSqlServer(Configuration.GetConnectionString(\"DefaultConnection\")); } public DbSet<Computer> Computers { get; set; } public DbSet<User> Users { get; set; } public DbSet<Lab> Labs { get; set; } protected override void OnModelCreating(ModelBuilder builder) { builder.Entity<WebApplication8.Models.Computer>() .ToTable(\"computer\"); } } } Connection String Insert the following in the appsettings.json file: \"ConnectionStrings\": { \"DefaultConnection\": \"Server=EMMA-LAPTOP\\\\SQLEXPRESS;Initial Catalog=emmatest;Integrated Security=True\" }, Here I've included the local machine's name in the connection string, as I might be running the application from within a Docker container. Program.cs In Program.cs, add the following: builder.Services.AddDbContext<ApplicationDbContext>(options => { options.UseSqlServer(builder.Configuration.GetConnectionString(\"DefaultConnection\")); }); Make sure to add it before the var app = builder.Build(); statement. At this point, we could use SQL Server Profiler to check whether the application is reading from the database. ( SQL Server Management Studio -> Tools -> SQL Server Profiler ) Data Validation It's a good idea to apply data validation to the model itself. In the model class file, add the following reference: using System.ComponentModel.DataAnnotations; Razor Views Obviously pretty much the same as with the older MVC pattern: @model IEnumerable<Test_MVC.Models.Computer> Using DbContext in the Controller Assuming we're just using the DbContext model without a Unit of Work Repository pattern: using Microsoft.AspNetCore.Mvc; using System.Diagnostics; using WebApplication8.Models; using WebApplication8.Data; private readonly ApplicationDbContext _context; public HomeController(ApplicationDbContext context) { _context = context; } In the controller method: public IActionResult Index() { var model = _context.Computers.ToList(); return View(model); } Further Information Microsoft Docs: Breaking Changes in .NET 6 Entity Framework Core tools reference - Package Manager Console in Visual Studio","title":".NET Core, Entity Framework and MVC"},{"location":"articles/dotnet-core-6-ef-mvc/#thoughts-on-net-core-60-entity-framework-and-the-new-mvc-pattern","text":"The following mostly doesn't refer to problems native to the .NET Core 6 Framework itself, but rather some difficulties I encountered while getting an Entity Framework Database-First model working in the MVC project template generated by Visual Studio 2022. Some of the problems centred around: - Breaking changes and things that might be awkward for developers accustomed to working with legacy non-Core versions of .NET (espectially .NET MVC). - Not much documentation out there, relative to what's published for the older .NET MVC patterns. - Apparently not many developers having worked with it yet. - A heavy amount of dependency injection included in the later MVC code patterns. This occasionally made it considerably more difficult to trace errors, and would present a learning curve for less experienced developers.","title":"Thoughts on NET Core 6.0, Entity Framework and the New MVC Pattern"},{"location":"articles/dotnet-core-6-ef-mvc/#mvc-project-changes","text":"When the ASP.NET MVC project template is set up by Visual Studio, the code patterns and source file structure are going to be different to what was generated by previous versions. The main three differences are a) a heavy amount of dependency injection, b) the Web.config has been replaced by an appsettings.json file and Startup.cs has been replaced by Program.cs (the code pattern here is also different), and c) The method for setting up an Entity Framework model, DbContext and data source connection is different.","title":"MVC Project Changes"},{"location":"articles/dotnet-core-6-ef-mvc/#database-first-entity-framework-model","text":"The first problem is that Visual Studio won't scaffold the application to use Entity Framework or DbContext , and there isn't an Entity Framework model designer. A partial solution is to run the following in NuGet Package Manager Console: Install-Package Microsoft.EntityFrameworkCore.Tools And: Scaffold-DbContext \"Server=.\\SQLExpress;Database=emmatest;Trusted_Connection=True;\" Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models Click here for further information about this and related tools... I recommend this method, as it will save a considerable amount of time setting up the model, and it should be a more reliable way of applying the correct data types. In addition to the model, the script will also create a DbContext class. This will need to be modified, if not rewritten. One reason for this is the connection string must be moved into appsettings.json . The following was my rewrite of DbContext : using Microsoft.EntityFrameworkCore; using WebApplication8.Models; namespace WebApplication8.Data { public class ApplicationDbContext : DbContext { protected readonly IConfiguration Configuration; public ApplicationDbContext(DbContextOptions<DbContext> options) : base(options) { } public ApplicationDbContext(IConfiguration configuration) { Configuration = configuration; } protected override void OnConfiguring(DbContextOptionsBuilder options) { // connect to sql server with connection string from app settings options.UseSqlServer(Configuration.GetConnectionString(\"DefaultConnection\")); } public DbSet<Computer> Computers { get; set; } public DbSet<User> Users { get; set; } public DbSet<Lab> Labs { get; set; } protected override void OnModelCreating(ModelBuilder builder) { builder.Entity<WebApplication8.Models.Computer>() .ToTable(\"computer\"); } } }","title":"Database-First Entity Framework Model"},{"location":"articles/dotnet-core-6-ef-mvc/#connection-string","text":"Insert the following in the appsettings.json file: \"ConnectionStrings\": { \"DefaultConnection\": \"Server=EMMA-LAPTOP\\\\SQLEXPRESS;Initial Catalog=emmatest;Integrated Security=True\" }, Here I've included the local machine's name in the connection string, as I might be running the application from within a Docker container.","title":"Connection String"},{"location":"articles/dotnet-core-6-ef-mvc/#programcs","text":"In Program.cs, add the following: builder.Services.AddDbContext<ApplicationDbContext>(options => { options.UseSqlServer(builder.Configuration.GetConnectionString(\"DefaultConnection\")); }); Make sure to add it before the var app = builder.Build(); statement. At this point, we could use SQL Server Profiler to check whether the application is reading from the database. ( SQL Server Management Studio -> Tools -> SQL Server Profiler )","title":"Program.cs"},{"location":"articles/dotnet-core-6-ef-mvc/#data-validation","text":"It's a good idea to apply data validation to the model itself. In the model class file, add the following reference: using System.ComponentModel.DataAnnotations;","title":"Data Validation"},{"location":"articles/dotnet-core-6-ef-mvc/#razor-views","text":"Obviously pretty much the same as with the older MVC pattern: @model IEnumerable<Test_MVC.Models.Computer>","title":"Razor Views"},{"location":"articles/dotnet-core-6-ef-mvc/#using-dbcontext-in-the-controller","text":"Assuming we're just using the DbContext model without a Unit of Work Repository pattern: using Microsoft.AspNetCore.Mvc; using System.Diagnostics; using WebApplication8.Models; using WebApplication8.Data; private readonly ApplicationDbContext _context; public HomeController(ApplicationDbContext context) { _context = context; } In the controller method: public IActionResult Index() { var model = _context.Computers.ToList(); return View(model); }","title":"Using DbContext in the Controller"},{"location":"articles/dotnet-core-6-ef-mvc/#further-information","text":"Microsoft Docs: Breaking Changes in .NET 6 Entity Framework Core tools reference - Package Manager Console in Visual Studio","title":"Further Information"},{"location":"articles/dotnet-core-ajax/","text":"Implementing an AJAX Request in .NET Core 6 MVC One of the problems with implementing a search feature that returns a table in a partial view is the Entity Framework model declarations must be strongly-typed. This means we run into difficulties declarating a model in the containing view and an IEnumerable instance of it for the partial view. One way around this is to instead discard the model for the mainview, and use jQuery and AJAX to read values from specific input elements in the main view, and pass those values to the controller action that returns the partial view with the IEnumerable model. One of the advantages of this method is the codebase might be cleaner and the application wouldn't need to reload entire sections to update parts of the main view - we can choose to update only the content within specific div tags. jQuery and AJAX Setup Install Microsoft.jQuery.Unobtrusive.Ajax using NuGet. This will be loaded into the /Dependencies/Packages folder of the project. For some reason this was missing in the template project. All the AJAX handlers should be in a dedicated JavaScript file. The .NET Core 6 MVC template provides a site.js file for this, but here I've placed the custom JS in ~/js/home.js . This should be referenced in ~\\Views\\Shared_Layout.cshtml as follows: <script src=\"~/js/site.js\" asp-append-version=\"true\"></script> <script src=\"~/js/home.js\"></script> Note: This should be placed after the line referencing jquery.min.js, as this needs to load first. Alternatively, setting RenderSectionAsync() to true might solve any loading issues related to this. An example: // AJAX handler for user search $(\".btn-submit-user-query\").click(function (e) { var searchTerm = $(\".user-search-term\").val(); $.ajax({ url: '/Home/AdminUsersSearch', type: 'GET', contentType: 'application/json; charset=utf-8', data: { searchTerm }, success: function (data) { //var message = data.Message; $('#user-search-results').html(data); } }) }); On success, the above function will replace whatever's within the #user-search-results div with whatever is returned by the IAction controller method. In this case it will be a partial view with a model populated by the search results. The controller action will require an [HttpAction] attribute to respond to AJAX calls: [HttpGet] public IActionResult AdminUsersSearch(string searchTerm) { var model = _context.Users.Where(m => m.UserName.Contains(searchTerm)).ToList(); return PartialView(\"_UsersSearchResults\", model); }","title":"Implementing AJAX in .NET Core MVC"},{"location":"articles/dotnet-core-ajax/#implementing-an-ajax-request-in-net-core-6-mvc","text":"One of the problems with implementing a search feature that returns a table in a partial view is the Entity Framework model declarations must be strongly-typed. This means we run into difficulties declarating a model in the containing view and an IEnumerable instance of it for the partial view. One way around this is to instead discard the model for the mainview, and use jQuery and AJAX to read values from specific input elements in the main view, and pass those values to the controller action that returns the partial view with the IEnumerable model. One of the advantages of this method is the codebase might be cleaner and the application wouldn't need to reload entire sections to update parts of the main view - we can choose to update only the content within specific div tags.","title":"Implementing an AJAX Request in .NET Core 6 MVC"},{"location":"articles/dotnet-core-ajax/#jquery-and-ajax-setup","text":"Install Microsoft.jQuery.Unobtrusive.Ajax using NuGet. This will be loaded into the /Dependencies/Packages folder of the project. For some reason this was missing in the template project. All the AJAX handlers should be in a dedicated JavaScript file. The .NET Core 6 MVC template provides a site.js file for this, but here I've placed the custom JS in ~/js/home.js . This should be referenced in ~\\Views\\Shared_Layout.cshtml as follows: <script src=\"~/js/site.js\" asp-append-version=\"true\"></script> <script src=\"~/js/home.js\"></script> Note: This should be placed after the line referencing jquery.min.js, as this needs to load first. Alternatively, setting RenderSectionAsync() to true might solve any loading issues related to this. An example: // AJAX handler for user search $(\".btn-submit-user-query\").click(function (e) { var searchTerm = $(\".user-search-term\").val(); $.ajax({ url: '/Home/AdminUsersSearch', type: 'GET', contentType: 'application/json; charset=utf-8', data: { searchTerm }, success: function (data) { //var message = data.Message; $('#user-search-results').html(data); } }) }); On success, the above function will replace whatever's within the #user-search-results div with whatever is returned by the IAction controller method. In this case it will be a partial view with a model populated by the search results. The controller action will require an [HttpAction] attribute to respond to AJAX calls: [HttpGet] public IActionResult AdminUsersSearch(string searchTerm) { var model = _context.Users.Where(m => m.UserName.Contains(searchTerm)).ToList(); return PartialView(\"_UsersSearchResults\", model); }","title":"jQuery and AJAX Setup"},{"location":"articles/dotnet-core-azure-ad-authentication/","text":"Azure Active Directory Authentication for .NET Core Services If upgrading an existing application, the following are required from NuGet: - Microsoft.Identity.Web - Microsoft.Identity.Web.UI Generating the Project The project in this repository was generated from the ASP.NET Core Web App template. When generating the project, select ' Microsoft identity platform ' as the Authentication type in the Additional Information section. HTTPS and Docker were also enabled for this project. After installing dotnet msidentity tool , Visual Studio will display the Service Dependencies and Service References . In Service Dependencies , the Microsoft identity platform needs to be configured. Visual Studio will display the Microsoft identity platform window, with the current user's account, the default tenant and a list of App Registrations. It might be necessary to click the refresh button to get a list of these. The following information should enable the upgrade of an existing service to use Azure Active Directory Authentication. Startup.cs The following assembly references are added: Microsoft.AspNetCore.Authentication Microsoft.AspNetCore.Authentication.OpenIdConnect Microsoft.AspNetCore.Authorization Microsoft.AspNetCore.Mvc.Authorization Microsoft.Identity.Web Microsoft.Identity.Web.UI Along with the following builder.Services sections: builder.Services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme) .AddMicrosoftIdentityWebApp(builder.Configuration.GetSection(\"AzureAd\")); builder.Services.AddAuthorization(options => { options.FallbackPolicy = options.DefaultPolicy; }); builder.Services.AddRazorPages() .AddMicrosoftIdentityUI(); app.UseAuthentication(); app.UseAuthorization(); appsettings.json The following section is added for the App Registrations entry: \"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"[Domain]\", \"TenantId\": \"e5aafe7c-971b-4ab7-b039-141ad36acec0\", \"ClientId\": \"56bfc2f2-3c6d-4792-8293-df85cf9cac48\", \"CallbackPath\": \"/signin-oidc\", \"Scopes\": \"access_as_user\", \"SignedOutCallbackPath\": \"/signout-callback-oidc\" } It should be possible to just replace the ClientId and Scopes values, as required, after an App Registration is created by the Azure service administrator. Properties/launchSettings.json There don't appear to be any changes in this file relevant to Azure AD authentication. Properties/servicesDepencies.json { \"dependencies\": { \"identityapp1\": { \"type\": \"identityapp\" }, \"secrets1\": { \"type\": \"secrets\" } } } Connected Services A reference to Microsoft identity platform is added under the Connected Services section. This can be edited at any point to add a new TenantId and/or ClientId. Dependencies Under the Depemdencies section there is the following: - Microsoft.AspNetCore.Authentication.JwtBearer - Microsoft.AspNetCore.Authentication.OpenIdConnect - Microsoft.Identity.Web - Microsoft.Identity.Web.UI The above dependencies can be installed through the NuGet Package Manager.","title":".NET Core Azure AD Authentication"},{"location":"articles/dotnet-core-azure-ad-authentication/#azure-active-directory-authentication-for-net-core-services","text":"If upgrading an existing application, the following are required from NuGet: - Microsoft.Identity.Web - Microsoft.Identity.Web.UI","title":"Azure Active Directory Authentication for .NET Core Services"},{"location":"articles/dotnet-core-azure-ad-authentication/#generating-the-project","text":"The project in this repository was generated from the ASP.NET Core Web App template. When generating the project, select ' Microsoft identity platform ' as the Authentication type in the Additional Information section. HTTPS and Docker were also enabled for this project. After installing dotnet msidentity tool , Visual Studio will display the Service Dependencies and Service References . In Service Dependencies , the Microsoft identity platform needs to be configured. Visual Studio will display the Microsoft identity platform window, with the current user's account, the default tenant and a list of App Registrations. It might be necessary to click the refresh button to get a list of these. The following information should enable the upgrade of an existing service to use Azure Active Directory Authentication.","title":"Generating the Project"},{"location":"articles/dotnet-core-azure-ad-authentication/#startupcs","text":"The following assembly references are added: Microsoft.AspNetCore.Authentication Microsoft.AspNetCore.Authentication.OpenIdConnect Microsoft.AspNetCore.Authorization Microsoft.AspNetCore.Mvc.Authorization Microsoft.Identity.Web Microsoft.Identity.Web.UI Along with the following builder.Services sections: builder.Services.AddAuthentication(OpenIdConnectDefaults.AuthenticationScheme) .AddMicrosoftIdentityWebApp(builder.Configuration.GetSection(\"AzureAd\")); builder.Services.AddAuthorization(options => { options.FallbackPolicy = options.DefaultPolicy; }); builder.Services.AddRazorPages() .AddMicrosoftIdentityUI(); app.UseAuthentication(); app.UseAuthorization();","title":"Startup.cs"},{"location":"articles/dotnet-core-azure-ad-authentication/#appsettingsjson","text":"The following section is added for the App Registrations entry: \"AzureAd\": { \"Instance\": \"https://login.microsoftonline.com/\", \"Domain\": \"[Domain]\", \"TenantId\": \"e5aafe7c-971b-4ab7-b039-141ad36acec0\", \"ClientId\": \"56bfc2f2-3c6d-4792-8293-df85cf9cac48\", \"CallbackPath\": \"/signin-oidc\", \"Scopes\": \"access_as_user\", \"SignedOutCallbackPath\": \"/signout-callback-oidc\" } It should be possible to just replace the ClientId and Scopes values, as required, after an App Registration is created by the Azure service administrator.","title":"appsettings.json"},{"location":"articles/dotnet-core-azure-ad-authentication/#propertieslaunchsettingsjson","text":"There don't appear to be any changes in this file relevant to Azure AD authentication.","title":"Properties/launchSettings.json"},{"location":"articles/dotnet-core-azure-ad-authentication/#propertiesservicesdepenciesjson","text":"{ \"dependencies\": { \"identityapp1\": { \"type\": \"identityapp\" }, \"secrets1\": { \"type\": \"secrets\" } } }","title":"Properties/servicesDepencies.json"},{"location":"articles/dotnet-core-azure-ad-authentication/#connected-services","text":"A reference to Microsoft identity platform is added under the Connected Services section. This can be edited at any point to add a new TenantId and/or ClientId.","title":"Connected Services"},{"location":"articles/dotnet-core-azure-ad-authentication/#dependencies","text":"Under the Depemdencies section there is the following: - Microsoft.AspNetCore.Authentication.JwtBearer - Microsoft.AspNetCore.Authentication.OpenIdConnect - Microsoft.Identity.Web - Microsoft.Identity.Web.UI The above dependencies can be installed through the NuGet Package Manager.","title":"Dependencies"},{"location":"articles/dotnet-core-xunit/","text":".NET Core: xUnit Tests with Repository Pattern and IUNitOfWork After some hours of frustration and asking myself whether I should have used Unit of Work in the first place (there are already layers of abstraction in the standard .NET Core template project) I managed to get a functioning xUnit test method that mocks the repositories, populates them with data and verifies the model returned by a controller method. Setting Up a Unit Test Project Add a xUnit project to the solution, then add an assembly reference in that to the project being tested. Just to check the test method executes and to show the format of a basic class: using Xunit; public class UnitTest1 { [Fact] public void SimpleTest() { int five = 5; Assert.Equal(5, five); } } When the test is run, the method will execute and show in the results as a test that passed. That's because I declared five as '5', and used Assert.Equal to check the two values match. Test methods typically have sections for Arrange, Act and Assert - instantiating a data model, defining an expected result and comparing the actual result against it - and, of course, there are multiple Assert methods that could be used for testing various other conditions. Testing a Home Controller Method Unit testing a HomeController in a .NET Core application that's using the repository pattern is less straightforward, as I'm not using DbContext or the Entity Framework model directly. The good news is it's not necessary to duplicate the Repository Pattern or the Unit of Work, as they can be imported fom the application project: using DotNetCore6.Controllers; using DotNetCore6.Data; using DotNetCore6.Models; using Microsoft.AspNetCore.Mvc; using System.Collections.Generic; The unit test must create an instance of HomeController , which requires something like IUnitOfWork to be passed to it. This takes the place of DbContext. public HomeController(IUnitOfWork unitOfWork) { _unitOfWork = unitOfWork; } Moq can be used for creating a test instance of IUnitOfWork and the repositories, and for passing them to HomeController . The method under test, PowerOn() , simply fetches a list of computers from the Computers model and returns that in a view. [Fact] public void TestPowerOnView() { var mockUoW = new Mock<IUnitOfWork>(); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService.View(); Assert.IsType<ViewResult>(result); } After setting a breakpoint within the HomeController constructor and running the test in debug mode, it will be evident that something like IUnitOfWork had been passed to it, and there are references to the Entity Framework models. Attaching Repositories Using Setup() The next step is to attach the repositories to the Unit of Work being passed to HomeController . The Setup() method will do all the work of adding them to Mock () anyway. If we use the following code and set a breakpoint on the HomeController again, we find that it's discovered IComputersRepository , ILabsRepository and IUsersRepository . The interfaces don't refer to the actual repositories or entities yet. [Fact] public void TestPowerOnView() { var mockUoW = new Mock<IUnitOfWork>(); mockUoW.Setup(a => a.Computers.GetAll()); mockUoW.Setup(a => a.Labs.GetAll()); mockUoW.Setup(a => a.Users.GetAll()); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService; Assert.IsType<ViewResult>(result); } Adding Mock Entities The next problem is that of getting the entities passed to the controller. Normally a repository will refer to an IEnumerable/List .For the sake of making things easier to follow, I've created a mock repository within the test method. [Fact] public void TestPowerOnView() { List<Computers> mockComputer = new List<Computers>() { new Computers() { id = 1, host = \"TEST COMPUTER\", ip = \"193.134.9.5\", mac = \"TEST MAC ADDRESS\", subnet = \"255.255.0.0\", broadcast = \"255.255.255.255\", created_at = System.DateTime.Now, updated_at = System.DateTime.Now } }; var mockUoW = new Mock<IUnitOfWork>(); mockUoW.Setup(a => a.Computers.GetAll()).Returns(mockComputer); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService; Assert.IsType<ViewResult>(result); } The method will try *GetAll()* with an empty repository, but still return the entities in *mockComputer*. What I didn't realise is we need to put a breakpoint in the method being tested itself to see the model being passed into it. This test will pass, but it only verifies that a view was returned by PowerOn(). It doesn't tell us anything about the data being returned. ## Testing the Return Data In this case, getting the model returned also wasn't straightforward, as I couldn't access the model directly. I needed to add some extra code to extract that. var viewResult = Assert.IsType (result); var model = Assert.IsAssignableFrom >(viewResult.ViewData.Model); Assert.Equal(mockComputer, model); ``` Sources Creating Unit Tests for ASP.NET MVC Applications (C#) Moq Quickstart: https://github.com/Moq/moq4/wiki/Quickstart xUnit.Net: https://xunit.net","title":".NET Core and xUnit Tests"},{"location":"articles/dotnet-core-xunit/#net-core-xunit-tests-with-repository-pattern-and-iunitofwork","text":"After some hours of frustration and asking myself whether I should have used Unit of Work in the first place (there are already layers of abstraction in the standard .NET Core template project) I managed to get a functioning xUnit test method that mocks the repositories, populates them with data and verifies the model returned by a controller method.","title":".NET Core: xUnit Tests with Repository Pattern and IUNitOfWork"},{"location":"articles/dotnet-core-xunit/#setting-up-a-unit-test-project","text":"Add a xUnit project to the solution, then add an assembly reference in that to the project being tested. Just to check the test method executes and to show the format of a basic class: using Xunit; public class UnitTest1 { [Fact] public void SimpleTest() { int five = 5; Assert.Equal(5, five); } } When the test is run, the method will execute and show in the results as a test that passed. That's because I declared five as '5', and used Assert.Equal to check the two values match. Test methods typically have sections for Arrange, Act and Assert - instantiating a data model, defining an expected result and comparing the actual result against it - and, of course, there are multiple Assert methods that could be used for testing various other conditions.","title":"Setting Up a Unit Test Project"},{"location":"articles/dotnet-core-xunit/#testing-a-home-controller-method","text":"Unit testing a HomeController in a .NET Core application that's using the repository pattern is less straightforward, as I'm not using DbContext or the Entity Framework model directly. The good news is it's not necessary to duplicate the Repository Pattern or the Unit of Work, as they can be imported fom the application project: using DotNetCore6.Controllers; using DotNetCore6.Data; using DotNetCore6.Models; using Microsoft.AspNetCore.Mvc; using System.Collections.Generic; The unit test must create an instance of HomeController , which requires something like IUnitOfWork to be passed to it. This takes the place of DbContext. public HomeController(IUnitOfWork unitOfWork) { _unitOfWork = unitOfWork; } Moq can be used for creating a test instance of IUnitOfWork and the repositories, and for passing them to HomeController . The method under test, PowerOn() , simply fetches a list of computers from the Computers model and returns that in a view. [Fact] public void TestPowerOnView() { var mockUoW = new Mock<IUnitOfWork>(); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService.View(); Assert.IsType<ViewResult>(result); } After setting a breakpoint within the HomeController constructor and running the test in debug mode, it will be evident that something like IUnitOfWork had been passed to it, and there are references to the Entity Framework models.","title":"Testing a Home Controller Method"},{"location":"articles/dotnet-core-xunit/#attaching-repositories-using-setup","text":"The next step is to attach the repositories to the Unit of Work being passed to HomeController . The Setup() method will do all the work of adding them to Mock () anyway. If we use the following code and set a breakpoint on the HomeController again, we find that it's discovered IComputersRepository , ILabsRepository and IUsersRepository . The interfaces don't refer to the actual repositories or entities yet. [Fact] public void TestPowerOnView() { var mockUoW = new Mock<IUnitOfWork>(); mockUoW.Setup(a => a.Computers.GetAll()); mockUoW.Setup(a => a.Labs.GetAll()); mockUoW.Setup(a => a.Users.GetAll()); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService; Assert.IsType<ViewResult>(result); }","title":"Attaching Repositories Using Setup()"},{"location":"articles/dotnet-core-xunit/#adding-mock-entities","text":"The next problem is that of getting the entities passed to the controller. Normally a repository will refer to an IEnumerable/List .For the sake of making things easier to follow, I've created a mock repository within the test method. [Fact] public void TestPowerOnView() { List<Computers> mockComputer = new List<Computers>() { new Computers() { id = 1, host = \"TEST COMPUTER\", ip = \"193.134.9.5\", mac = \"TEST MAC ADDRESS\", subnet = \"255.255.0.0\", broadcast = \"255.255.255.255\", created_at = System.DateTime.Now, updated_at = System.DateTime.Now } }; var mockUoW = new Mock<IUnitOfWork>(); mockUoW.Setup(a => a.Computers.GetAll()).Returns(mockComputer); var powerOnService = new HomeController(mockUoW.Object).PowerOn(); var result = powerOnService; Assert.IsType<ViewResult>(result); } The method will try *GetAll()* with an empty repository, but still return the entities in *mockComputer*. What I didn't realise is we need to put a breakpoint in the method being tested itself to see the model being passed into it. This test will pass, but it only verifies that a view was returned by PowerOn(). It doesn't tell us anything about the data being returned. ## Testing the Return Data In this case, getting the model returned also wasn't straightforward, as I couldn't access the model directly. I needed to add some extra code to extract that. var viewResult = Assert.IsType (result); var model = Assert.IsAssignableFrom >(viewResult.ViewData.Model); Assert.Equal(mockComputer, model); ``` Sources Creating Unit Tests for ASP.NET MVC Applications (C#) Moq Quickstart: https://github.com/Moq/moq4/wiki/Quickstart xUnit.Net: https://xunit.net","title":"Adding Mock Entities"},{"location":"articles/ef-core-cli-reference/","text":".NET Core Entity Framework Command Line Reference Installing Entity command line The .NET Entity Framework command line tools can be installed globally in PowerShell, or just for a specific project. dotnet tool install --global dotnet-ef After installation, it might be a good idea to update the tools: dotnet tool update --global dotnet-ef Add the Entity Framework Packages to a Project In order to use the Entity Framework command line tools for a .NET Core project, it's necessary to install the Design package. This could be done using the EF Core CLI or the NuGet Package Manager. dotnet add package Microsoft.EntityFrameworkCore.Design I also recommend ensuring the following packages are also installed: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.SqlServer - Microsoft.EntityFrameworkCore.Tools Scaffolding DbContext To generate the migrations, Entity Framework will require the project to have a DbContext. If there isn't one present (there should be), this can be scaffolded from whatever connection string(s) might exist in the configuration file. For this, it might be necessary to define the connection string and the provider. dotnet ef dbcontext scaffold [Connection String] [Microsoft.EntityFrameworkCore.SqlServer] -o Models This should create the DbContext and place the model .cs files in the Models directory. Generate a SQL script from the DbContext dotnet ef dbcontext script Add a migration for the project dotnet ef migrations add [Name] Generate a SQL script for the latest migration dotnet ef migrations script Create a bundle/executable to update the database dotnet ef migrations bundle","title":"Entity Framework Core CLI Reference"},{"location":"articles/ef-core-cli-reference/#net-core-entity-framework-command-line-reference","text":"","title":".NET Core Entity Framework Command Line Reference"},{"location":"articles/ef-core-cli-reference/#installing-entity-command-line","text":"The .NET Entity Framework command line tools can be installed globally in PowerShell, or just for a specific project. dotnet tool install --global dotnet-ef After installation, it might be a good idea to update the tools: dotnet tool update --global dotnet-ef","title":"Installing Entity command line"},{"location":"articles/ef-core-cli-reference/#add-the-entity-framework-packages-to-a-project","text":"In order to use the Entity Framework command line tools for a .NET Core project, it's necessary to install the Design package. This could be done using the EF Core CLI or the NuGet Package Manager. dotnet add package Microsoft.EntityFrameworkCore.Design I also recommend ensuring the following packages are also installed: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.SqlServer - Microsoft.EntityFrameworkCore.Tools","title":"Add the Entity Framework Packages to a Project"},{"location":"articles/ef-core-cli-reference/#scaffolding-dbcontext","text":"To generate the migrations, Entity Framework will require the project to have a DbContext. If there isn't one present (there should be), this can be scaffolded from whatever connection string(s) might exist in the configuration file. For this, it might be necessary to define the connection string and the provider. dotnet ef dbcontext scaffold [Connection String] [Microsoft.EntityFrameworkCore.SqlServer] -o Models This should create the DbContext and place the model .cs files in the Models directory.","title":"Scaffolding DbContext"},{"location":"articles/ef-core-cli-reference/#generate-a-sql-script-from-the-dbcontext","text":"dotnet ef dbcontext script","title":"Generate a SQL script from the DbContext"},{"location":"articles/ef-core-cli-reference/#add-a-migration-for-the-project","text":"dotnet ef migrations add [Name]","title":"Add a migration for the project"},{"location":"articles/ef-core-cli-reference/#generate-a-sql-script-for-the-latest-migration","text":"dotnet ef migrations script","title":"Generate a SQL script for the latest migration"},{"location":"articles/ef-core-cli-reference/#create-a-bundleexecutable-to-update-the-database","text":"dotnet ef migrations bundle","title":"Create a bundle/executable to update the database"},{"location":"articles/entity-framework-migrations/","text":"Entity Framework Migrations The purpose of generating Entity Framework migrations is to apply database schema changes consistently across Staging, UAT and Production environments. The proposed method would involve developers generating migration a migration script whenever changes are made to the Entity Framework model in their projects. The DevOps team would apply that migration script to update the Azure environments. The services and applications being developed themselves should not have permissions to modify the environment. Requirements The following assumes the migration C# files, and the SQL scripts, are being generated for a .NET service/application with a working database connection and DbContext. Install the following NuGet packages to the project: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.SqlServer - Microsoft.EntityFrameworkCore.Tools Install the Entity Framework Command Line Tools The method for generating the migration scripts will be different, according to whether it's being done through PowerShell or Visual Studio's Package Manager Console. In the Windows Terminal, the Entity Framework tools are installed globally: dotnet tool install --global dotnet-ef The tools can also be installed for the Visual Studio project using the Package Manager Console: Install-Package Microsoft.EntityFrameworkCore.Tools Update-Package Microsoft.EntityFrameworkCore.Tools Create a migration: If there is more than one project in the Visual Studio solution (such as unit tests), it will be necessary to specify the project to generate the migration source files from, using the '-Project' or '-P' parameter. e.g. Add-Migration TestMigration -Project DotNet-Core-6-MVC This should create a Migrations directory in the Visual Studio project. Generating SQL Script The following generates a SQL script from the last created migration: dotnet ef migrations script Or, in the Package Manager Console: script-migration -P DotNet-Core-6-MVC Again, the project name must be defined, if there are multiple in the currently-opened solution. Generating an Entity Framework Migrations Bundle A bundle is an executable file that applies the migrations. Might be very useful for DevOps, and might be possible to execute through Azure PowerShell. This can be done with the folloing command: dotnet ef migrations bundle --self-contained To apply this to a specific database from a specific migration: .\\bundle.exe [Migration Name] --connection \"[Connection String]\" Migrations and Docker One method is to add something like the following command to the Dockerfile: RUN dotnet ef database update Applying Migrations on Azure There are several potential solutions: - Apply migrations by running generated SQL scripts through PowerShell and the AZ command line. - Create another Docker container as an agent to apply the migrations (tricky). - Create a Logic App on Azure that contains the SQL script and can be triggered after a build is tested. - Add the migration to Tasks under the 'Automation' section for the relevant App Service. Suggested Method The solution being used has something to do with YAML-defined Azure deployment pipelines. Further Information Entity Framework Core tools reference Applying Entity Framework Migrations to a Docker Container Introducing DevOps-friendly EF Core Migration Bundles Further Inormation (DevOps) YAML schema: target definition Run EF Core Migrations in Azure DevOps Migration #EntityFramework #Database #DevOps","title":"Entity Framework Migrations"},{"location":"articles/entity-framework-migrations/#entity-framework-migrations","text":"The purpose of generating Entity Framework migrations is to apply database schema changes consistently across Staging, UAT and Production environments. The proposed method would involve developers generating migration a migration script whenever changes are made to the Entity Framework model in their projects. The DevOps team would apply that migration script to update the Azure environments. The services and applications being developed themselves should not have permissions to modify the environment.","title":"Entity Framework Migrations"},{"location":"articles/entity-framework-migrations/#requirements","text":"The following assumes the migration C# files, and the SQL scripts, are being generated for a .NET service/application with a working database connection and DbContext. Install the following NuGet packages to the project: - Microsoft.EntityFrameworkCore - Microsoft.EntityFrameworkCore.SqlServer - Microsoft.EntityFrameworkCore.Tools","title":"Requirements"},{"location":"articles/entity-framework-migrations/#install-the-entity-framework-command-line-tools","text":"The method for generating the migration scripts will be different, according to whether it's being done through PowerShell or Visual Studio's Package Manager Console. In the Windows Terminal, the Entity Framework tools are installed globally: dotnet tool install --global dotnet-ef The tools can also be installed for the Visual Studio project using the Package Manager Console: Install-Package Microsoft.EntityFrameworkCore.Tools Update-Package Microsoft.EntityFrameworkCore.Tools","title":"Install the Entity Framework Command Line Tools"},{"location":"articles/entity-framework-migrations/#create-a-migration","text":"If there is more than one project in the Visual Studio solution (such as unit tests), it will be necessary to specify the project to generate the migration source files from, using the '-Project' or '-P' parameter. e.g. Add-Migration TestMigration -Project DotNet-Core-6-MVC This should create a Migrations directory in the Visual Studio project.","title":"Create a migration:"},{"location":"articles/entity-framework-migrations/#generating-sql-script","text":"The following generates a SQL script from the last created migration: dotnet ef migrations script Or, in the Package Manager Console: script-migration -P DotNet-Core-6-MVC Again, the project name must be defined, if there are multiple in the currently-opened solution.","title":"Generating SQL Script"},{"location":"articles/entity-framework-migrations/#generating-an-entity-framework-migrations-bundle","text":"A bundle is an executable file that applies the migrations. Might be very useful for DevOps, and might be possible to execute through Azure PowerShell. This can be done with the folloing command: dotnet ef migrations bundle --self-contained To apply this to a specific database from a specific migration: .\\bundle.exe [Migration Name] --connection \"[Connection String]\"","title":"Generating an Entity Framework Migrations Bundle"},{"location":"articles/entity-framework-migrations/#migrations-and-docker","text":"One method is to add something like the following command to the Dockerfile: RUN dotnet ef database update","title":"Migrations and Docker"},{"location":"articles/entity-framework-migrations/#applying-migrations-on-azure","text":"There are several potential solutions: - Apply migrations by running generated SQL scripts through PowerShell and the AZ command line. - Create another Docker container as an agent to apply the migrations (tricky). - Create a Logic App on Azure that contains the SQL script and can be triggered after a build is tested. - Add the migration to Tasks under the 'Automation' section for the relevant App Service.","title":"Applying Migrations on Azure"},{"location":"articles/entity-framework-migrations/#suggested-method","text":"The solution being used has something to do with YAML-defined Azure deployment pipelines.","title":"Suggested Method"},{"location":"articles/entity-framework-migrations/#further-information","text":"Entity Framework Core tools reference Applying Entity Framework Migrations to a Docker Container Introducing DevOps-friendly EF Core Migration Bundles","title":"Further Information"},{"location":"articles/entity-framework-migrations/#further-inormation-devops","text":"YAML schema: target definition Run EF Core Migrations in Azure DevOps","title":"Further Inormation (DevOps)"},{"location":"articles/entity-framework-migrations/#migration-entityframework-database-devops","text":"","title":"Migration #EntityFramework #Database #DevOps"},{"location":"articles/kestrel-errors/","text":"Kestrel HTTPS Error - .Net Core and Docker The following is another commonly-posted question in Stack Overflow, which I've been looking at while debugging the same problem: System.InvalidOperationException: Unable to configure HTTPS endpoint. No server certificate was specified, and the default developer certificate could not be found or is out of date. The most obvious cause would be Kestrel trying to use invalid or expired HTTPS certificates. However, looking more closely, my error message was preceded by: [...] [15:25:25 FTL] Unable to start Kestrel. System.InvalidOperationException: Unable to configure HTTPS endpoint. No server certificate was specified, and the default developer certificate could not be found or is out of date. To generate a developer certificate run 'dotnet dev-certs https'. To trust the certificate (Windows and macOS only) run 'dotnet dev-certs https --trust'. [...] And is followed by: [...] Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.BindAsync(IServerAddressesFeature addresses, KestrelServerOptions serverOptions, ILogger logger, Func`2 createBinding) So, the real problem could have been anything from Kestrel simply being unable to start, which is less likely, to Kestrel being unable to bind the service to a port. The easiest thing to check is that the development certificates are valid anyway, using the following: dotnet dev-certs https --clean dotnet dev-certs https dotnet dev-certs https --trust And, after this, restarting Visual Studio. The Actual Problem As it turned out, the problem was caused by the exposed ports and port mappings being incorrectly defined in the Docker configuration. First, shift the ASPNETCORE_URLS variables from the Dockerfile to docker-compose.yml environment section. This is good practice, in my opinion, because all environment variables should ideally be in one place. Next, explicitly set the Kestrel endpoint. environment: - ASPNETCORE_URLS=https://+:443;http://+:80 - ASPNETCORE_HTTPS_PORT=5001 - Kestrel__Endpoints__Http__Url=http://*:80 - ASPNETCORE_ENVIRONMENT= Development","title":"Kestrel HTTPS Error"},{"location":"articles/kestrel-errors/#kestrel-https-error-net-core-and-docker","text":"The following is another commonly-posted question in Stack Overflow, which I've been looking at while debugging the same problem: System.InvalidOperationException: Unable to configure HTTPS endpoint. No server certificate was specified, and the default developer certificate could not be found or is out of date. The most obvious cause would be Kestrel trying to use invalid or expired HTTPS certificates. However, looking more closely, my error message was preceded by: [...] [15:25:25 FTL] Unable to start Kestrel. System.InvalidOperationException: Unable to configure HTTPS endpoint. No server certificate was specified, and the default developer certificate could not be found or is out of date. To generate a developer certificate run 'dotnet dev-certs https'. To trust the certificate (Windows and macOS only) run 'dotnet dev-certs https --trust'. [...] And is followed by: [...] Microsoft.AspNetCore.Server.Kestrel.Core.Internal.AddressBinder.BindAsync(IServerAddressesFeature addresses, KestrelServerOptions serverOptions, ILogger logger, Func`2 createBinding) So, the real problem could have been anything from Kestrel simply being unable to start, which is less likely, to Kestrel being unable to bind the service to a port. The easiest thing to check is that the development certificates are valid anyway, using the following: dotnet dev-certs https --clean dotnet dev-certs https dotnet dev-certs https --trust And, after this, restarting Visual Studio.","title":"Kestrel HTTPS Error - .Net Core and Docker"},{"location":"articles/kestrel-errors/#the-actual-problem","text":"As it turned out, the problem was caused by the exposed ports and port mappings being incorrectly defined in the Docker configuration. First, shift the ASPNETCORE_URLS variables from the Dockerfile to docker-compose.yml environment section. This is good practice, in my opinion, because all environment variables should ideally be in one place. Next, explicitly set the Kestrel endpoint. environment: - ASPNETCORE_URLS=https://+:443;http://+:80 - ASPNETCORE_HTTPS_PORT=5001 - Kestrel__Endpoints__Http__Url=http://*:80 - ASPNETCORE_ENVIRONMENT= Development","title":"The Actual Problem"},{"location":"articles/repository-pattern-and-uow/","text":".NET Core 6: Dependency Injection, Repository Pattern and Unit of Work A repository pattern exists to add another layer of abstraction between the application logic and the data layer, using dependency injection to decouple the two. It's not my preferred way of doing things, because of the effort and complexity involved, and I'm certainly not the only person who initially struggled to understand the concepts behind it. Plus, it's apparent that Entity Framework makes heavy use of that dependency injection already. But a Repository Pattern is widely considered - by software engineers far more experienced than I am - best practice for engineering services that deal with critical data. The repository pattern is implemented generally the same way dependency injection is in .NET Core. What I've ended up with is: 1. An interface that's called when a service is required. 2. Classes that implement the service, basically performing the data access functions. These will be generic repository classes that are extended elsewhere by repository classes specific to the Entity Framework model. 3. Code in Program.cs to register the service. 4. Constructor for the service in the HomeController class. The repository should decouple the business logic from Entity Framework and the data access layer, and prevent partial writes that affect the integrity of the database. It is also useful for testing, as we can swap one data access layer with another. In my project, all the repository and Unit of Work code is within the DotNetCore6.Data namespace. Add IGeneric Repository Interface This is essentially a boilerplate interface, and will be exposed to the application controller methods. It points to methods in the Generic class that implement fetch and write operations. public interface IGenericRepository<T> where T : class { T GetById(int id); IEnumerable<T> GetAll(); IEnumerable<T> Find(Expression<Func<T, bool>> expression); void Add(T entity); void AddRange(IEnumerable<T> entities); void Remove(T entity); void RemoveRange(IEnumerable<T> entities); } Add a Generic Repository Class The GenericRepository class is where generic services exposed by the interface are implemented, and it uses the ApplicationDbContext directly. It passes ApplicationDbContext to a class and constructs it as ' _context ', just as I did in the HomeController referred to in an earlier post. Notice that this is a generic repository, which isn't specific to anything in the Entity Framework model. This means I'm not duplicating the same data access code for each model class - though the code would likely be easier to follow if I did. Instead we'll declare and extend this class elsewhere, as needed. public class GenericRepository<T> : IGenericRepository<T> where T : class { protected readonly ApplicationDbContext _context; public GenericRepository(ApplicationDbContext context) { _context = context; } ... public void Add(T entity) { _context.Set<T>().Add(entity); } public IEnumerable<T> GetAll() { return _context.Set<T>().ToList(); } public T GetById(int id) { return _context.Set<T>().Find(id); } ... } The above sections can be considered (and used as) boilerplate code, though I'll likely be adding several other methods for other read/write/update functions later on. From this point, we need to extend this generic repository for working with the Entity Framework model classes. Extend the Generic Repository for the Entity Framework Classes In this section of code ( ComputersRepository.cs ), we create a non-generic repository for each model class. In this case, IComputersRepository will extend IGenericRepository . As we can see, I've also placed its implementation, ComputersRepository , in the same file. public interface IComputersRepository : IGenericRepository<Computer> { IEnumerable<Computer> GetComputers(int count); } public class ComputersRepository : GenericRepository<Computer>, IComputersRepository { public ComputersRepository(ApplicationDbContext context) : base(context) { } public IEnumerable<Computer> GetComputers(int count) { return _context.Computers.ToList(); } } Unit of Work Classes Next I wanted to add a Unit of Work pattern to the project. Instead of using _context.Computers or _context.Labs , as I did originally, I'm defining them as interfaces within IUnitOfWork , containing an interface for each model class. public interface IUnitOfWork : IDisposable { IComputersRepository Computers { get; } ILabsRepository Labs { get; } IUsersRepository Users { get; } int Complete(); } Only when whichever operation is completed is Complete() called, which in turn saves changes to the DbContext. This should prevent partial updates that might corrupt the data source. public class UnitOfWork : IUnitOfWork { private readonly ApplicationDbContext _context; public UnitOfWork(ApplicationDbContext context) { _context = context; Computers = new ComputersRepository(_context); Labs = new LabsRepository(_context); Users = new UsersRepository(_context); } public IComputersRepository Computers { get; private set; } public ILabsRepository Labs { get; private set; } public IUsersRepository Users { get; private set; } public int Complete() { return _context.SaveChanges(); } public void Dispose() { _context.Dispose(); } } At this point, the project will include something like the following for the Repository Pattern, Unit of Work and Entity Framework code: |-- Data |-- dbcontext.cs |-- UnitOfWork.cs |-- Models |-- Computer.cs |-- Lab.cs |-- User.cs |-- Repository |-- ComputersRepository.cs |-- GenericRepository.cs |-- IGenericRepository.cs |-- LabsRepository.cs |-- UsersRepository.cs Registering the Services When registering the interfaces in Program.cs in .NET Core 6, we must use ' builder.Services ' instead of just the ' services ' namespace: builder.Services.AddTransient(typeof(IGenericRepository<>), typeof(GenericRepository<>)); builder.Services.AddTransient<IComputersRepository, ComputersRepository>(); builder.Services.AddTransient<ILabsRepository, LabsRepository>(); builder.Services.AddTransient<IUsersRepository, UsersRepository>(); builder.Services.AddScoped<IUnitOfWork, UnitOfWork>(); Using the Repository Pattern in the Controller Swap ApplicationDbContext with a constructor for the Unit of Work service: private readonly IUnitOfWork _unitOfWork; public HomeController(IUnitOfWork unitOfWork) { _unitOfWork = unitOfWork; } Accessing data with LINQ: var model = _unitOfWork.Users.GetById(id); var model = _unitOfWork.Labs.GetAll().Where(m => m.Name.Contains(searchTerm)).ToList();","title":"Repository Pattern and UoW"},{"location":"articles/repository-pattern-and-uow/#net-core-6-dependency-injection-repository-pattern-and-unit-of-work","text":"A repository pattern exists to add another layer of abstraction between the application logic and the data layer, using dependency injection to decouple the two. It's not my preferred way of doing things, because of the effort and complexity involved, and I'm certainly not the only person who initially struggled to understand the concepts behind it. Plus, it's apparent that Entity Framework makes heavy use of that dependency injection already. But a Repository Pattern is widely considered - by software engineers far more experienced than I am - best practice for engineering services that deal with critical data. The repository pattern is implemented generally the same way dependency injection is in .NET Core. What I've ended up with is: 1. An interface that's called when a service is required. 2. Classes that implement the service, basically performing the data access functions. These will be generic repository classes that are extended elsewhere by repository classes specific to the Entity Framework model. 3. Code in Program.cs to register the service. 4. Constructor for the service in the HomeController class. The repository should decouple the business logic from Entity Framework and the data access layer, and prevent partial writes that affect the integrity of the database. It is also useful for testing, as we can swap one data access layer with another. In my project, all the repository and Unit of Work code is within the DotNetCore6.Data namespace.","title":".NET Core 6: Dependency Injection, Repository Pattern and Unit of Work"},{"location":"articles/repository-pattern-and-uow/#add-igeneric-repository-interface","text":"This is essentially a boilerplate interface, and will be exposed to the application controller methods. It points to methods in the Generic class that implement fetch and write operations. public interface IGenericRepository<T> where T : class { T GetById(int id); IEnumerable<T> GetAll(); IEnumerable<T> Find(Expression<Func<T, bool>> expression); void Add(T entity); void AddRange(IEnumerable<T> entities); void Remove(T entity); void RemoveRange(IEnumerable<T> entities); }","title":"Add IGeneric Repository Interface"},{"location":"articles/repository-pattern-and-uow/#add-a-generic-repository-class","text":"The GenericRepository class is where generic services exposed by the interface are implemented, and it uses the ApplicationDbContext directly. It passes ApplicationDbContext to a class and constructs it as ' _context ', just as I did in the HomeController referred to in an earlier post. Notice that this is a generic repository, which isn't specific to anything in the Entity Framework model. This means I'm not duplicating the same data access code for each model class - though the code would likely be easier to follow if I did. Instead we'll declare and extend this class elsewhere, as needed. public class GenericRepository<T> : IGenericRepository<T> where T : class { protected readonly ApplicationDbContext _context; public GenericRepository(ApplicationDbContext context) { _context = context; } ... public void Add(T entity) { _context.Set<T>().Add(entity); } public IEnumerable<T> GetAll() { return _context.Set<T>().ToList(); } public T GetById(int id) { return _context.Set<T>().Find(id); } ... } The above sections can be considered (and used as) boilerplate code, though I'll likely be adding several other methods for other read/write/update functions later on. From this point, we need to extend this generic repository for working with the Entity Framework model classes.","title":"Add a Generic Repository Class"},{"location":"articles/repository-pattern-and-uow/#extend-the-generic-repository-for-the-entity-framework-classes","text":"In this section of code ( ComputersRepository.cs ), we create a non-generic repository for each model class. In this case, IComputersRepository will extend IGenericRepository . As we can see, I've also placed its implementation, ComputersRepository , in the same file. public interface IComputersRepository : IGenericRepository<Computer> { IEnumerable<Computer> GetComputers(int count); } public class ComputersRepository : GenericRepository<Computer>, IComputersRepository { public ComputersRepository(ApplicationDbContext context) : base(context) { } public IEnumerable<Computer> GetComputers(int count) { return _context.Computers.ToList(); } }","title":"Extend the Generic Repository for the Entity Framework Classes"},{"location":"articles/repository-pattern-and-uow/#unit-of-work-classes","text":"Next I wanted to add a Unit of Work pattern to the project. Instead of using _context.Computers or _context.Labs , as I did originally, I'm defining them as interfaces within IUnitOfWork , containing an interface for each model class. public interface IUnitOfWork : IDisposable { IComputersRepository Computers { get; } ILabsRepository Labs { get; } IUsersRepository Users { get; } int Complete(); } Only when whichever operation is completed is Complete() called, which in turn saves changes to the DbContext. This should prevent partial updates that might corrupt the data source. public class UnitOfWork : IUnitOfWork { private readonly ApplicationDbContext _context; public UnitOfWork(ApplicationDbContext context) { _context = context; Computers = new ComputersRepository(_context); Labs = new LabsRepository(_context); Users = new UsersRepository(_context); } public IComputersRepository Computers { get; private set; } public ILabsRepository Labs { get; private set; } public IUsersRepository Users { get; private set; } public int Complete() { return _context.SaveChanges(); } public void Dispose() { _context.Dispose(); } } At this point, the project will include something like the following for the Repository Pattern, Unit of Work and Entity Framework code: |-- Data |-- dbcontext.cs |-- UnitOfWork.cs |-- Models |-- Computer.cs |-- Lab.cs |-- User.cs |-- Repository |-- ComputersRepository.cs |-- GenericRepository.cs |-- IGenericRepository.cs |-- LabsRepository.cs |-- UsersRepository.cs","title":"Unit of Work Classes"},{"location":"articles/repository-pattern-and-uow/#registering-the-services","text":"When registering the interfaces in Program.cs in .NET Core 6, we must use ' builder.Services ' instead of just the ' services ' namespace: builder.Services.AddTransient(typeof(IGenericRepository<>), typeof(GenericRepository<>)); builder.Services.AddTransient<IComputersRepository, ComputersRepository>(); builder.Services.AddTransient<ILabsRepository, LabsRepository>(); builder.Services.AddTransient<IUsersRepository, UsersRepository>(); builder.Services.AddScoped<IUnitOfWork, UnitOfWork>();","title":"Registering the Services"},{"location":"articles/repository-pattern-and-uow/#using-the-repository-pattern-in-the-controller","text":"Swap ApplicationDbContext with a constructor for the Unit of Work service: private readonly IUnitOfWork _unitOfWork; public HomeController(IUnitOfWork unitOfWork) { _unitOfWork = unitOfWork; } Accessing data with LINQ: var model = _unitOfWork.Users.GetById(id); var model = _unitOfWork.Labs.GetAll().Where(m => m.Name.Contains(searchTerm)).ToList();","title":"Using the Repository Pattern in the Controller"},{"location":"articles/sql-server-database-vs-project/","text":"Migrations Using Visual Studio SQL Server Database Project Setting Up the Migration Project Add a new project to the Visual Studio solution. This project will initially be empty, without assembly references or scaffold files - it appears everything is native to Visual Studio. To this we need to add the connection to the database the schema is to be imported from. There will be a menu option for ' Import ' -> ' Database '. After the connection is made, the project will be populated by a SQL script for each table, and, by default, a few other scripts to apply or update security policies for the database. Making Database Schema Changes If we double-click on a given script, the main window will display the script and designer views. Either one could be used for modifying the schema and/or fields. Another view of a given database table can be displayed in the SQL Server Object Explorer window, if we select ' View in Object Explorer '. After changes are made, Visual Studio can display a comparison between the local schema and that on the target database, using ' Schema Compare... '. This might take a few moments, if it's running through a large number of tables and .sql files. Updating the Target Database There are three ways I can see of using this project to update the database: - The script(s) can be run manually on the database server. - We can build/rebuild the project, which will generate a DACPAC file in /bin/Debug - this doesn't appear to work in SQL Server Management Studio v18, but I'm guessing the aim here is to use the file in some automated Azure process. - The Publish feature will attempt to run the scripts on a target database server to push the schema update. While the scripts listed in Solution Explorer are CREATE TABLE scripts, the ones generated in the Schema Compare run the ALTER TABLE command. Other Features: The ' Snapshot Project ' feature will also generate a DACPAC file. Testing I have done some cursory testing, by attempting various updates to a couple of the tables on the target database. For one of the tables I changed the data type of the id field to int, and this failed because the new data type was incompatible with the existing data, and a forced update would have most likely caused data loss. It is possible to override these checks in the Schema Compare Options, if the current settings are blocking the desired changes. Next, I added a column called 'ExtraTest' to both tables, and used the Update option in the Schema Compare window to push the update. The columns were added successfully without any loss of data. VisualStudio #Migrations #SQL #Database","title":"Visual Studio SQL Server Database Project"},{"location":"articles/sql-server-database-vs-project/#migrations-using-visual-studio-sql-server-database-project","text":"","title":"Migrations Using Visual Studio SQL Server Database Project"},{"location":"articles/sql-server-database-vs-project/#setting-up-the-migration-project","text":"Add a new project to the Visual Studio solution. This project will initially be empty, without assembly references or scaffold files - it appears everything is native to Visual Studio. To this we need to add the connection to the database the schema is to be imported from. There will be a menu option for ' Import ' -> ' Database '. After the connection is made, the project will be populated by a SQL script for each table, and, by default, a few other scripts to apply or update security policies for the database.","title":"Setting Up the Migration Project"},{"location":"articles/sql-server-database-vs-project/#making-database-schema-changes","text":"If we double-click on a given script, the main window will display the script and designer views. Either one could be used for modifying the schema and/or fields. Another view of a given database table can be displayed in the SQL Server Object Explorer window, if we select ' View in Object Explorer '. After changes are made, Visual Studio can display a comparison between the local schema and that on the target database, using ' Schema Compare... '. This might take a few moments, if it's running through a large number of tables and .sql files.","title":"Making Database Schema Changes"},{"location":"articles/sql-server-database-vs-project/#updating-the-target-database","text":"There are three ways I can see of using this project to update the database: - The script(s) can be run manually on the database server. - We can build/rebuild the project, which will generate a DACPAC file in /bin/Debug - this doesn't appear to work in SQL Server Management Studio v18, but I'm guessing the aim here is to use the file in some automated Azure process. - The Publish feature will attempt to run the scripts on a target database server to push the schema update. While the scripts listed in Solution Explorer are CREATE TABLE scripts, the ones generated in the Schema Compare run the ALTER TABLE command. Other Features: The ' Snapshot Project ' feature will also generate a DACPAC file.","title":"Updating the Target Database"},{"location":"articles/sql-server-database-vs-project/#testing","text":"I have done some cursory testing, by attempting various updates to a couple of the tables on the target database. For one of the tables I changed the data type of the id field to int, and this failed because the new data type was incompatible with the existing data, and a forced update would have most likely caused data loss. It is possible to override these checks in the Schema Compare Options, if the current settings are blocking the desired changes. Next, I added a column called 'ExtraTest' to both tables, and used the Update option in the Schema Compare window to push the update. The columns were added successfully without any loss of data.","title":"Testing"},{"location":"articles/sql-server-database-vs-project/#visualstudio-migrations-sql-database","text":"","title":"VisualStudio #Migrations #SQL #Database"},{"location":"articles/sql-server-db-vs-project/","text":"Migrations Using Visual Studio SQL Server Database Project Setting Up the Migration Project Add a new project to the Visual Studio solution. This project will initially be empty, without assembly references or scaffold files - it appears everything is native to Visual Studio. To this we need to add the connection to the database the schema is to be imported from. There will be a menu option for ' Import ' -> ' Database '. After the connection is made, the project will be populated by a SQL script for each table, and, by default, a few other scripts to apply or update security policies for the database. It is preferable to remove (don't delete) the Security directory from the project. Making Database Schema Changes If we double-click on a given script, the main window will display the script and designer views. Either one could be used for modifying the schema and/or fields. Another view of a given database table can be displayed in the SQL Server Object Explorer window, if we select ' View in Object Explorer '. After changes are made, Visual Studio can display a comparison between the local schema and that on the target database, using ' Schema Compare... '. This might take a few moments, if it's running through a large number of tables and .sql files. Updating the Target Database There are three ways I can see of using this project to update the database: - The script(s) can be run manually on the database server. - We can build/rebuild the project, which will generate a DACPAC file in /bin/Debug - this doesn't appear to work in SQL Server Management Studio v18, but I'm guessing the aim here is to use the file in some automated Azure process. - The Publish feature will attempt to run the scripts on a target database server to push the schema update. While the scripts listed in Solution Explorer are CREATE TABLE scripts, the ones generated in the Schema Compare run the ALTER TABLE command. Other Features: The ' Snapshot Project ' feature will also generate a DACPAC file. Testing I have done some cursory testing, by attempting various updates to a couple of the tables on the target database. For one of the tables I changed the data type of the id field to int, and this failed because the new data type was incompatible with the existing data, and a forced update would have most likely caused data loss. It is possible to override these checks in the Schema Compare Options, if the current settings are blocking the desired changes. Next, I added a column called 'ExtraTest' to both tables, and used the Update option in the Schema Compare window to push the update. The columns were added successfully without any loss of data.","title":"Visual Studio SQL Server Database Project"},{"location":"articles/sql-server-db-vs-project/#migrations-using-visual-studio-sql-server-database-project","text":"","title":"Migrations Using Visual Studio SQL Server Database Project"},{"location":"articles/sql-server-db-vs-project/#setting-up-the-migration-project","text":"Add a new project to the Visual Studio solution. This project will initially be empty, without assembly references or scaffold files - it appears everything is native to Visual Studio. To this we need to add the connection to the database the schema is to be imported from. There will be a menu option for ' Import ' -> ' Database '. After the connection is made, the project will be populated by a SQL script for each table, and, by default, a few other scripts to apply or update security policies for the database. It is preferable to remove (don't delete) the Security directory from the project.","title":"Setting Up the Migration Project"},{"location":"articles/sql-server-db-vs-project/#making-database-schema-changes","text":"If we double-click on a given script, the main window will display the script and designer views. Either one could be used for modifying the schema and/or fields. Another view of a given database table can be displayed in the SQL Server Object Explorer window, if we select ' View in Object Explorer '. After changes are made, Visual Studio can display a comparison between the local schema and that on the target database, using ' Schema Compare... '. This might take a few moments, if it's running through a large number of tables and .sql files.","title":"Making Database Schema Changes"},{"location":"articles/sql-server-db-vs-project/#updating-the-target-database","text":"There are three ways I can see of using this project to update the database: - The script(s) can be run manually on the database server. - We can build/rebuild the project, which will generate a DACPAC file in /bin/Debug - this doesn't appear to work in SQL Server Management Studio v18, but I'm guessing the aim here is to use the file in some automated Azure process. - The Publish feature will attempt to run the scripts on a target database server to push the schema update. While the scripts listed in Solution Explorer are CREATE TABLE scripts, the ones generated in the Schema Compare run the ALTER TABLE command. Other Features: The ' Snapshot Project ' feature will also generate a DACPAC file.","title":"Updating the Target Database"},{"location":"articles/sql-server-db-vs-project/#testing","text":"I have done some cursory testing, by attempting various updates to a couple of the tables on the target database. For one of the tables I changed the data type of the id field to int, and this failed because the new data type was incompatible with the existing data, and a forced update would have most likely caused data loss. It is possible to override these checks in the Schema Compare Options, if the current settings are blocking the desired changes. Next, I added a column called 'ExtraTest' to both tables, and used the Update option in the Schema Compare window to push the update. The columns were added successfully without any loss of data.","title":"Testing"}]}